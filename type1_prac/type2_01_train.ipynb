{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9add4c5e",
   "metadata": {},
   "source": [
    "## 01. 항공권 가격 예측\n",
    "\n",
    "* 항공권 티겟 가격을 예측하시오.\n",
    "    * 제공된 데이터 목록: filght_train.csv, filght_test.csv\n",
    "    * 예측할 컬럼: price\n",
    "\n",
    "* 학습용 데이터(train)를 이용해 티켓가격을 예측하는 모델을 만든 후 이를 평가용 데이터(test)에 적용해 얻은 예측 값을 다음과 같은 형식의 CSV 파일로 생성하시오.\n",
    "\n",
    "제출 파일은 다음 1개의 컬럼을 포함해야 한다.\n",
    "* pred: 예측값(가격)\n",
    "* 제출 파일명: 'result.csv'\n",
    "제출한 모델의 성능은 RMSE 평가지표에 따라 채점한다.\n",
    "\n",
    "* 제출 csv 파일명 및 형태: result.csv\n",
    "```python\n",
    "pred\n",
    "56000\n",
    "7000\n",
    "11000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81990813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db976830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10505, 11)\n",
      "(4502, 10)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../bigdata_analyst_cert/part2/ch8/flight_train.csv\")\n",
    "df_test = pd.read_csv(\"../bigdata_analyst_cert/part2/ch8/flight_test.csv\")\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bdb218c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>flight</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-776</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Evening</td>\n",
       "      <td>one</td>\n",
       "      <td>Late_Night</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Economy</td>\n",
       "      <td>6.58</td>\n",
       "      <td>31</td>\n",
       "      <td>7056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-852</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Business</td>\n",
       "      <td>1.92</td>\n",
       "      <td>37</td>\n",
       "      <td>20760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indigo</td>\n",
       "      <td>6E-2348</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Evening</td>\n",
       "      <td>one</td>\n",
       "      <td>Late_Night</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Economy</td>\n",
       "      <td>5.58</td>\n",
       "      <td>25</td>\n",
       "      <td>3671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Air_India</td>\n",
       "      <td>AI-763</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Business</td>\n",
       "      <td>12.00</td>\n",
       "      <td>15</td>\n",
       "      <td>55983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indigo</td>\n",
       "      <td>6E-752</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>9.50</td>\n",
       "      <td>20</td>\n",
       "      <td>5220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     airline   flight source_city departure_time stops arrival_time  \\\n",
       "0    Vistara   UK-776     Kolkata        Evening   one   Late_Night   \n",
       "1    Vistara   UK-852   Bangalore        Morning  zero      Morning   \n",
       "2     Indigo  6E-2348       Delhi        Evening   one   Late_Night   \n",
       "3  Air_India   AI-763     Kolkata  Early_Morning   one      Evening   \n",
       "4     Indigo   6E-752   Hyderabad  Early_Morning   one      Evening   \n",
       "\n",
       "  destination_city     class  duration  days_left  price  \n",
       "0            Delhi   Economy      6.58         31   7056  \n",
       "1           Mumbai  Business      1.92         37  20760  \n",
       "2        Bangalore   Economy      5.58         25   3671  \n",
       "3          Chennai  Business     12.00         15  55983  \n",
       "4          Chennai   Economy      9.50         20   5220  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08ea338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10505 entries, 0 to 10504\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   airline           10505 non-null  object \n",
      " 1   flight            10505 non-null  object \n",
      " 2   source_city       10505 non-null  object \n",
      " 3   departure_time    10505 non-null  object \n",
      " 4   stops             10505 non-null  object \n",
      " 5   arrival_time      10505 non-null  object \n",
      " 6   destination_city  10505 non-null  object \n",
      " 7   class             10505 non-null  object \n",
      " 8   duration          10505 non-null  float64\n",
      " 9   days_left         10505 non-null  int64  \n",
      " 10  price             10505 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(8)\n",
      "memory usage: 902.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c25045ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10505.000000</td>\n",
       "      <td>10505.000000</td>\n",
       "      <td>10505.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.225536</td>\n",
       "      <td>26.050547</td>\n",
       "      <td>20650.139838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.182264</td>\n",
       "      <td>13.539947</td>\n",
       "      <td>22570.924117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.830000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1105.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.750000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4755.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.250000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>7455.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.170000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>42457.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40.500000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>110936.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           duration     days_left          price\n",
       "count  10505.000000  10505.000000   10505.000000\n",
       "mean      12.225536     26.050547   20650.139838\n",
       "std        7.182264     13.539947   22570.924117\n",
       "min        0.830000      1.000000    1105.000000\n",
       "25%        6.750000     15.000000    4755.000000\n",
       "50%       11.250000     26.000000    7455.000000\n",
       "75%       16.170000     38.000000   42457.000000\n",
       "max       40.500000     49.000000  110936.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beccc8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['airline', 'flight', 'source_city', 'departure_time', 'stops',\n",
       "       'arrival_time', 'destination_city', 'class', 'duration', 'days_left',\n",
       "       'price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob_index = (df_train.dtypes == 'object').index\n",
    "ob_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bda5b226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline                6\n",
       "flight              1153\n",
       "source_city            6\n",
       "departure_time         6\n",
       "stops                  3\n",
       "arrival_time           6\n",
       "destination_city       6\n",
       "class                  2\n",
       "duration             392\n",
       "days_left             49\n",
       "price               3116\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33824d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = ['duration', 'days_left', 'price']\n",
    "categorical = ['airline', 'flight', 'source_city', 'departure_time', 'stops', 'arrival_time', 'destination_city', 'class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3c08e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.050043</td>\n",
       "      <td>0.210738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>days_left</th>\n",
       "      <td>-0.050043</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.092827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>0.210738</td>\n",
       "      <td>-0.092827</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           duration  days_left     price\n",
       "duration   1.000000  -0.050043  0.210738\n",
       "days_left -0.050043   1.000000 -0.092827\n",
       "price      0.210738  -0.092827  1.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_numeric = df_train[numeric]\n",
    "df_train_numeric.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "080bd1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airline 컬럼의 유니크 밸류\n",
      "['Vistara' 'Indigo' 'Air_India' 'AirAsia' 'GO_FIRST' 'SpiceJet']\n",
      "\n",
      "airline nunique\n",
      "6\n",
      "\n",
      "airline 컬럼의 유니크 걔수\n",
      "airline\n",
      "Vistara      4425\n",
      "Air_India    2842\n",
      "Indigo       1556\n",
      "GO_FIRST      800\n",
      "AirAsia       564\n",
      "SpiceJet      318\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "flight 컬럼의 유니크 밸류\n",
      "['UK-776' 'UK-852' '6E-2348' ... '6E-7076' 'AI-435' 'SG-5002']\n",
      "\n",
      "flight nunique\n",
      "1153\n",
      "\n",
      "flight 컬럼의 유니크 걔수\n",
      "flight\n",
      "UK-706     108\n",
      "UK-832      99\n",
      "UK-828      98\n",
      "UK-720      98\n",
      "UK-822      97\n",
      "          ... \n",
      "G8-2502      1\n",
      "6E-995       1\n",
      "G8-287       1\n",
      "6E-7118      1\n",
      "SG-438       1\n",
      "Name: count, Length: 1153, dtype: int64\n",
      "--------------------------------------------------\n",
      "source_city 컬럼의 유니크 밸류\n",
      "['Kolkata' 'Bangalore' 'Delhi' 'Hyderabad' 'Mumbai' 'Chennai']\n",
      "\n",
      "source_city nunique\n",
      "6\n",
      "\n",
      "source_city 컬럼의 유니크 걔수\n",
      "source_city\n",
      "Mumbai       2112\n",
      "Delhi        2074\n",
      "Bangalore    1865\n",
      "Kolkata      1623\n",
      "Hyderabad    1418\n",
      "Chennai      1413\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "departure_time 컬럼의 유니크 밸류\n",
      "['Evening' 'Morning' 'Early_Morning' 'Afternoon' 'Night' 'Late_Night']\n",
      "\n",
      "departure_time nunique\n",
      "6\n",
      "\n",
      "departure_time 컬럼의 유니크 걔수\n",
      "departure_time\n",
      "Morning          2470\n",
      "Early_Morning    2313\n",
      "Evening          2302\n",
      "Night            1752\n",
      "Afternoon        1615\n",
      "Late_Night         53\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "stops 컬럼의 유니크 밸류\n",
      "['one' 'zero' 'two_or_more']\n",
      "\n",
      "stops nunique\n",
      "3\n",
      "\n",
      "stops 컬럼의 유니크 걔수\n",
      "stops\n",
      "one            8750\n",
      "zero           1274\n",
      "two_or_more     481\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "arrival_time 컬럼의 유니크 밸류\n",
      "['Late_Night' 'Morning' 'Evening' 'Afternoon' 'Night' 'Early_Morning']\n",
      "\n",
      "arrival_time nunique\n",
      "6\n",
      "\n",
      "arrival_time 컬럼의 유니크 걔수\n",
      "arrival_time\n",
      "Night            3220\n",
      "Evening          2795\n",
      "Morning          2176\n",
      "Afternoon        1319\n",
      "Early_Morning     504\n",
      "Late_Night        491\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "destination_city 컬럼의 유니크 밸류\n",
      "['Delhi' 'Mumbai' 'Bangalore' 'Chennai' 'Kolkata' 'Hyderabad']\n",
      "\n",
      "destination_city nunique\n",
      "6\n",
      "\n",
      "destination_city 컬럼의 유니크 걔수\n",
      "destination_city\n",
      "Mumbai       2103\n",
      "Delhi        2040\n",
      "Bangalore    1773\n",
      "Kolkata      1715\n",
      "Hyderabad    1506\n",
      "Chennai      1368\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "class 컬럼의 유니크 밸류\n",
      "['Economy' 'Business']\n",
      "\n",
      "class nunique\n",
      "2\n",
      "\n",
      "class 컬럼의 유니크 걔수\n",
      "class\n",
      "Economy     7306\n",
      "Business    3199\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for ctg in categorical:\n",
    "    print(f'{ctg} 컬럼의 유니크 밸류')\n",
    "    print(df_train[ctg].unique())\n",
    "    print()\n",
    "    print(f'{ctg} nunique')\n",
    "    print(df_train[ctg].nunique())\n",
    "    print()\n",
    "    print(f'{ctg} 컬럼의 유니크 걔수')\n",
    "    print(df_train[ctg].value_counts())\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d449d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flight\n",
       "G8-719     1\n",
       "6E-319     1\n",
       "G8-2601    1\n",
       "G8-2502    1\n",
       "6E-995     1\n",
       "G8-287     1\n",
       "6E-7118    1\n",
       "SG-438     1\n",
       "6E-448     1\n",
       "6E-7583    1\n",
       "6E-453     1\n",
       "6E-6464    1\n",
       "SG-5002    1\n",
       "SG-6027    1\n",
       "6E-2032    1\n",
       "6E-546     1\n",
       "6E-2097    1\n",
       "I5-881     1\n",
       "SG-8104    1\n",
       "SG-8483    1\n",
       "AI-688     1\n",
       "AI-713     1\n",
       "AI-531     1\n",
       "AI-9609    1\n",
       "AI-604     1\n",
       "6E-2007    1\n",
       "6E-6517    1\n",
       "6E-234     1\n",
       "UK-627     1\n",
       "6E-5343    1\n",
       "6E-5336    1\n",
       "SG-5012    1\n",
       "6E-7224    1\n",
       "6E-575     1\n",
       "G8-2440    1\n",
       "6E-782     1\n",
       "6E-146     1\n",
       "SG-242     1\n",
       "SG-480     1\n",
       "6E-172     1\n",
       "SG-401     1\n",
       "6E-6798    1\n",
       "I5-1622    1\n",
       "6E-7062    1\n",
       "6E-809     1\n",
       "AI-643     1\n",
       "6E-5316    1\n",
       "6E-6234    1\n",
       "6E-161     1\n",
       "6E-356     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['flight'].value_counts(ascending=True).iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "886bd899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Evening</td>\n",
       "      <td>one</td>\n",
       "      <td>Late_Night</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Economy</td>\n",
       "      <td>6.58</td>\n",
       "      <td>31</td>\n",
       "      <td>7056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Business</td>\n",
       "      <td>1.92</td>\n",
       "      <td>37</td>\n",
       "      <td>20760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indigo</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Evening</td>\n",
       "      <td>one</td>\n",
       "      <td>Late_Night</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Economy</td>\n",
       "      <td>5.58</td>\n",
       "      <td>25</td>\n",
       "      <td>3671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Air_India</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Business</td>\n",
       "      <td>12.00</td>\n",
       "      <td>15</td>\n",
       "      <td>55983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indigo</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>9.50</td>\n",
       "      <td>20</td>\n",
       "      <td>5220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     airline source_city departure_time stops arrival_time destination_city  \\\n",
       "0    Vistara     Kolkata        Evening   one   Late_Night            Delhi   \n",
       "1    Vistara   Bangalore        Morning  zero      Morning           Mumbai   \n",
       "2     Indigo       Delhi        Evening   one   Late_Night        Bangalore   \n",
       "3  Air_India     Kolkata  Early_Morning   one      Evening          Chennai   \n",
       "4     Indigo   Hyderabad  Early_Morning   one      Evening          Chennai   \n",
       "\n",
       "      class  duration  days_left  price  \n",
       "0   Economy      6.58         31   7056  \n",
       "1  Business      1.92         37  20760  \n",
       "2   Economy      5.58         25   3671  \n",
       "3  Business     12.00         15  55983  \n",
       "4   Economy      9.50         20   5220  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 쿨하게 flight 드랍하시죠? \n",
    "\n",
    "df_train = df_train.drop('flight', axis=1)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c9890e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb38efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a4066",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f504c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.get_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b52ba782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = df_train['price']\n",
    "df_train = df_train.drop('price', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa973690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Evening</td>\n",
       "      <td>one</td>\n",
       "      <td>Late_Night</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Economy</td>\n",
       "      <td>6.58</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Business</td>\n",
       "      <td>1.92</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indigo</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Evening</td>\n",
       "      <td>one</td>\n",
       "      <td>Late_Night</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Economy</td>\n",
       "      <td>5.58</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Air_India</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Business</td>\n",
       "      <td>12.00</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indigo</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>9.50</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     airline source_city departure_time stops arrival_time destination_city  \\\n",
       "0    Vistara     Kolkata        Evening   one   Late_Night            Delhi   \n",
       "1    Vistara   Bangalore        Morning  zero      Morning           Mumbai   \n",
       "2     Indigo       Delhi        Evening   one   Late_Night        Bangalore   \n",
       "3  Air_India     Kolkata  Early_Morning   one      Evening          Chennai   \n",
       "4     Indigo   Hyderabad  Early_Morning   one      Evening          Chennai   \n",
       "\n",
       "      class  duration  days_left  \n",
       "0   Economy      6.58         31  \n",
       "1  Business      1.92         37  \n",
       "2   Economy      5.58         25  \n",
       "3  Business     12.00         15  \n",
       "4   Economy      9.50         20  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2717f53e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flight'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical.pop(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "67db91df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_AirAsia</th>\n",
       "      <th>airline_Air_India</th>\n",
       "      <th>airline_GO_FIRST</th>\n",
       "      <th>airline_Indigo</th>\n",
       "      <th>airline_SpiceJet</th>\n",
       "      <th>airline_Vistara</th>\n",
       "      <th>source_city_Bangalore</th>\n",
       "      <th>source_city_Chennai</th>\n",
       "      <th>source_city_Delhi</th>\n",
       "      <th>source_city_Hyderabad</th>\n",
       "      <th>...</th>\n",
       "      <th>arrival_time_Morning</th>\n",
       "      <th>arrival_time_Night</th>\n",
       "      <th>destination_city_Bangalore</th>\n",
       "      <th>destination_city_Chennai</th>\n",
       "      <th>destination_city_Delhi</th>\n",
       "      <th>destination_city_Hyderabad</th>\n",
       "      <th>destination_city_Kolkata</th>\n",
       "      <th>destination_city_Mumbai</th>\n",
       "      <th>class_Business</th>\n",
       "      <th>class_Economy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10500</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10502</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10503</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10504</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10505 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       airline_AirAsia  airline_Air_India  airline_GO_FIRST  airline_Indigo  \\\n",
       "0                False              False             False           False   \n",
       "1                False              False             False           False   \n",
       "2                False              False             False            True   \n",
       "3                False               True             False           False   \n",
       "4                False              False             False            True   \n",
       "...                ...                ...               ...             ...   \n",
       "10500            False              False             False           False   \n",
       "10501            False              False             False           False   \n",
       "10502             True              False             False           False   \n",
       "10503            False              False             False           False   \n",
       "10504            False              False             False           False   \n",
       "\n",
       "       airline_SpiceJet  airline_Vistara  source_city_Bangalore  \\\n",
       "0                 False             True                  False   \n",
       "1                 False             True                   True   \n",
       "2                 False            False                  False   \n",
       "3                 False            False                  False   \n",
       "4                 False            False                  False   \n",
       "...                 ...              ...                    ...   \n",
       "10500             False             True                   True   \n",
       "10501             False             True                  False   \n",
       "10502             False            False                  False   \n",
       "10503             False             True                  False   \n",
       "10504              True            False                   True   \n",
       "\n",
       "       source_city_Chennai  source_city_Delhi  source_city_Hyderabad  ...  \\\n",
       "0                    False              False                  False  ...   \n",
       "1                    False              False                  False  ...   \n",
       "2                    False               True                  False  ...   \n",
       "3                    False              False                  False  ...   \n",
       "4                    False              False                   True  ...   \n",
       "...                    ...                ...                    ...  ...   \n",
       "10500                False              False                  False  ...   \n",
       "10501                False              False                  False  ...   \n",
       "10502                 True              False                  False  ...   \n",
       "10503                False              False                  False  ...   \n",
       "10504                False              False                  False  ...   \n",
       "\n",
       "       arrival_time_Morning  arrival_time_Night  destination_city_Bangalore  \\\n",
       "0                     False               False                       False   \n",
       "1                      True               False                       False   \n",
       "2                     False               False                        True   \n",
       "3                     False               False                       False   \n",
       "4                     False               False                       False   \n",
       "...                     ...                 ...                         ...   \n",
       "10500                  True               False                       False   \n",
       "10501                 False                True                       False   \n",
       "10502                 False               False                       False   \n",
       "10503                 False                True                       False   \n",
       "10504                 False                True                       False   \n",
       "\n",
       "       destination_city_Chennai  destination_city_Delhi  \\\n",
       "0                         False                    True   \n",
       "1                         False                   False   \n",
       "2                         False                   False   \n",
       "3                          True                   False   \n",
       "4                          True                   False   \n",
       "...                         ...                     ...   \n",
       "10500                      True                   False   \n",
       "10501                     False                   False   \n",
       "10502                     False                    True   \n",
       "10503                     False                   False   \n",
       "10504                     False                    True   \n",
       "\n",
       "       destination_city_Hyderabad  destination_city_Kolkata  \\\n",
       "0                           False                     False   \n",
       "1                           False                     False   \n",
       "2                           False                     False   \n",
       "3                           False                     False   \n",
       "4                           False                     False   \n",
       "...                           ...                       ...   \n",
       "10500                       False                     False   \n",
       "10501                       False                     False   \n",
       "10502                       False                     False   \n",
       "10503                       False                      True   \n",
       "10504                       False                     False   \n",
       "\n",
       "       destination_city_Mumbai  class_Business  class_Economy  \n",
       "0                        False           False           True  \n",
       "1                         True            True          False  \n",
       "2                        False           False           True  \n",
       "3                        False            True          False  \n",
       "4                        False           False           True  \n",
       "...                        ...             ...            ...  \n",
       "10500                    False           False           True  \n",
       "10501                     True           False           True  \n",
       "10502                    False           False           True  \n",
       "10503                    False            True          False  \n",
       "10504                    False           False           True  \n",
       "\n",
       "[10505 rows x 35 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies = pd.get_dummies(df_train[categorical])\n",
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0d9e6c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>airline_AirAsia</th>\n",
       "      <th>...</th>\n",
       "      <th>arrival_time_Morning</th>\n",
       "      <th>arrival_time_Night</th>\n",
       "      <th>destination_city_Bangalore</th>\n",
       "      <th>destination_city_Chennai</th>\n",
       "      <th>destination_city_Delhi</th>\n",
       "      <th>destination_city_Hyderabad</th>\n",
       "      <th>destination_city_Kolkata</th>\n",
       "      <th>destination_city_Mumbai</th>\n",
       "      <th>class_Business</th>\n",
       "      <th>class_Economy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Evening</td>\n",
       "      <td>one</td>\n",
       "      <td>Late_Night</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Economy</td>\n",
       "      <td>6.58</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Morning</td>\n",
       "      <td>zero</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Business</td>\n",
       "      <td>1.92</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indigo</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Evening</td>\n",
       "      <td>one</td>\n",
       "      <td>Late_Night</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Economy</td>\n",
       "      <td>5.58</td>\n",
       "      <td>25</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Air_India</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Business</td>\n",
       "      <td>12.00</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indigo</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>9.50</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10500</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Evening</td>\n",
       "      <td>one</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>16.83</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Night</td>\n",
       "      <td>zero</td>\n",
       "      <td>Night</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.92</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10502</th>\n",
       "      <td>AirAsia</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Night</td>\n",
       "      <td>one</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Economy</td>\n",
       "      <td>16.08</td>\n",
       "      <td>27</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10503</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Evening</td>\n",
       "      <td>one</td>\n",
       "      <td>Night</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Business</td>\n",
       "      <td>28.67</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10504</th>\n",
       "      <td>SpiceJet</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Evening</td>\n",
       "      <td>zero</td>\n",
       "      <td>Night</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Economy</td>\n",
       "      <td>2.75</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10505 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         airline source_city departure_time stops arrival_time  \\\n",
       "0        Vistara     Kolkata        Evening   one   Late_Night   \n",
       "1        Vistara   Bangalore        Morning  zero      Morning   \n",
       "2         Indigo       Delhi        Evening   one   Late_Night   \n",
       "3      Air_India     Kolkata  Early_Morning   one      Evening   \n",
       "4         Indigo   Hyderabad  Early_Morning   one      Evening   \n",
       "...          ...         ...            ...   ...          ...   \n",
       "10500    Vistara   Bangalore        Evening   one      Morning   \n",
       "10501    Vistara     Kolkata          Night  zero        Night   \n",
       "10502    AirAsia     Chennai          Night   one    Afternoon   \n",
       "10503    Vistara      Mumbai        Evening   one        Night   \n",
       "10504   SpiceJet   Bangalore        Evening  zero        Night   \n",
       "\n",
       "      destination_city     class  duration  days_left  airline_AirAsia  ...  \\\n",
       "0                Delhi   Economy      6.58         31            False  ...   \n",
       "1               Mumbai  Business      1.92         37            False  ...   \n",
       "2            Bangalore   Economy      5.58         25            False  ...   \n",
       "3              Chennai  Business     12.00         15            False  ...   \n",
       "4              Chennai   Economy      9.50         20            False  ...   \n",
       "...                ...       ...       ...        ...              ...  ...   \n",
       "10500          Chennai   Economy     16.83         11            False  ...   \n",
       "10501           Mumbai   Economy      2.92         20            False  ...   \n",
       "10502            Delhi   Economy     16.08         27             True  ...   \n",
       "10503          Kolkata  Business     28.67          2            False  ...   \n",
       "10504            Delhi   Economy      2.75         18            False  ...   \n",
       "\n",
       "       arrival_time_Morning  arrival_time_Night  destination_city_Bangalore  \\\n",
       "0                     False               False                       False   \n",
       "1                      True               False                       False   \n",
       "2                     False               False                        True   \n",
       "3                     False               False                       False   \n",
       "4                     False               False                       False   \n",
       "...                     ...                 ...                         ...   \n",
       "10500                  True               False                       False   \n",
       "10501                 False                True                       False   \n",
       "10502                 False               False                       False   \n",
       "10503                 False                True                       False   \n",
       "10504                 False                True                       False   \n",
       "\n",
       "       destination_city_Chennai  destination_city_Delhi  \\\n",
       "0                         False                    True   \n",
       "1                         False                   False   \n",
       "2                         False                   False   \n",
       "3                          True                   False   \n",
       "4                          True                   False   \n",
       "...                         ...                     ...   \n",
       "10500                      True                   False   \n",
       "10501                     False                   False   \n",
       "10502                     False                    True   \n",
       "10503                     False                   False   \n",
       "10504                     False                    True   \n",
       "\n",
       "       destination_city_Hyderabad  destination_city_Kolkata  \\\n",
       "0                           False                     False   \n",
       "1                           False                     False   \n",
       "2                           False                     False   \n",
       "3                           False                     False   \n",
       "4                           False                     False   \n",
       "...                           ...                       ...   \n",
       "10500                       False                     False   \n",
       "10501                       False                     False   \n",
       "10502                       False                     False   \n",
       "10503                       False                      True   \n",
       "10504                       False                     False   \n",
       "\n",
       "       destination_city_Mumbai  class_Business  class_Economy  \n",
       "0                        False           False           True  \n",
       "1                         True            True          False  \n",
       "2                        False           False           True  \n",
       "3                        False            True          False  \n",
       "4                        False           False           True  \n",
       "...                        ...             ...            ...  \n",
       "10500                    False           False           True  \n",
       "10501                     True           False           True  \n",
       "10502                    False           False           True  \n",
       "10503                    False            True          False  \n",
       "10504                    False           False           True  \n",
       "\n",
       "[10505 rows x 44 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_dummies = pd.concat([df_train, dummies], axis=1)\n",
    "df_train_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c1f2d05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>airline_AirAsia</th>\n",
       "      <th>airline_Air_India</th>\n",
       "      <th>airline_GO_FIRST</th>\n",
       "      <th>airline_Indigo</th>\n",
       "      <th>airline_SpiceJet</th>\n",
       "      <th>airline_Vistara</th>\n",
       "      <th>source_city_Bangalore</th>\n",
       "      <th>source_city_Chennai</th>\n",
       "      <th>...</th>\n",
       "      <th>arrival_time_Morning</th>\n",
       "      <th>arrival_time_Night</th>\n",
       "      <th>destination_city_Bangalore</th>\n",
       "      <th>destination_city_Chennai</th>\n",
       "      <th>destination_city_Delhi</th>\n",
       "      <th>destination_city_Hyderabad</th>\n",
       "      <th>destination_city_Kolkata</th>\n",
       "      <th>destination_city_Mumbai</th>\n",
       "      <th>class_Business</th>\n",
       "      <th>class_Economy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.58</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.92</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.58</td>\n",
       "      <td>25</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.00</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.50</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10500</th>\n",
       "      <td>16.83</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>2.92</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10502</th>\n",
       "      <td>16.08</td>\n",
       "      <td>27</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10503</th>\n",
       "      <td>28.67</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10504</th>\n",
       "      <td>2.75</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10505 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration  days_left  airline_AirAsia  airline_Air_India  \\\n",
       "0          6.58         31            False              False   \n",
       "1          1.92         37            False              False   \n",
       "2          5.58         25            False              False   \n",
       "3         12.00         15            False               True   \n",
       "4          9.50         20            False              False   \n",
       "...         ...        ...              ...                ...   \n",
       "10500     16.83         11            False              False   \n",
       "10501      2.92         20            False              False   \n",
       "10502     16.08         27             True              False   \n",
       "10503     28.67          2            False              False   \n",
       "10504      2.75         18            False              False   \n",
       "\n",
       "       airline_GO_FIRST  airline_Indigo  airline_SpiceJet  airline_Vistara  \\\n",
       "0                 False           False             False             True   \n",
       "1                 False           False             False             True   \n",
       "2                 False            True             False            False   \n",
       "3                 False           False             False            False   \n",
       "4                 False            True             False            False   \n",
       "...                 ...             ...               ...              ...   \n",
       "10500             False           False             False             True   \n",
       "10501             False           False             False             True   \n",
       "10502             False           False             False            False   \n",
       "10503             False           False             False             True   \n",
       "10504             False           False              True            False   \n",
       "\n",
       "       source_city_Bangalore  source_city_Chennai  ...  arrival_time_Morning  \\\n",
       "0                      False                False  ...                 False   \n",
       "1                       True                False  ...                  True   \n",
       "2                      False                False  ...                 False   \n",
       "3                      False                False  ...                 False   \n",
       "4                      False                False  ...                 False   \n",
       "...                      ...                  ...  ...                   ...   \n",
       "10500                   True                False  ...                  True   \n",
       "10501                  False                False  ...                 False   \n",
       "10502                  False                 True  ...                 False   \n",
       "10503                  False                False  ...                 False   \n",
       "10504                   True                False  ...                 False   \n",
       "\n",
       "       arrival_time_Night  destination_city_Bangalore  \\\n",
       "0                   False                       False   \n",
       "1                   False                       False   \n",
       "2                   False                        True   \n",
       "3                   False                       False   \n",
       "4                   False                       False   \n",
       "...                   ...                         ...   \n",
       "10500               False                       False   \n",
       "10501                True                       False   \n",
       "10502               False                       False   \n",
       "10503                True                       False   \n",
       "10504                True                       False   \n",
       "\n",
       "       destination_city_Chennai  destination_city_Delhi  \\\n",
       "0                         False                    True   \n",
       "1                         False                   False   \n",
       "2                         False                   False   \n",
       "3                          True                   False   \n",
       "4                          True                   False   \n",
       "...                         ...                     ...   \n",
       "10500                      True                   False   \n",
       "10501                     False                   False   \n",
       "10502                     False                    True   \n",
       "10503                     False                   False   \n",
       "10504                     False                    True   \n",
       "\n",
       "       destination_city_Hyderabad  destination_city_Kolkata  \\\n",
       "0                           False                     False   \n",
       "1                           False                     False   \n",
       "2                           False                     False   \n",
       "3                           False                     False   \n",
       "4                           False                     False   \n",
       "...                           ...                       ...   \n",
       "10500                       False                     False   \n",
       "10501                       False                     False   \n",
       "10502                       False                     False   \n",
       "10503                       False                      True   \n",
       "10504                       False                     False   \n",
       "\n",
       "       destination_city_Mumbai  class_Business  class_Economy  \n",
       "0                        False           False           True  \n",
       "1                         True            True          False  \n",
       "2                        False           False           True  \n",
       "3                        False            True          False  \n",
       "4                        False           False           True  \n",
       "...                        ...             ...            ...  \n",
       "10500                    False           False           True  \n",
       "10501                     True           False           True  \n",
       "10502                    False           False           True  \n",
       "10503                    False            True          False  \n",
       "10504                    False           False           True  \n",
       "\n",
       "[10505 rows x 37 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_dummies = df_train_dummies.drop(categorical, axis=1)\n",
    "df_train_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "357158d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.786076</td>\n",
       "      <td>0.365562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.434927</td>\n",
       "      <td>0.808716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.925314</td>\n",
       "      <td>-0.077592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.031403</td>\n",
       "      <td>-0.816183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.379500</td>\n",
       "      <td>-0.446888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10500</th>\n",
       "      <td>0.641119</td>\n",
       "      <td>-1.111619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>-1.295689</td>\n",
       "      <td>-0.446888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10502</th>\n",
       "      <td>0.536690</td>\n",
       "      <td>0.070126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10503</th>\n",
       "      <td>2.289702</td>\n",
       "      <td>-1.776350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10504</th>\n",
       "      <td>-1.319359</td>\n",
       "      <td>-0.594606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10505 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration  days_left\n",
       "0     -0.786076   0.365562\n",
       "1     -1.434927   0.808716\n",
       "2     -0.925314  -0.077592\n",
       "3     -0.031403  -0.816183\n",
       "4     -0.379500  -0.446888\n",
       "...         ...        ...\n",
       "10500  0.641119  -1.111619\n",
       "10501 -1.295689  -0.446888\n",
       "10502  0.536690   0.070126\n",
       "10503  2.289702  -1.776350\n",
       "10504 -1.319359  -0.594606\n",
       "\n",
       "[10505 rows x 2 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler()\n",
    "\n",
    "after_std = std.fit_transform(df_train_numeric)\n",
    "after_std = pd.DataFrame(after_std, columns=df_train_numeric.columns)\n",
    "after_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ae8de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_numeric = df_train_numeric.drop('price', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0afd71e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "      <th>airline_AirAsia</th>\n",
       "      <th>airline_Air_India</th>\n",
       "      <th>airline_GO_FIRST</th>\n",
       "      <th>airline_Indigo</th>\n",
       "      <th>airline_SpiceJet</th>\n",
       "      <th>airline_Vistara</th>\n",
       "      <th>source_city_Bangalore</th>\n",
       "      <th>source_city_Chennai</th>\n",
       "      <th>...</th>\n",
       "      <th>arrival_time_Morning</th>\n",
       "      <th>arrival_time_Night</th>\n",
       "      <th>destination_city_Bangalore</th>\n",
       "      <th>destination_city_Chennai</th>\n",
       "      <th>destination_city_Delhi</th>\n",
       "      <th>destination_city_Hyderabad</th>\n",
       "      <th>destination_city_Kolkata</th>\n",
       "      <th>destination_city_Mumbai</th>\n",
       "      <th>class_Business</th>\n",
       "      <th>class_Economy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.786076</td>\n",
       "      <td>0.365562</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.434927</td>\n",
       "      <td>0.808716</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.925314</td>\n",
       "      <td>-0.077592</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.031403</td>\n",
       "      <td>-0.816183</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.379500</td>\n",
       "      <td>-0.446888</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10500</th>\n",
       "      <td>0.641119</td>\n",
       "      <td>-1.111619</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10501</th>\n",
       "      <td>-1.295689</td>\n",
       "      <td>-0.446888</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10502</th>\n",
       "      <td>0.536690</td>\n",
       "      <td>0.070126</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10503</th>\n",
       "      <td>2.289702</td>\n",
       "      <td>-1.776350</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10504</th>\n",
       "      <td>-1.319359</td>\n",
       "      <td>-0.594606</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10505 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration  days_left  airline_AirAsia  airline_Air_India  \\\n",
       "0     -0.786076   0.365562            False              False   \n",
       "1     -1.434927   0.808716            False              False   \n",
       "2     -0.925314  -0.077592            False              False   \n",
       "3     -0.031403  -0.816183            False               True   \n",
       "4     -0.379500  -0.446888            False              False   \n",
       "...         ...        ...              ...                ...   \n",
       "10500  0.641119  -1.111619            False              False   \n",
       "10501 -1.295689  -0.446888            False              False   \n",
       "10502  0.536690   0.070126             True              False   \n",
       "10503  2.289702  -1.776350            False              False   \n",
       "10504 -1.319359  -0.594606            False              False   \n",
       "\n",
       "       airline_GO_FIRST  airline_Indigo  airline_SpiceJet  airline_Vistara  \\\n",
       "0                 False           False             False             True   \n",
       "1                 False           False             False             True   \n",
       "2                 False            True             False            False   \n",
       "3                 False           False             False            False   \n",
       "4                 False            True             False            False   \n",
       "...                 ...             ...               ...              ...   \n",
       "10500             False           False             False             True   \n",
       "10501             False           False             False             True   \n",
       "10502             False           False             False            False   \n",
       "10503             False           False             False             True   \n",
       "10504             False           False              True            False   \n",
       "\n",
       "       source_city_Bangalore  source_city_Chennai  ...  arrival_time_Morning  \\\n",
       "0                      False                False  ...                 False   \n",
       "1                       True                False  ...                  True   \n",
       "2                      False                False  ...                 False   \n",
       "3                      False                False  ...                 False   \n",
       "4                      False                False  ...                 False   \n",
       "...                      ...                  ...  ...                   ...   \n",
       "10500                   True                False  ...                  True   \n",
       "10501                  False                False  ...                 False   \n",
       "10502                  False                 True  ...                 False   \n",
       "10503                  False                False  ...                 False   \n",
       "10504                   True                False  ...                 False   \n",
       "\n",
       "       arrival_time_Night  destination_city_Bangalore  \\\n",
       "0                   False                       False   \n",
       "1                   False                       False   \n",
       "2                   False                        True   \n",
       "3                   False                       False   \n",
       "4                   False                       False   \n",
       "...                   ...                         ...   \n",
       "10500               False                       False   \n",
       "10501                True                       False   \n",
       "10502               False                       False   \n",
       "10503                True                       False   \n",
       "10504                True                       False   \n",
       "\n",
       "       destination_city_Chennai  destination_city_Delhi  \\\n",
       "0                         False                    True   \n",
       "1                         False                   False   \n",
       "2                         False                   False   \n",
       "3                          True                   False   \n",
       "4                          True                   False   \n",
       "...                         ...                     ...   \n",
       "10500                      True                   False   \n",
       "10501                     False                   False   \n",
       "10502                     False                    True   \n",
       "10503                     False                   False   \n",
       "10504                     False                    True   \n",
       "\n",
       "       destination_city_Hyderabad  destination_city_Kolkata  \\\n",
       "0                           False                     False   \n",
       "1                           False                     False   \n",
       "2                           False                     False   \n",
       "3                           False                     False   \n",
       "4                           False                     False   \n",
       "...                           ...                       ...   \n",
       "10500                       False                     False   \n",
       "10501                       False                     False   \n",
       "10502                       False                     False   \n",
       "10503                       False                      True   \n",
       "10504                       False                     False   \n",
       "\n",
       "       destination_city_Mumbai  class_Business  class_Economy  \n",
       "0                        False           False           True  \n",
       "1                         True            True          False  \n",
       "2                        False           False           True  \n",
       "3                        False            True          False  \n",
       "4                        False           False           True  \n",
       "...                        ...             ...            ...  \n",
       "10500                    False           False           True  \n",
       "10501                     True           False           True  \n",
       "10502                    False           False           True  \n",
       "10503                    False            True          False  \n",
       "10504                    False           False           True  \n",
       "\n",
       "[10505 rows x 37 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_dummies[df_train_numeric.columns] = after_std\n",
    "df_train_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "26935b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         7056\n",
       "1        20760\n",
       "2         3671\n",
       "3        55983\n",
       "4         5220\n",
       "         ...  \n",
       "10500     9241\n",
       "10501     4499\n",
       "10502     2477\n",
       "10503    79148\n",
       "10504     4756\n",
       "Name: price, Length: 10505, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "967ec13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7353, 37)\n",
      "(3152, 37)\n",
      "(7353,)\n",
      "(3152,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train_dummies, df_label, test_size=0.3, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "52525b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn - Configure global settings and get information about the working environment.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __check_build (package)\n",
      "    _build_utils (package)\n",
      "    _built_with_meson\n",
      "    _config\n",
      "    _cyutility\n",
      "    _distributor_init\n",
      "    _isotonic\n",
      "    _loss (package)\n",
      "    _min_dependencies\n",
      "    base\n",
      "    calibration\n",
      "    cluster (package)\n",
      "    compose (package)\n",
      "    conftest\n",
      "    covariance (package)\n",
      "    cross_decomposition (package)\n",
      "    datasets (package)\n",
      "    decomposition (package)\n",
      "    discriminant_analysis\n",
      "    dummy\n",
      "    ensemble (package)\n",
      "    exceptions\n",
      "    experimental (package)\n",
      "    externals (package)\n",
      "    feature_extraction (package)\n",
      "    feature_selection (package)\n",
      "    frozen (package)\n",
      "    gaussian_process (package)\n",
      "    impute (package)\n",
      "    inspection (package)\n",
      "    isotonic\n",
      "    kernel_approximation\n",
      "    kernel_ridge\n",
      "    linear_model (package)\n",
      "    manifold (package)\n",
      "    metrics (package)\n",
      "    mixture (package)\n",
      "    model_selection (package)\n",
      "    multiclass\n",
      "    multioutput\n",
      "    naive_bayes\n",
      "    neighbors (package)\n",
      "    neural_network (package)\n",
      "    pipeline\n",
      "    preprocessing (package)\n",
      "    random_projection\n",
      "    semi_supervised (package)\n",
      "    svm (package)\n",
      "    tests (package)\n",
      "    tree (package)\n",
      "    utils (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    clone(estimator, *, safe=True)\n",
      "        Construct a new unfitted estimator with the same parameters.\n",
      "        \n",
      "        Clone does a deep copy of the model in an estimator\n",
      "        without actually copying attached data. It returns a new estimator\n",
      "        with the same parameters that has not been fitted on any data.\n",
      "        \n",
      "        .. versionchanged:: 1.3\n",
      "            Delegates to `estimator.__sklearn_clone__` if the method exists.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : {list, tuple, set} of estimator instance or a single             estimator instance\n",
      "            The estimator or group of estimators to be cloned.\n",
      "        safe : bool, default=True\n",
      "            If safe is False, clone will fall back to a deep copy on objects\n",
      "            that are not estimators. Ignored if `estimator.__sklearn_clone__`\n",
      "            exists.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        estimator : object\n",
      "            The deep copy of the input, an estimator if input is an estimator.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If the estimator's `random_state` parameter is an integer (or if the\n",
      "        estimator doesn't have a `random_state` parameter), an *exact clone* is\n",
      "        returned: the clone and the original estimator will give the exact same\n",
      "        results. Otherwise, *statistical clone* is returned: the clone might\n",
      "        return different results from the original estimator. More details can be\n",
      "        found in :ref:`randomness`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.base import clone\n",
      "        >>> from sklearn.linear_model import LogisticRegression\n",
      "        >>> X = [[-1, 0], [0, 1], [0, -1], [1, 0]]\n",
      "        >>> y = [0, 0, 1, 1]\n",
      "        >>> classifier = LogisticRegression().fit(X, y)\n",
      "        >>> cloned_classifier = clone(classifier)\n",
      "        >>> hasattr(classifier, \"classes_\")\n",
      "        True\n",
      "        >>> hasattr(cloned_classifier, \"classes_\")\n",
      "        False\n",
      "        >>> classifier is cloned_classifier\n",
      "        False\n",
      "    \n",
      "    config_context(*, assume_finite=None, working_memory=None, print_changed_only=None, display=None, pairwise_dist_chunk_size=None, enable_cython_pairwise_dist=None, array_api_dispatch=None, transform_output=None, enable_metadata_routing=None, skip_parameter_validation=None)\n",
      "        Context manager to temporarily change the global scikit-learn configuration.\n",
      "        \n",
      "        This context manager can be used to apply scikit-learn configuration changes within\n",
      "        the scope of the with statement. Once the context exits, the global configuration is\n",
      "        restored again.\n",
      "        \n",
      "        The default global configurations (which take effect when scikit-learn is imported)\n",
      "        are defined below in the parameter list.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=None\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error. If None, the existing configuration won't change.\n",
      "            Global default: False.\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. If None, the existing configuration won't change.\n",
      "            Global default: 1024.\n",
      "        \n",
      "        print_changed_only : bool, default=None\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()', but would print\n",
      "            'SVC(C=1.0, cache_size=200, ...)' with all the non-changed parameters\n",
      "            when False. If None, the existing configuration won't change.\n",
      "            Global default: True.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Global default configuration changed from False to True.\n",
      "        \n",
      "        display : {'text', 'diagram'}, default=None\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. If None, the existing configuration won't change.\n",
      "            Global default: 'diagram'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        pairwise_dist_chunk_size : int, default=None\n",
      "            The number of row vectors per chunk for the accelerated pairwise-\n",
      "            distances reduction backend. Global default: 256 (suitable for most of\n",
      "            modern laptops' caches and architectures).\n",
      "        \n",
      "            Intended for easier benchmarking and testing of scikit-learn internals.\n",
      "            End users are not expected to benefit from customizing this configuration\n",
      "            setting.\n",
      "        \n",
      "            .. versionadded:: 1.1\n",
      "        \n",
      "        enable_cython_pairwise_dist : bool, default=None\n",
      "            Use the accelerated pairwise-distances reduction backend when\n",
      "            possible. Global default: True.\n",
      "        \n",
      "            Intended for easier benchmarking and testing of scikit-learn internals.\n",
      "            End users are not expected to benefit from customizing this configuration\n",
      "            setting.\n",
      "        \n",
      "            .. versionadded:: 1.1\n",
      "        \n",
      "        array_api_dispatch : bool, default=None\n",
      "            Use Array API dispatching when inputs follow the Array API standard.\n",
      "            Global default: False.\n",
      "        \n",
      "            See the :ref:`User Guide <array_api>` for more details.\n",
      "        \n",
      "            .. versionadded:: 1.2\n",
      "        \n",
      "        transform_output : str, default=None\n",
      "            Configure output of `transform` and `fit_transform`.\n",
      "        \n",
      "            See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "            for an example on how to use the API.\n",
      "        \n",
      "            - `\"default\"`: Default output format of a transformer\n",
      "            - `\"pandas\"`: DataFrame output\n",
      "            - `\"polars\"`: Polars output\n",
      "            - `None`: Transform configuration is unchanged\n",
      "        \n",
      "            Global default: \"default\".\n",
      "        \n",
      "            .. versionadded:: 1.2\n",
      "            .. versionadded:: 1.4\n",
      "                `\"polars\"` option was added.\n",
      "        \n",
      "        enable_metadata_routing : bool, default=None\n",
      "            Enable metadata routing. By default this feature is disabled.\n",
      "        \n",
      "            Refer to :ref:`metadata routing user guide <metadata_routing>` for more\n",
      "            details.\n",
      "        \n",
      "            - `True`: Metadata routing is enabled\n",
      "            - `False`: Metadata routing is disabled, use the old syntax.\n",
      "            - `None`: Configuration is unchanged\n",
      "        \n",
      "            Global default: False.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "        \n",
      "        skip_parameter_validation : bool, default=None\n",
      "            If `True`, disable the validation of the hyper-parameters' types and values in\n",
      "            the fit method of estimators and for arguments passed to public helper\n",
      "            functions. It can save time in some situations but can lead to low level\n",
      "            crashes and exceptions with confusing error messages.\n",
      "            Global default: False.\n",
      "        \n",
      "            Note that for data parameters, such as `X` and `y`, only type validation is\n",
      "            skipped but validation with `check_array` will continue to run.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        None.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        All settings, not just those presently modified, will be returned to\n",
      "        their previous values when the context manager is exited.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import sklearn\n",
      "        >>> from sklearn.utils.validation import assert_all_finite\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     assert_all_finite([float('nan')])\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     with sklearn.config_context(assume_finite=False):\n",
      "        ...         assert_all_finite([float('nan')])\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        ValueError: Input contains NaN...\n",
      "    \n",
      "    get_config()\n",
      "        Retrieve the current scikit-learn configuration.\n",
      "        \n",
      "        This reflects the effective global configurations as established by default upon\n",
      "        library import, or modified via :func:`set_config` or :func:`config_context`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        config : dict\n",
      "            Keys are parameter names that can be passed to :func:`set_config`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import sklearn\n",
      "        >>> config = sklearn.get_config()\n",
      "        >>> config.keys()\n",
      "        dict_keys([...])\n",
      "    \n",
      "    set_config(assume_finite=None, working_memory=None, print_changed_only=None, display=None, pairwise_dist_chunk_size=None, enable_cython_pairwise_dist=None, array_api_dispatch=None, transform_output=None, enable_metadata_routing=None, skip_parameter_validation=None)\n",
      "        Set global scikit-learn configuration.\n",
      "        \n",
      "        These settings control the behaviour of scikit-learn functions during a library\n",
      "        usage session. Global configuration defaults (as described in the parameter list\n",
      "        below) take effect when scikit-learn is imported.\n",
      "        \n",
      "        This function can be used to modify the global scikit-learn configuration at\n",
      "        runtime. Passing `None` as an argument (the default) leaves the corresponding\n",
      "        setting unchanged. This allows users to selectively update the global configuration\n",
      "        values without affecting the others.\n",
      "        \n",
      "        .. versionadded:: 0.19\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=None\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error. Global default: False.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. Global default: 1024.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        print_changed_only : bool, default=None\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()' while the default\n",
      "            behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n",
      "            all the non-changed parameters. Global default: True.\n",
      "        \n",
      "            .. versionadded:: 0.21\n",
      "            .. versionchanged:: 0.23\n",
      "               Global default configuration changed from False to True.\n",
      "        \n",
      "        display : {'text', 'diagram'}, default=None\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. Global default: 'diagram'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        pairwise_dist_chunk_size : int, default=None\n",
      "            The number of row vectors per chunk for the accelerated pairwise-\n",
      "            distances reduction backend. Global default: 256 (suitable for most of\n",
      "            modern laptops' caches and architectures).\n",
      "        \n",
      "            Intended for easier benchmarking and testing of scikit-learn internals.\n",
      "            End users are not expected to benefit from customizing this configuration\n",
      "            setting.\n",
      "        \n",
      "            .. versionadded:: 1.1\n",
      "        \n",
      "        enable_cython_pairwise_dist : bool, default=None\n",
      "            Use the accelerated pairwise-distances reduction backend when\n",
      "            possible. Global default: True.\n",
      "        \n",
      "            Intended for easier benchmarking and testing of scikit-learn internals.\n",
      "            End users are not expected to benefit from customizing this configuration\n",
      "            setting.\n",
      "        \n",
      "            .. versionadded:: 1.1\n",
      "        \n",
      "        array_api_dispatch : bool, default=None\n",
      "            Use Array API dispatching when inputs follow the Array API standard.\n",
      "            Global default: False.\n",
      "        \n",
      "            See the :ref:`User Guide <array_api>` for more details.\n",
      "        \n",
      "            .. versionadded:: 1.2\n",
      "        \n",
      "        transform_output : str, default=None\n",
      "            Configure output of `transform` and `fit_transform`.\n",
      "        \n",
      "            See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "            for an example on how to use the API.\n",
      "        \n",
      "            - `\"default\"`: Default output format of a transformer\n",
      "            - `\"pandas\"`: DataFrame output\n",
      "            - `\"polars\"`: Polars output\n",
      "            - `None`: Transform configuration is unchanged\n",
      "        \n",
      "            Global default: \"default\".\n",
      "        \n",
      "            .. versionadded:: 1.2\n",
      "            .. versionadded:: 1.4\n",
      "                `\"polars\"` option was added.\n",
      "        \n",
      "        enable_metadata_routing : bool, default=None\n",
      "            Enable metadata routing. By default this feature is disabled.\n",
      "        \n",
      "            Refer to :ref:`metadata routing user guide <metadata_routing>` for more\n",
      "            details.\n",
      "        \n",
      "            - `True`: Metadata routing is enabled\n",
      "            - `False`: Metadata routing is disabled, use the old syntax.\n",
      "            - `None`: Configuration is unchanged\n",
      "        \n",
      "            Global default: False.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "        \n",
      "        skip_parameter_validation : bool, default=None\n",
      "            If `True`, disable the validation of the hyper-parameters' types and values in\n",
      "            the fit method of estimators and for arguments passed to public helper\n",
      "            functions. It can save time in some situations but can lead to low level\n",
      "            crashes and exceptions with confusing error messages.\n",
      "            Global default: False.\n",
      "        \n",
      "            Note that for data parameters, such as `X` and `y`, only type validation is\n",
      "            skipped but validation with `check_array` will continue to run.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import set_config\n",
      "        >>> set_config(display='diagram')  # doctest: +SKIP\n",
      "    \n",
      "    show_versions()\n",
      "        Print useful debugging information\"\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import show_versions\n",
      "        >>> show_versions()  # doctest: +SKIP\n",
      "\n",
      "VERSION\n",
      "    1.7.2\n",
      "\n",
      "FILE\n",
      "    c:\\project\\bigdata_cert\\.venv\\lib\\site-packages\\sklearn\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3620c249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.model_selection in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.model_selection - Tools for model selection, such as cross validation and hyper-parameter tuning.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _classification_threshold\n",
      "    _plot\n",
      "    _search\n",
      "    _search_successive_halving\n",
      "    _split\n",
      "    _validation\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        sklearn.model_selection._search.ParameterGrid\n",
      "        sklearn.model_selection._search.ParameterSampler\n",
      "    sklearn.model_selection._classification_threshold.BaseThresholdClassifier(sklearn.base.ClassifierMixin, sklearn.base.MetaEstimatorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.model_selection._classification_threshold.FixedThresholdClassifier\n",
      "        sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV\n",
      "    sklearn.model_selection._plot._BaseCurveDisplay(builtins.object)\n",
      "        sklearn.model_selection._plot.LearningCurveDisplay\n",
      "        sklearn.model_selection._plot.ValidationCurveDisplay\n",
      "    sklearn.model_selection._search.BaseSearchCV(sklearn.base.MetaEstimatorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.model_selection._search.GridSearchCV\n",
      "        sklearn.model_selection._search.RandomizedSearchCV\n",
      "    sklearn.model_selection._split.GroupsConsumerMixin(sklearn.utils._metadata_requests._MetadataRequester)\n",
      "        sklearn.model_selection._split.GroupKFold(sklearn.model_selection._split.GroupsConsumerMixin, sklearn.model_selection._split._BaseKFold)\n",
      "        sklearn.model_selection._split.GroupShuffleSplit(sklearn.model_selection._split.GroupsConsumerMixin, sklearn.model_selection._split.BaseShuffleSplit)\n",
      "        sklearn.model_selection._split.LeaveOneGroupOut(sklearn.model_selection._split.GroupsConsumerMixin, sklearn.model_selection._split.BaseCrossValidator)\n",
      "        sklearn.model_selection._split.LeavePGroupsOut(sklearn.model_selection._split.GroupsConsumerMixin, sklearn.model_selection._split.BaseCrossValidator)\n",
      "        sklearn.model_selection._split.StratifiedGroupKFold(sklearn.model_selection._split.GroupsConsumerMixin, sklearn.model_selection._split._BaseKFold)\n",
      "    sklearn.model_selection._split._BaseKFold(sklearn.model_selection._split.BaseCrossValidator)\n",
      "        sklearn.model_selection._split.GroupKFold(sklearn.model_selection._split.GroupsConsumerMixin, sklearn.model_selection._split._BaseKFold)\n",
      "        sklearn.model_selection._split.KFold(sklearn.model_selection._split._UnsupportedGroupCVMixin, sklearn.model_selection._split._BaseKFold)\n",
      "        sklearn.model_selection._split.StratifiedGroupKFold(sklearn.model_selection._split.GroupsConsumerMixin, sklearn.model_selection._split._BaseKFold)\n",
      "        sklearn.model_selection._split.StratifiedKFold\n",
      "        sklearn.model_selection._split.TimeSeriesSplit\n",
      "    sklearn.model_selection._split._RepeatedSplits(sklearn.utils._metadata_requests._MetadataRequester)\n",
      "        sklearn.model_selection._split.RepeatedKFold(sklearn.model_selection._split._UnsupportedGroupCVMixin, sklearn.model_selection._split._RepeatedSplits)\n",
      "        sklearn.model_selection._split.RepeatedStratifiedKFold(sklearn.model_selection._split._UnsupportedGroupCVMixin, sklearn.model_selection._split._RepeatedSplits)\n",
      "    sklearn.model_selection._split._UnsupportedGroupCVMixin(builtins.object)\n",
      "        sklearn.model_selection._split.KFold(sklearn.model_selection._split._UnsupportedGroupCVMixin, sklearn.model_selection._split._BaseKFold)\n",
      "        sklearn.model_selection._split.LeaveOneOut(sklearn.model_selection._split._UnsupportedGroupCVMixin, sklearn.model_selection._split.BaseCrossValidator)\n",
      "        sklearn.model_selection._split.LeavePOut(sklearn.model_selection._split._UnsupportedGroupCVMixin, sklearn.model_selection._split.BaseCrossValidator)\n",
      "        sklearn.model_selection._split.RepeatedKFold(sklearn.model_selection._split._UnsupportedGroupCVMixin, sklearn.model_selection._split._RepeatedSplits)\n",
      "        sklearn.model_selection._split.RepeatedStratifiedKFold(sklearn.model_selection._split._UnsupportedGroupCVMixin, sklearn.model_selection._split._RepeatedSplits)\n",
      "        sklearn.model_selection._split.ShuffleSplit(sklearn.model_selection._split._UnsupportedGroupCVMixin, sklearn.model_selection._split.BaseShuffleSplit)\n",
      "    sklearn.utils._metadata_requests._MetadataRequester(builtins.object)\n",
      "        sklearn.model_selection._split.BaseCrossValidator\n",
      "            sklearn.model_selection._split.LeaveOneGroupOut(sklearn.model_selection._split.GroupsConsumerMixin, sklearn.model_selection._split.BaseCrossValidator)\n",
      "            sklearn.model_selection._split.LeaveOneOut(sklearn.model_selection._split._UnsupportedGroupCVMixin, sklearn.model_selection._split.BaseCrossValidator)\n",
      "            sklearn.model_selection._split.LeavePGroupsOut(sklearn.model_selection._split.GroupsConsumerMixin, sklearn.model_selection._split.BaseCrossValidator)\n",
      "            sklearn.model_selection._split.LeavePOut(sklearn.model_selection._split._UnsupportedGroupCVMixin, sklearn.model_selection._split.BaseCrossValidator)\n",
      "            sklearn.model_selection._split.PredefinedSplit\n",
      "        sklearn.model_selection._split.BaseShuffleSplit\n",
      "            sklearn.model_selection._split.GroupShuffleSplit(sklearn.model_selection._split.GroupsConsumerMixin, sklearn.model_selection._split.BaseShuffleSplit)\n",
      "            sklearn.model_selection._split.ShuffleSplit(sklearn.model_selection._split._UnsupportedGroupCVMixin, sklearn.model_selection._split.BaseShuffleSplit)\n",
      "            sklearn.model_selection._split.StratifiedShuffleSplit\n",
      "    \n",
      "    class BaseCrossValidator(sklearn.utils._metadata_requests._MetadataRequester)\n",
      "     |  Base class for all cross-validators.\n",
      "     |  \n",
      "     |  Implementations must define `_iter_test_masks` or `_iter_test_indices`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BaseCrossValidator\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'get_n_splits'})\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class BaseShuffleSplit(sklearn.utils._metadata_requests._MetadataRequester)\n",
      "     |  BaseShuffleSplit(n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |  \n",
      "     |  Base class for *ShuffleSplit.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=10\n",
      "     |      Number of re-shuffling & splitting iterations.\n",
      "     |  \n",
      "     |  test_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "     |      of the dataset to include in the test split. If int, represents the\n",
      "     |      absolute number of test samples. If None, the value is set to the\n",
      "     |      complement of the train size. If ``train_size`` is also None, it will\n",
      "     |      be set to 0.1.\n",
      "     |  \n",
      "     |  train_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the dataset to include in the train split. If\n",
      "     |      int, represents the absolute number of train samples. If None,\n",
      "     |      the value is automatically set to the complement of the test size.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of the training and testing indices produced.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BaseShuffleSplit\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Randomized CV splitters may return different results for each call of\n",
      "     |      split. You can make the results identical by setting `random_state`\n",
      "     |      to an integer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class FixedThresholdClassifier(BaseThresholdClassifier)\n",
      "     |  FixedThresholdClassifier(estimator, *, threshold='auto', pos_label=None, response_method='auto')\n",
      "     |  \n",
      "     |  Binary classifier that manually sets the decision threshold.\n",
      "     |  \n",
      "     |  This classifier allows to change the default decision threshold used for\n",
      "     |  converting posterior probability estimates (i.e. output of `predict_proba`) or\n",
      "     |  decision scores (i.e. output of `decision_function`) into a class label.\n",
      "     |  \n",
      "     |  Here, the threshold is not optimized and is set to a constant value.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <FixedThresholdClassifier>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.5\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : estimator instance\n",
      "     |      The binary classifier, fitted or not, for which we want to optimize\n",
      "     |      the decision threshold used during `predict`.\n",
      "     |  \n",
      "     |  threshold : {\"auto\"} or float, default=\"auto\"\n",
      "     |      The decision threshold to use when converting posterior probability estimates\n",
      "     |      (i.e. output of `predict_proba`) or decision scores (i.e. output of\n",
      "     |      `decision_function`) into a class label. When `\"auto\"`, the threshold is set\n",
      "     |      to 0.5 if `predict_proba` is used as `response_method`, otherwise it is set to\n",
      "     |      0 (i.e. the default threshold for `decision_function`).\n",
      "     |  \n",
      "     |  pos_label : int, float, bool or str, default=None\n",
      "     |      The label of the positive class. Used to process the output of the\n",
      "     |      `response_method` method. When `pos_label=None`, if `y_true` is in `{-1, 1}` or\n",
      "     |      `{0, 1}`, `pos_label` is set to 1, otherwise an error will be raised.\n",
      "     |  \n",
      "     |  response_method : {\"auto\", \"decision_function\", \"predict_proba\"}, default=\"auto\"\n",
      "     |      Methods by the classifier `estimator` corresponding to the\n",
      "     |      decision function for which we want to find a threshold. It can be:\n",
      "     |  \n",
      "     |      * if `\"auto\"`, it will try to invoke `\"predict_proba\"` or `\"decision_function\"`\n",
      "     |        in that order.\n",
      "     |      * otherwise, one of `\"predict_proba\"` or `\"decision_function\"`.\n",
      "     |        If the method is not implemented by the classifier, it will raise an\n",
      "     |        error.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : estimator instance\n",
      "     |      The fitted classifier used when predicting.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The class labels.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`. Only defined if the\n",
      "     |      underlying estimator exposes such an attribute when fit.\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Only defined if the\n",
      "     |      underlying estimator exposes such an attribute when fit.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.model_selection.TunedThresholdClassifierCV : Classifier that post-tunes\n",
      "     |      the decision threshold based on some metrics and using cross-validation.\n",
      "     |  sklearn.calibration.CalibratedClassifierCV : Estimator that calibrates\n",
      "     |      probabilities.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> from sklearn.metrics import confusion_matrix\n",
      "     |  >>> from sklearn.model_selection import FixedThresholdClassifier, train_test_split\n",
      "     |  >>> X, y = make_classification(\n",
      "     |  ...     n_samples=1_000, weights=[0.9, 0.1], class_sep=0.8, random_state=42\n",
      "     |  ... )\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...     X, y, stratify=y, random_state=42\n",
      "     |  ... )\n",
      "     |  >>> classifier = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
      "     |  >>> print(confusion_matrix(y_test, classifier.predict(X_test)))\n",
      "     |  [[217   7]\n",
      "     |   [ 19   7]]\n",
      "     |  >>> classifier_other_threshold = FixedThresholdClassifier(\n",
      "     |  ...     classifier, threshold=0.1, response_method=\"predict_proba\"\n",
      "     |  ... ).fit(X_train, y_train)\n",
      "     |  >>> print(confusion_matrix(y_test, classifier_other_threshold.predict(X_test)))\n",
      "     |  [[184  40]\n",
      "     |   [  6  20]]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FixedThresholdClassifier\n",
      "     |      BaseThresholdClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator, *, threshold='auto', pos_label=None, response_method='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict the target of new samples.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The samples, as accepted by `estimator.predict`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      class_labels : ndarray of shape (n_samples,)\n",
      "     |          The predicted class.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.model_selection._classification_threshold.FixedThresholdClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.model_selection._classification_threshold.FixedThresholdClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |      Classes labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseThresholdClassifier:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Decision function for samples in `X` using the fitted estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      decisions : ndarray of shape (n_samples,)\n",
      "     |          The decision function computed the fitted estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, **params)\n",
      "     |      Fit the classifier.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      **params : dict\n",
      "     |          Parameters to pass to the `fit` method of the underlying\n",
      "     |          classifier.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict logarithm class probabilities for `X` using the fitted estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      log_probabilities : ndarray of shape (n_samples, n_classes)\n",
      "     |          The logarithm class probabilities of the input samples.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for `X` using the fitted estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      probabilities : ndarray of shape (n_samples, n_classes)\n",
      "     |          The class probabilities of the input samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class GridSearchCV(BaseSearchCV)\n",
      "     |  GridSearchCV(estimator, param_grid, *, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
      "     |  \n",
      "     |  Exhaustive search over specified parameter values for an estimator.\n",
      "     |  \n",
      "     |  Important members are fit, predict.\n",
      "     |  \n",
      "     |  GridSearchCV implements a \"fit\" and a \"score\" method.\n",
      "     |  It also implements \"score_samples\", \"predict\", \"predict_proba\",\n",
      "     |  \"decision_function\", \"transform\" and \"inverse_transform\" if they are\n",
      "     |  implemented in the estimator used.\n",
      "     |  \n",
      "     |  The parameters of the estimator used to apply these methods are optimized\n",
      "     |  by cross-validated grid-search over a parameter grid.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <grid_search>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : estimator object\n",
      "     |      This is assumed to implement the scikit-learn estimator interface.\n",
      "     |      Either estimator needs to provide a ``score`` function,\n",
      "     |      or ``scoring`` must be passed.\n",
      "     |  \n",
      "     |  param_grid : dict or list of dictionaries\n",
      "     |      Dictionary with parameters names (`str`) as keys and lists of\n",
      "     |      parameter settings to try as values, or a list of such\n",
      "     |      dictionaries, in which case the grids spanned by each dictionary\n",
      "     |      in the list are explored. This enables searching over any sequence\n",
      "     |      of parameter settings.\n",
      "     |  \n",
      "     |  scoring : str, callable, list, tuple or dict, default=None\n",
      "     |      Strategy to evaluate the performance of the cross-validated model on\n",
      "     |      the test set.\n",
      "     |  \n",
      "     |      If `scoring` represents a single score, one can use:\n",
      "     |  \n",
      "     |      - a single string (see :ref:`scoring_string_names`);\n",
      "     |      - a callable (see :ref:`scoring_callable`) that returns a single value;\n",
      "     |      - `None`, the `estimator`'s\n",
      "     |        :ref:`default evaluation criterion <scoring_api_overview>` is used.\n",
      "     |  \n",
      "     |      If `scoring` represents multiple scores, one can use:\n",
      "     |  \n",
      "     |      - a list or tuple of unique strings;\n",
      "     |      - a callable returning a dictionary where the keys are the metric\n",
      "     |        names and the values are the metric scores;\n",
      "     |      - a dictionary with metric names as keys and callables as values.\n",
      "     |  \n",
      "     |      See :ref:`multimetric_grid_search` for an example.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of jobs to run in parallel.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |      .. versionchanged:: v0.20\n",
      "     |         `n_jobs` default changed from 1 to None\n",
      "     |  \n",
      "     |  refit : bool, str, or callable, default=True\n",
      "     |      Refit an estimator using the best found parameters on the whole\n",
      "     |      dataset.\n",
      "     |  \n",
      "     |      For multiple metric evaluation, this needs to be a `str` denoting the\n",
      "     |      scorer that would be used to find the best parameters for refitting\n",
      "     |      the estimator at the end.\n",
      "     |  \n",
      "     |      Where there are considerations other than maximum score in\n",
      "     |      choosing a best estimator, ``refit`` can be set to a function which\n",
      "     |      returns the selected ``best_index_`` given ``cv_results_``. In that\n",
      "     |      case, the ``best_estimator_`` and ``best_params_`` will be set\n",
      "     |      according to the returned ``best_index_`` while the ``best_score_``\n",
      "     |      attribute will not be available.\n",
      "     |  \n",
      "     |      The refitted estimator is made available at the ``best_estimator_``\n",
      "     |      attribute and permits using ``predict`` directly on this\n",
      "     |      ``GridSearchCV`` instance.\n",
      "     |  \n",
      "     |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      "     |      ``best_score_`` and ``best_params_`` will only be available if\n",
      "     |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
      "     |      scorer.\n",
      "     |  \n",
      "     |      See ``scoring`` parameter to know more about multiple metric\n",
      "     |      evaluation.\n",
      "     |  \n",
      "     |      See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`\n",
      "     |      to see how to design a custom selection strategy using a callable\n",
      "     |      via `refit`.\n",
      "     |  \n",
      "     |      See :ref:`this example\n",
      "     |      <sphx_glr_auto_examples_model_selection_plot_grid_search_refit_callable.py>`\n",
      "     |      for an example of how to use ``refit=callable`` to balance model\n",
      "     |      complexity and cross-validated score.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |          Support for callable added.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross validation,\n",
      "     |      - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      "     |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "     |      other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "     |      with `shuffle=False` so the splits will be the same across calls.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  verbose : int\n",
      "     |      Controls the verbosity: the higher, the more messages.\n",
      "     |  \n",
      "     |      - >1 : the computation time for each fold and parameter candidate is\n",
      "     |        displayed;\n",
      "     |      - >2 : the score is also displayed;\n",
      "     |      - >3 : the fold and candidate parameter indexes are also displayed\n",
      "     |        together with the starting time of the computation.\n",
      "     |  \n",
      "     |  pre_dispatch : int, or str, default='2*n_jobs'\n",
      "     |      Controls the number of jobs that get dispatched during parallel\n",
      "     |      execution. Reducing this number can be useful to avoid an\n",
      "     |      explosion of memory consumption when more jobs get dispatched\n",
      "     |      than CPUs can process. This parameter can be:\n",
      "     |  \n",
      "     |      - None, in which case all the jobs are immediately created and spawned. Use\n",
      "     |        this for lightweight and fast-running jobs, to avoid delays due to on-demand\n",
      "     |        spawning of the jobs\n",
      "     |      - An int, giving the exact number of total jobs that are spawned\n",
      "     |      - A str, giving an expression as a function of n_jobs, as in '2*n_jobs'\n",
      "     |  \n",
      "     |  error_score : 'raise' or numeric, default=np.nan\n",
      "     |      Value to assign to the score if an error occurs in estimator fitting.\n",
      "     |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      "     |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      "     |      step, which will always raise the error.\n",
      "     |  \n",
      "     |  return_train_score : bool, default=False\n",
      "     |      If ``False``, the ``cv_results_`` attribute will not include training\n",
      "     |      scores.\n",
      "     |      Computing training scores is used to get insights on how different\n",
      "     |      parameter settings impact the overfitting/underfitting trade-off.\n",
      "     |      However computing the scores on the training set can be computationally\n",
      "     |      expensive and is not strictly required to select the parameters that\n",
      "     |      yield the best generalization performance.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.21\n",
      "     |          Default value was changed from ``True`` to ``False``\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_results_ : dict of numpy (masked) ndarrays\n",
      "     |      A dict with keys as column headers and values as columns, that can be\n",
      "     |      imported into a pandas ``DataFrame``.\n",
      "     |  \n",
      "     |      For instance the below given table\n",
      "     |  \n",
      "     |      +------------+-----------+------------+-----------------+---+---------+\n",
      "     |      |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n",
      "     |      +============+===========+============+=================+===+=========+\n",
      "     |      |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n",
      "     |      +------------+-----------+------------+-----------------+---+---------+\n",
      "     |      |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n",
      "     |      +------------+-----------+------------+-----------------+---+---------+\n",
      "     |      |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n",
      "     |      +------------+-----------+------------+-----------------+---+---------+\n",
      "     |      |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n",
      "     |      +------------+-----------+------------+-----------------+---+---------+\n",
      "     |  \n",
      "     |      will be represented by a ``cv_results_`` dict of::\n",
      "     |  \n",
      "     |          {\n",
      "     |          'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n",
      "     |                                       mask = [False False False False]...)\n",
      "     |          'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
      "     |                                      mask = [ True  True False False]...),\n",
      "     |          'param_degree': masked_array(data = [2.0 3.0 -- --],\n",
      "     |                                       mask = [False False  True  True]...),\n",
      "     |          'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
      "     |          'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
      "     |          'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
      "     |          'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
      "     |          'rank_test_score'    : [2, 4, 3, 1],\n",
      "     |          'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
      "     |          'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
      "     |          'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
      "     |          'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
      "     |          'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
      "     |          'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
      "     |          'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n",
      "     |          'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n",
      "     |          'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
      "     |          }\n",
      "     |  \n",
      "     |      NOTE\n",
      "     |  \n",
      "     |      The key ``'params'`` is used to store a list of parameter\n",
      "     |      settings dicts for all the parameter candidates.\n",
      "     |  \n",
      "     |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      "     |      ``std_score_time`` are all in seconds.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, the scores for all the scorers are\n",
      "     |      available in the ``cv_results_`` dict at the keys ending with that\n",
      "     |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      "     |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      "     |  \n",
      "     |  best_estimator_ : estimator\n",
      "     |      Estimator that was chosen by the search, i.e. estimator\n",
      "     |      which gave highest score (or smallest loss if specified)\n",
      "     |      on the left out data. Not available if ``refit=False``.\n",
      "     |  \n",
      "     |      See ``refit`` parameter for more information on allowed values.\n",
      "     |  \n",
      "     |  best_score_ : float\n",
      "     |      Mean cross-validated score of the best_estimator\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      "     |      specified.\n",
      "     |  \n",
      "     |      This attribute is not available if ``refit`` is a function.\n",
      "     |  \n",
      "     |  best_params_ : dict\n",
      "     |      Parameter setting that gave the best results on the hold out data.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      "     |      specified.\n",
      "     |  \n",
      "     |  best_index_ : int\n",
      "     |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      "     |      candidate parameter setting.\n",
      "     |  \n",
      "     |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      "     |      the parameter setting for the best model, that gives the highest\n",
      "     |      mean score (``search.best_score_``).\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      "     |      specified.\n",
      "     |  \n",
      "     |  scorer_ : function or a dict\n",
      "     |      Scorer function used on the held out data to choose the best\n",
      "     |      parameters for the model.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this attribute holds the validated\n",
      "     |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      "     |  \n",
      "     |  n_splits_ : int\n",
      "     |      The number of cross-validation splits (folds/iterations).\n",
      "     |  \n",
      "     |  refit_time_ : float\n",
      "     |      Seconds used for refitting the best model on the whole dataset.\n",
      "     |  \n",
      "     |      This is present only if ``refit`` is not False.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  multimetric_ : bool\n",
      "     |      Whether or not the scorers compute several metrics.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels. This is present only if ``refit`` is specified and\n",
      "     |      the underlying estimator is a classifier.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`. Only defined if\n",
      "     |      `best_estimator_` is defined (see the documentation for the `refit`\n",
      "     |      parameter for more details) and that `best_estimator_` exposes\n",
      "     |      `n_features_in_` when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Only defined if\n",
      "     |      `best_estimator_` is defined (see the documentation for the `refit`\n",
      "     |      parameter for more details) and that `best_estimator_` exposes\n",
      "     |      `feature_names_in_` when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  ParameterGrid : Generates all the combinations of a hyperparameter grid.\n",
      "     |  train_test_split : Utility function to split the data into a development\n",
      "     |      set usable for fitting a GridSearchCV instance and an evaluation set\n",
      "     |      for its final evaluation.\n",
      "     |  sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n",
      "     |      loss function.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The parameters selected are those that maximize the score of the left out\n",
      "     |  data, unless an explicit score is passed in which case it is used instead.\n",
      "     |  \n",
      "     |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      "     |  point in the grid (and not `n_jobs` times). This is done for efficiency\n",
      "     |  reasons if individual jobs take very little time, but may raise errors if\n",
      "     |  the dataset is large and not enough memory is available.  A workaround in\n",
      "     |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      "     |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      "     |  n_jobs`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import svm, datasets\n",
      "     |  >>> from sklearn.model_selection import GridSearchCV\n",
      "     |  >>> iris = datasets.load_iris()\n",
      "     |  >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "     |  >>> svc = svm.SVC()\n",
      "     |  >>> clf = GridSearchCV(svc, parameters)\n",
      "     |  >>> clf.fit(iris.data, iris.target)\n",
      "     |  GridSearchCV(estimator=SVC(),\n",
      "     |               param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n",
      "     |  >>> sorted(clf.cv_results_.keys())\n",
      "     |  ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
      "     |   'param_C', 'param_kernel', 'params',...\n",
      "     |   'rank_test_score', 'split0_test_score',...\n",
      "     |   'split2_test_score', ...\n",
      "     |   'std_fit_time', 'std_score_time', 'std_test_score']\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GridSearchCV\n",
      "     |      BaseSearchCV\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator, param_grid, *, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSearchCV:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Call decision_function on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``decision_function``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_score : ndarray of shape (n_samples,) or (n_samples, n_classes)                 or (n_samples, n_classes * (n_classes-1) / 2)\n",
      "     |          Result of the decision function for `X` based on the estimator with\n",
      "     |          the best found parameters.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, **params)\n",
      "     |      Run fit with all sets of parameters.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      \n",
      "     |      X : array-like of shape (n_samples, n_features) or (n_samples, n_samples)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features. For precomputed kernel or\n",
      "     |          distance matrix, the expected shape of X is (n_samples, n_samples).\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      "     |          Target relative to X for classification or regression;\n",
      "     |          None for unsupervised learning.\n",
      "     |      \n",
      "     |      **params : dict of str -> object\n",
      "     |          Parameters passed to the ``fit`` method of the estimator, the scorer,\n",
      "     |          and the CV splitter.\n",
      "     |      \n",
      "     |          If a fit parameter is an array-like whose length is equal to\n",
      "     |          `num_samples` then it will be split by cross-validation along with\n",
      "     |          `X` and `y`. For example, the :term:`sample_weight` parameter is\n",
      "     |          split because `len(sample_weights) = len(X)`. However, this behavior\n",
      "     |          does not apply to `groups` which is passed to the splitter configured\n",
      "     |          via the `cv` parameter of the constructor. Thus, `groups` is used\n",
      "     |          *to perform the split* and determines which samples are\n",
      "     |          assigned to the each side of the a split.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Instance of fitted estimator.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Call inverse_transform on the estimator with the best found params.\n",
      "     |      \n",
      "     |      Only available if the underlying estimator implements\n",
      "     |      ``inverse_transform`` and ``refit=True``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_original : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Result of the `inverse_transform` function for `X` based on the\n",
      "     |          estimator with the best found parameters.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Call predict on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          The predicted labels or values for `X` based on the estimator with\n",
      "     |          the best found parameters.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Call predict_log_proba on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict_log_proba``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Predicted class log-probabilities for `X` based on the estimator\n",
      "     |          with the best found parameters. The order of the classes\n",
      "     |          corresponds to that in the fitted attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Call predict_proba on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict_proba``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Predicted class probabilities for `X` based on the estimator with\n",
      "     |          the best found parameters. The order of the classes corresponds\n",
      "     |          to that in the fitted attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  score(self, X, y=None, **params)\n",
      "     |      Return the score on the given data, if the estimator has been refit.\n",
      "     |      \n",
      "     |      This uses the score defined by ``scoring`` where provided, and the\n",
      "     |      ``best_estimator_.score`` method otherwise.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      "     |          Target relative to X for classification or regression;\n",
      "     |          None for unsupervised learning.\n",
      "     |      \n",
      "     |      **params : dict\n",
      "     |          Parameters to be passed to the underlying scorer(s).\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |              Only available if `enable_metadata_routing=True`. See\n",
      "     |              :ref:`Metadata Routing User Guide <metadata_routing>` for more\n",
      "     |              details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          The score defined by ``scoring`` if provided, and the\n",
      "     |          ``best_estimator_.score`` method otherwise.\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Call score_samples on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``score_samples``.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.24\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : iterable\n",
      "     |          Data to predict on. Must fulfill input requirements\n",
      "     |          of the underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_score : ndarray of shape (n_samples,)\n",
      "     |          The ``best_estimator_.score_samples`` method.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Call transform on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if the underlying estimator supports ``transform`` and\n",
      "     |      ``refit=True``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          `X` transformed in the new space based on the estimator with\n",
      "     |          the best found parameters.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseSearchCV:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |      Class labels.\n",
      "     |      \n",
      "     |      Only available when `refit=True` and the estimator is a classifier.\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |      \n",
      "     |      Only available when `refit=True`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class GroupKFold(GroupsConsumerMixin, _BaseKFold)\n",
      "     |  GroupKFold(n_splits=5, *, shuffle=False, random_state=None)\n",
      "     |  \n",
      "     |  K-fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  Each group will appear exactly once in the test set across all folds (the\n",
      "     |  number of distinct groups has to be at least equal to the number of folds).\n",
      "     |  \n",
      "     |  The folds are approximately balanced in the sense that the number of\n",
      "     |  samples is approximately the same in each test fold when `shuffle` is True.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <group_k_fold>`.\n",
      "     |  \n",
      "     |  For visualisation of cross-validation behaviour and\n",
      "     |  comparison between common scikit-learn split methods\n",
      "     |  refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``n_splits`` default value changed from 3 to 5.\n",
      "     |  \n",
      "     |  shuffle : bool, default=False\n",
      "     |      Whether to shuffle the groups before splitting into batches.\n",
      "     |      Note that the samples within each split will not be shuffled.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.6\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      When `shuffle` is True, `random_state` affects the ordering of the\n",
      "     |      indices, which controls the randomness of each fold. Otherwise, this\n",
      "     |      parameter has no effect.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.6\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Groups appear in an arbitrary order throughout the folds.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import GroupKFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
      "     |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      "     |  >>> groups = np.array([0, 0, 2, 2, 3, 3])\n",
      "     |  >>> group_kfold = GroupKFold(n_splits=2)\n",
      "     |  >>> group_kfold.get_n_splits(X, y, groups)\n",
      "     |  2\n",
      "     |  >>> print(group_kfold)\n",
      "     |  GroupKFold(n_splits=2, random_state=None, shuffle=False)\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(group_kfold.split(X, y, groups)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}, group={groups[train_index]}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}, group={groups[test_index]}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[2 3], group=[2 2]\n",
      "     |    Test:  index=[0 1 4 5], group=[0 0 3 3]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[0 1 4 5], group=[0 0 3 3]\n",
      "     |    Test:  index=[2 3], group=[2 2]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  LeaveOneGroupOut : For splitting the data according to explicit\n",
      "     |      domain-specific stratification of the dataset.\n",
      "     |  \n",
      "     |  StratifiedKFold : Takes class information into account to avoid building\n",
      "     |      folds with imbalanced class proportions (for binary or multiclass\n",
      "     |      classification tasks).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GroupKFold\n",
      "     |      GroupsConsumerMixin\n",
      "     |      _BaseKFold\n",
      "     |      BaseCrossValidator\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5, *, shuffle=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_split_request(self: sklearn.model_selection._split.GroupKFold, *, groups: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.model_selection._split.GroupKFold from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``split`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``split`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``split``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      groups : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``groups`` parameter in ``split``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseKFold:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class GroupShuffleSplit(GroupsConsumerMixin, BaseShuffleSplit)\n",
      "     |  GroupShuffleSplit(n_splits=5, *, test_size=None, train_size=None, random_state=None)\n",
      "     |  \n",
      "     |  Shuffle-Group(s)-Out cross-validation iterator.\n",
      "     |  \n",
      "     |  Provides randomized train/test indices to split data according to a\n",
      "     |  third-party provided group. This group information can be used to encode\n",
      "     |  arbitrary domain specific stratifications of the samples as integers.\n",
      "     |  \n",
      "     |  For instance the groups could be the year of collection of the samples\n",
      "     |  and thus allow for cross-validation against time-based splits.\n",
      "     |  \n",
      "     |  The difference between :class:`LeavePGroupsOut` and ``GroupShuffleSplit`` is that\n",
      "     |  the former generates splits using all subsets of size ``p`` unique groups,\n",
      "     |  whereas ``GroupShuffleSplit`` generates a user-determined number of random\n",
      "     |  test splits, each with a user-determined fraction of unique groups.\n",
      "     |  \n",
      "     |  For example, a less computationally intensive alternative to\n",
      "     |  ``LeavePGroupsOut(p=10)`` would be\n",
      "     |  ``GroupShuffleSplit(test_size=10, n_splits=100)``.\n",
      "     |  \n",
      "     |  Contrary to other cross-validation strategies, the random splits\n",
      "     |  do not guarantee that test sets across all folds will be mutually exclusive,\n",
      "     |  and might include overlapping samples. However, this is still very likely for\n",
      "     |  sizeable datasets.\n",
      "     |  \n",
      "     |  Note: The parameters ``test_size`` and ``train_size`` refer to groups, and\n",
      "     |  not to samples as in :class:`ShuffleSplit`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <group_shuffle_split>`.\n",
      "     |  \n",
      "     |  For visualisation of cross-validation behaviour and\n",
      "     |  comparison between common scikit-learn split methods\n",
      "     |  refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of re-shuffling & splitting iterations.\n",
      "     |  \n",
      "     |  test_size : float, int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "     |      of groups to include in the test split (rounded up). If int,\n",
      "     |      represents the absolute number of test groups. If None, the value is\n",
      "     |      set to the complement of the train size. If ``train_size`` is also None,\n",
      "     |      it will be set to 0.2.\n",
      "     |  \n",
      "     |  train_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the groups to include in the train split. If\n",
      "     |      int, represents the absolute number of train groups. If None,\n",
      "     |      the value is automatically set to the complement of the test size.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of the training and testing indices produced.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import GroupShuffleSplit\n",
      "     |  >>> X = np.ones(shape=(8, 2))\n",
      "     |  >>> y = np.ones(shape=(8, 1))\n",
      "     |  >>> groups = np.array([1, 1, 2, 2, 2, 3, 3, 3])\n",
      "     |  >>> print(groups.shape)\n",
      "     |  (8,)\n",
      "     |  >>> gss = GroupShuffleSplit(n_splits=2, train_size=.7, random_state=42)\n",
      "     |  >>> gss.get_n_splits()\n",
      "     |  2\n",
      "     |  >>> print(gss)\n",
      "     |  GroupShuffleSplit(n_splits=2, random_state=42, test_size=None, train_size=0.7)\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(gss.split(X, y, groups)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}, group={groups[train_index]}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}, group={groups[test_index]}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[2 3 4 5 6 7], group=[2 2 2 3 3 3]\n",
      "     |    Test:  index=[0 1], group=[1 1]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[0 1 5 6 7], group=[1 1 3 3 3]\n",
      "     |    Test:  index=[2 3 4], group=[2 2 2]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  ShuffleSplit : Shuffles samples to create independent test/train sets.\n",
      "     |  \n",
      "     |  LeavePGroupsOut : Train set leaves out all possible subsets of `p` groups.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GroupShuffleSplit\n",
      "     |      GroupsConsumerMixin\n",
      "     |      BaseShuffleSplit\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5, *, test_size=None, train_size=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_split_request(self: sklearn.model_selection._split.GroupShuffleSplit, *, groups: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.model_selection._split.GroupShuffleSplit from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``split`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``split`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``split``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      groups : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``groups`` parameter in ``split``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Randomized CV splitters may return different results for each call of\n",
      "     |      split. You can make the results identical by setting `random_state`\n",
      "     |      to an integer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class KFold(_UnsupportedGroupCVMixin, _BaseKFold)\n",
      "     |  KFold(n_splits=5, *, shuffle=False, random_state=None)\n",
      "     |  \n",
      "     |  K-Fold cross-validator.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train/test sets. Split\n",
      "     |  dataset into k consecutive folds (without shuffling by default).\n",
      "     |  \n",
      "     |  Each fold is then used once as a validation while the k - 1 remaining\n",
      "     |  folds form the training set.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <k_fold>`.\n",
      "     |  \n",
      "     |  For visualisation of cross-validation behaviour and\n",
      "     |  comparison between common scikit-learn split methods\n",
      "     |  refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``n_splits`` default value changed from 3 to 5.\n",
      "     |  \n",
      "     |  shuffle : bool, default=False\n",
      "     |      Whether to shuffle the data before splitting into batches.\n",
      "     |      Note that the samples within each split will not be shuffled.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      When `shuffle` is True, `random_state` affects the ordering of the\n",
      "     |      indices, which controls the randomness of each fold. Otherwise, this\n",
      "     |      parameter has no effect.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import KFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([1, 2, 3, 4])\n",
      "     |  >>> kf = KFold(n_splits=2)\n",
      "     |  >>> kf.get_n_splits(X)\n",
      "     |  2\n",
      "     |  >>> print(kf)\n",
      "     |  KFold(n_splits=2, random_state=None, shuffle=False)\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[2 3]\n",
      "     |    Test:  index=[0 1]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[0 1]\n",
      "     |    Test:  index=[2 3]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The first ``n_samples % n_splits`` folds have size\n",
      "     |  ``n_samples // n_splits + 1``, other folds have size\n",
      "     |  ``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n",
      "     |  \n",
      "     |  Randomized CV splitters may return different results for each call of\n",
      "     |  split. You can make the results identical by setting `random_state`\n",
      "     |  to an integer.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  StratifiedKFold : Takes class information into account to avoid building\n",
      "     |      folds with imbalanced class distributions (for binary or multiclass\n",
      "     |      classification tasks).\n",
      "     |  \n",
      "     |  GroupKFold : K-fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  RepeatedKFold : Repeats K-Fold n times.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KFold\n",
      "     |      _UnsupportedGroupCVMixin\n",
      "     |      _BaseKFold\n",
      "     |      BaseCrossValidator\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5, *, shuffle=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _UnsupportedGroupCVMixin:\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _UnsupportedGroupCVMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseKFold:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class LearningCurveDisplay(_BaseCurveDisplay)\n",
      "     |  LearningCurveDisplay(*, train_sizes, train_scores, test_scores, score_name=None)\n",
      "     |  \n",
      "     |  Learning Curve visualization.\n",
      "     |  \n",
      "     |  It is recommended to use\n",
      "     |  :meth:`~sklearn.model_selection.LearningCurveDisplay.from_estimator` to\n",
      "     |  create a :class:`~sklearn.model_selection.LearningCurveDisplay` instance.\n",
      "     |  All parameters are stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>` for general information\n",
      "     |  about the visualization API and\n",
      "     |  :ref:`detailed documentation <learning_curve>` regarding the learning\n",
      "     |  curve visualization.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.2\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  train_sizes : ndarray of shape (n_unique_ticks,)\n",
      "     |      Numbers of training examples that has been used to generate the\n",
      "     |      learning curve.\n",
      "     |  \n",
      "     |  train_scores : ndarray of shape (n_ticks, n_cv_folds)\n",
      "     |      Scores on training sets.\n",
      "     |  \n",
      "     |  test_scores : ndarray of shape (n_ticks, n_cv_folds)\n",
      "     |      Scores on test set.\n",
      "     |  \n",
      "     |  score_name : str, default=None\n",
      "     |      The name of the score used in `learning_curve`. It will override the name\n",
      "     |      inferred from the `scoring` parameter. If `score` is `None`, we use `\"Score\"` if\n",
      "     |      `negate_score` is `False` and `\"Negative score\"` otherwise. If `scoring` is a\n",
      "     |      string or a callable, we infer the name. We replace `_` by spaces and capitalize\n",
      "     |      the first letter. We remove `neg_` and replace it by `\"Negative\"` if\n",
      "     |      `negate_score` is `False` or just remove it otherwise.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with the learning curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the learning curve.\n",
      "     |  \n",
      "     |  errorbar_ : list of matplotlib Artist or None\n",
      "     |      When the `std_display_style` is `\"errorbar\"`, this is a list of\n",
      "     |      `matplotlib.container.ErrorbarContainer` objects. If another style is\n",
      "     |      used, `errorbar_` is `None`.\n",
      "     |  \n",
      "     |  lines_ : list of matplotlib Artist or None\n",
      "     |      When the `std_display_style` is `\"fill_between\"`, this is a list of\n",
      "     |      `matplotlib.lines.Line2D` objects corresponding to the mean train and\n",
      "     |      test scores. If another style is used, `line_` is `None`.\n",
      "     |  \n",
      "     |  fill_between_ : list of matplotlib Artist or None\n",
      "     |      When the `std_display_style` is `\"fill_between\"`, this is a list of\n",
      "     |      `matplotlib.collections.PolyCollection` objects. If another style is\n",
      "     |      used, `fill_between_` is `None`.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.model_selection.learning_curve : Compute the learning curve.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.model_selection import LearningCurveDisplay, learning_curve\n",
      "     |  >>> from sklearn.tree import DecisionTreeClassifier\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> tree = DecisionTreeClassifier(random_state=0)\n",
      "     |  >>> train_sizes, train_scores, test_scores = learning_curve(\n",
      "     |  ...     tree, X, y)\n",
      "     |  >>> display = LearningCurveDisplay(train_sizes=train_sizes,\n",
      "     |  ...     train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\n",
      "     |  >>> display.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LearningCurveDisplay\n",
      "     |      _BaseCurveDisplay\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, train_sizes, train_scores, test_scores, score_name=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, negate_score=False, score_name=None, score_type='both', std_display_style='fill_between', line_kw=None, fill_between_kw=None, errorbar_kw=None)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      negate_score : bool, default=False\n",
      "     |          Whether or not to negate the scores obtained through\n",
      "     |          :func:`~sklearn.model_selection.learning_curve`. This is\n",
      "     |          particularly useful when using the error denoted by `neg_*` in\n",
      "     |          `scikit-learn`.\n",
      "     |      \n",
      "     |      score_name : str, default=None\n",
      "     |          The name of the score used to decorate the y-axis of the plot. It will\n",
      "     |          override the name inferred from the `scoring` parameter. If `score` is\n",
      "     |          `None`, we use `\"Score\"` if `negate_score` is `False` and `\"Negative score\"`\n",
      "     |          otherwise. If `scoring` is a string or a callable, we infer the name. We\n",
      "     |          replace `_` by spaces and capitalize the first letter. We remove `neg_` and\n",
      "     |          replace it by `\"Negative\"` if `negate_score` is\n",
      "     |          `False` or just remove it otherwise.\n",
      "     |      \n",
      "     |      score_type : {\"test\", \"train\", \"both\"}, default=\"both\"\n",
      "     |          The type of score to plot. Can be one of `\"test\"`, `\"train\"`, or\n",
      "     |          `\"both\"`.\n",
      "     |      \n",
      "     |      std_display_style : {\"errorbar\", \"fill_between\"} or None, default=\"fill_between\"\n",
      "     |          The style used to display the score standard deviation around the\n",
      "     |          mean score. If None, no standard deviation representation is\n",
      "     |          displayed.\n",
      "     |      \n",
      "     |      line_kw : dict, default=None\n",
      "     |          Additional keyword arguments passed to the `plt.plot` used to draw\n",
      "     |          the mean score.\n",
      "     |      \n",
      "     |      fill_between_kw : dict, default=None\n",
      "     |          Additional keyword arguments passed to the `plt.fill_between` used\n",
      "     |          to draw the score standard deviation.\n",
      "     |      \n",
      "     |      errorbar_kw : dict, default=None\n",
      "     |          Additional keyword arguments passed to the `plt.errorbar` used to\n",
      "     |          draw mean score and standard deviation score.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.model_selection.LearningCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, groups=None, train_sizes=array([0.1  , 0.325, 0.55 , 0.775, 1.   ]), cv=None, scoring=None, exploit_incremental_learning=False, n_jobs=None, pre_dispatch='all', verbose=0, shuffle=False, random_state=None, error_score=nan, fit_params=None, ax=None, negate_score=False, score_name=None, score_type='both', std_display_style='fill_between', line_kw=None, fill_between_kw=None, errorbar_kw=None)\n",
      "     |      Create a learning curve display from an estimator.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <visualizations>` for general\n",
      "     |      information about the visualization API and :ref:`detailed\n",
      "     |      documentation <learning_curve>` regarding the learning curve\n",
      "     |      visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : object type that implements the \"fit\" and \"predict\" methods\n",
      "     |          An object of that type which is cloned for each validation.\n",
      "     |      \n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n",
      "     |          Target relative to X for classification or regression;\n",
      "     |          None for unsupervised learning.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "     |          instance (e.g., :class:`GroupKFold`).\n",
      "     |      \n",
      "     |      train_sizes : array-like of shape (n_ticks,),                 default=np.linspace(0.1, 1.0, 5)\n",
      "     |          Relative or absolute numbers of training examples that will be used\n",
      "     |          to generate the learning curve. If the dtype is float, it is\n",
      "     |          regarded as a fraction of the maximum size of the training set\n",
      "     |          (that is determined by the selected validation method), i.e. it has\n",
      "     |          to be within (0, 1]. Otherwise it is interpreted as absolute sizes\n",
      "     |          of the training sets. Note that for classification the number of\n",
      "     |          samples usually have to be big enough to contain at least one\n",
      "     |          sample from each class.\n",
      "     |      \n",
      "     |      cv : int, cross-validation generator or an iterable, default=None\n",
      "     |          Determines the cross-validation splitting strategy.\n",
      "     |          Possible inputs for cv are:\n",
      "     |      \n",
      "     |          - None, to use the default 5-fold cross validation,\n",
      "     |          - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "     |          - :term:`CV splitter`,\n",
      "     |          - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |      \n",
      "     |          For int/None inputs, if the estimator is a classifier and `y` is\n",
      "     |          either binary or multiclass,\n",
      "     |          :class:`~sklearn.model_selection.StratifiedKFold` is used. In all\n",
      "     |          other cases, :class:`~sklearn.model_selection.KFold` is used. These\n",
      "     |          splitters are instantiated with `shuffle=False` so the splits will\n",
      "     |          be the same across calls.\n",
      "     |      \n",
      "     |          Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |          cross-validation strategies that can be used here.\n",
      "     |      \n",
      "     |      scoring : str or callable, default=None\n",
      "     |          The scoring method to use when calculating the learning curve. Options:\n",
      "     |      \n",
      "     |          - str: see :ref:`scoring_string_names` for options.\n",
      "     |          - callable: a scorer callable object (e.g., function) with signature\n",
      "     |            ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n",
      "     |          - `None`: the `estimator`'s\n",
      "     |            :ref:`default evaluation criterion <scoring_api_overview>` is used.\n",
      "     |      \n",
      "     |      exploit_incremental_learning : bool, default=False\n",
      "     |          If the estimator supports incremental learning, this will be\n",
      "     |          used to speed up fitting for different training set sizes.\n",
      "     |      \n",
      "     |      n_jobs : int, default=None\n",
      "     |          Number of jobs to run in parallel. Training the estimator and\n",
      "     |          computing the score are parallelized over the different training\n",
      "     |          and test sets. `None` means 1 unless in a\n",
      "     |          :obj:`joblib.parallel_backend` context. `-1` means using all\n",
      "     |          processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "     |      \n",
      "     |      pre_dispatch : int or str, default='all'\n",
      "     |          Number of predispatched jobs for parallel execution (default is\n",
      "     |          all). The option can reduce the allocated memory. The str can\n",
      "     |          be an expression like '2*n_jobs'.\n",
      "     |      \n",
      "     |      verbose : int, default=0\n",
      "     |          Controls the verbosity: the higher, the more messages.\n",
      "     |      \n",
      "     |      shuffle : bool, default=False\n",
      "     |          Whether to shuffle training data before taking prefixes of it\n",
      "     |          based on`train_sizes`.\n",
      "     |      \n",
      "     |      random_state : int, RandomState instance or None, default=None\n",
      "     |          Used when `shuffle` is True. Pass an int for reproducible\n",
      "     |          output across multiple function calls.\n",
      "     |          See :term:`Glossary <random_state>`.\n",
      "     |      \n",
      "     |      error_score : 'raise' or numeric, default=np.nan\n",
      "     |          Value to assign to the score if an error occurs in estimator\n",
      "     |          fitting. If set to 'raise', the error is raised. If a numeric value\n",
      "     |          is given, FitFailedWarning is raised.\n",
      "     |      \n",
      "     |      fit_params : dict, default=None\n",
      "     |          Parameters to pass to the fit method of the estimator.\n",
      "     |      \n",
      "     |      ax : matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      negate_score : bool, default=False\n",
      "     |          Whether or not to negate the scores obtained through\n",
      "     |          :func:`~sklearn.model_selection.learning_curve`. This is\n",
      "     |          particularly useful when using the error denoted by `neg_*` in\n",
      "     |          `scikit-learn`.\n",
      "     |      \n",
      "     |      score_name : str, default=None\n",
      "     |          The name of the score used to decorate the y-axis of the plot. It will\n",
      "     |          override the name inferred from the `scoring` parameter. If `score` is\n",
      "     |          `None`, we use `\"Score\"` if `negate_score` is `False` and `\"Negative score\"`\n",
      "     |          otherwise. If `scoring` is a string or a callable, we infer the name. We\n",
      "     |          replace `_` by spaces and capitalize the first letter. We remove `neg_` and\n",
      "     |          replace it by `\"Negative\"` if `negate_score` is\n",
      "     |          `False` or just remove it otherwise.\n",
      "     |      \n",
      "     |      score_type : {\"test\", \"train\", \"both\"}, default=\"both\"\n",
      "     |          The type of score to plot. Can be one of `\"test\"`, `\"train\"`, or\n",
      "     |          `\"both\"`.\n",
      "     |      \n",
      "     |      std_display_style : {\"errorbar\", \"fill_between\"} or None, default=\"fill_between\"\n",
      "     |          The style used to display the score standard deviation around the\n",
      "     |          mean score. If `None`, no representation of the standard deviation\n",
      "     |          is displayed.\n",
      "     |      \n",
      "     |      line_kw : dict, default=None\n",
      "     |          Additional keyword arguments passed to the `plt.plot` used to draw\n",
      "     |          the mean score.\n",
      "     |      \n",
      "     |      fill_between_kw : dict, default=None\n",
      "     |          Additional keyword arguments passed to the `plt.fill_between` used\n",
      "     |          to draw the score standard deviation.\n",
      "     |      \n",
      "     |      errorbar_kw : dict, default=None\n",
      "     |          Additional keyword arguments passed to the `plt.errorbar` used to\n",
      "     |          draw mean score and standard deviation score.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.model_selection.LearningCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import load_iris\n",
      "     |      >>> from sklearn.model_selection import LearningCurveDisplay\n",
      "     |      >>> from sklearn.tree import DecisionTreeClassifier\n",
      "     |      >>> X, y = load_iris(return_X_y=True)\n",
      "     |      >>> tree = DecisionTreeClassifier(random_state=0)\n",
      "     |      >>> LearningCurveDisplay.from_estimator(tree, X, y)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _BaseCurveDisplay:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class LeaveOneGroupOut(GroupsConsumerMixin, BaseCrossValidator)\n",
      "     |  Leave One Group Out cross-validator.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data such that each training set is\n",
      "     |  comprised of all samples except ones belonging to one specific group.\n",
      "     |  Arbitrary domain specific group information is provided as an array of integers\n",
      "     |  that encodes the group of each sample.\n",
      "     |  \n",
      "     |  For instance the groups could be the year of collection of the samples\n",
      "     |  and thus allow for cross-validation against time-based splits.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <leave_one_group_out>`.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Splits are ordered according to the index of the group left out. The first\n",
      "     |  split has testing set consisting of the group whose index in `groups` is\n",
      "     |  lowest, and so on.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import LeaveOneGroupOut\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
      "     |  >>> y = np.array([1, 2, 1, 2])\n",
      "     |  >>> groups = np.array([1, 1, 2, 2])\n",
      "     |  >>> logo = LeaveOneGroupOut()\n",
      "     |  >>> logo.get_n_splits(X, y, groups)\n",
      "     |  2\n",
      "     |  >>> logo.get_n_splits(groups=groups)  # 'groups' is always required\n",
      "     |  2\n",
      "     |  >>> print(logo)\n",
      "     |  LeaveOneGroupOut()\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(logo.split(X, y, groups)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}, group={groups[train_index]}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}, group={groups[test_index]}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[2 3], group=[2 2]\n",
      "     |    Test:  index=[0 1], group=[1 1]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[0 1], group=[1 1]\n",
      "     |    Test:  index=[2 3], group=[2 2]\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  GroupKFold: K-fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeaveOneGroupOut\n",
      "     |      GroupsConsumerMixin\n",
      "     |      BaseCrossValidator\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set. This 'groups' parameter must always be specified to\n",
      "     |          calculate the number of splits, though the other parameters can be\n",
      "     |          omitted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  set_split_request(self: sklearn.model_selection._split.LeaveOneGroupOut, *, groups: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.model_selection._split.LeaveOneGroupOut from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``split`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``split`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``split``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      groups : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``groups`` parameter in ``split``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class LeaveOneOut(_UnsupportedGroupCVMixin, BaseCrossValidator)\n",
      "     |  Leave-One-Out cross-validator.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train/test sets. Each\n",
      "     |  sample is used once as a test set (singleton) while the remaining\n",
      "     |  samples form the training set.\n",
      "     |  \n",
      "     |  Note: ``LeaveOneOut()`` is equivalent to ``KFold(n_splits=n)`` and\n",
      "     |  ``LeavePOut(p=1)`` where ``n`` is the number of samples.\n",
      "     |  \n",
      "     |  Due to the high number of test sets (which is the same as the\n",
      "     |  number of samples) this cross-validation method can be very costly.\n",
      "     |  For large datasets one should favor :class:`KFold`, :class:`ShuffleSplit`\n",
      "     |  or :class:`StratifiedKFold`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <leave_one_out>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import LeaveOneOut\n",
      "     |  >>> X = np.array([[1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([1, 2])\n",
      "     |  >>> loo = LeaveOneOut()\n",
      "     |  >>> loo.get_n_splits(X)\n",
      "     |  2\n",
      "     |  >>> print(loo)\n",
      "     |  LeaveOneOut()\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(loo.split(X)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[1]\n",
      "     |    Test:  index=[0]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[0]\n",
      "     |    Test:  index=[1]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  LeaveOneGroupOut : For splitting the data according to explicit,\n",
      "     |      domain-specific stratification of the dataset.\n",
      "     |  GroupKFold : K-fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeaveOneOut\n",
      "     |      _UnsupportedGroupCVMixin\n",
      "     |      BaseCrossValidator\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_n_splits(self, X, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _UnsupportedGroupCVMixin:\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _UnsupportedGroupCVMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class LeavePGroupsOut(GroupsConsumerMixin, BaseCrossValidator)\n",
      "     |  LeavePGroupsOut(n_groups)\n",
      "     |  \n",
      "     |  Leave P Group(s) Out cross-validator.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data according to a third-party\n",
      "     |  provided group. This group information can be used to encode arbitrary\n",
      "     |  domain specific stratifications of the samples as integers.\n",
      "     |  \n",
      "     |  For instance the groups could be the year of collection of the samples\n",
      "     |  and thus allow for cross-validation against time-based splits.\n",
      "     |  \n",
      "     |  The difference between LeavePGroupsOut and LeaveOneGroupOut is that\n",
      "     |  the former builds the test sets with all the samples assigned to\n",
      "     |  ``p`` different values of the groups while the latter uses samples\n",
      "     |  all assigned the same groups.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <leave_p_groups_out>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_groups : int\n",
      "     |      Number of groups (``p``) to leave out in the test split.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import LeavePGroupsOut\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6]])\n",
      "     |  >>> y = np.array([1, 2, 1])\n",
      "     |  >>> groups = np.array([1, 2, 3])\n",
      "     |  >>> lpgo = LeavePGroupsOut(n_groups=2)\n",
      "     |  >>> lpgo.get_n_splits(X, y, groups)\n",
      "     |  3\n",
      "     |  >>> lpgo.get_n_splits(groups=groups)  # 'groups' is always required\n",
      "     |  3\n",
      "     |  >>> print(lpgo)\n",
      "     |  LeavePGroupsOut(n_groups=2)\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(lpgo.split(X, y, groups)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}, group={groups[train_index]}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}, group={groups[test_index]}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[2], group=[3]\n",
      "     |    Test:  index=[0 1], group=[1 2]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[1], group=[2]\n",
      "     |    Test:  index=[0 2], group=[1 3]\n",
      "     |  Fold 2:\n",
      "     |    Train: index=[0], group=[1]\n",
      "     |    Test:  index=[1 2], group=[2 3]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  GroupKFold : K-fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeavePGroupsOut\n",
      "     |      GroupsConsumerMixin\n",
      "     |      BaseCrossValidator\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_groups)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set. This 'groups' parameter must always be specified to\n",
      "     |          calculate the number of splits, though the other parameters can be\n",
      "     |          omitted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  set_split_request(self: sklearn.model_selection._split.LeavePGroupsOut, *, groups: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.model_selection._split.LeavePGroupsOut from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``split`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``split`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``split``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      groups : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``groups`` parameter in ``split``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class LeavePOut(_UnsupportedGroupCVMixin, BaseCrossValidator)\n",
      "     |  LeavePOut(p)\n",
      "     |  \n",
      "     |  Leave-P-Out cross-validator.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train/test sets. This results\n",
      "     |  in testing on all distinct samples of size p, while the remaining n - p\n",
      "     |  samples form the training set in each iteration.\n",
      "     |  \n",
      "     |  Note: ``LeavePOut(p)`` is NOT equivalent to\n",
      "     |  ``KFold(n_splits=n_samples // p)`` which creates non-overlapping test sets.\n",
      "     |  \n",
      "     |  Due to the high number of iterations which grows combinatorically with the\n",
      "     |  number of samples this cross-validation method can be very costly. For\n",
      "     |  large datasets one should favor :class:`KFold`, :class:`StratifiedKFold`\n",
      "     |  or :class:`ShuffleSplit`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <leave_p_out>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  p : int\n",
      "     |      Size of the test sets. Must be strictly less than the number of\n",
      "     |      samples.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import LeavePOut\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
      "     |  >>> y = np.array([1, 2, 3, 4])\n",
      "     |  >>> lpo = LeavePOut(2)\n",
      "     |  >>> lpo.get_n_splits(X)\n",
      "     |  6\n",
      "     |  >>> print(lpo)\n",
      "     |  LeavePOut(p=2)\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(lpo.split(X)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[2 3]\n",
      "     |    Test:  index=[0 1]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[1 3]\n",
      "     |    Test:  index=[0 2]\n",
      "     |  Fold 2:\n",
      "     |    Train: index=[1 2]\n",
      "     |    Test:  index=[0 3]\n",
      "     |  Fold 3:\n",
      "     |    Train: index=[0 3]\n",
      "     |    Test:  index=[1 2]\n",
      "     |  Fold 4:\n",
      "     |    Train: index=[0 2]\n",
      "     |    Test:  index=[1 3]\n",
      "     |  Fold 5:\n",
      "     |    Train: index=[0 1]\n",
      "     |    Test:  index=[2 3]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeavePOut\n",
      "     |      _UnsupportedGroupCVMixin\n",
      "     |      BaseCrossValidator\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, p)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_n_splits(self, X, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _UnsupportedGroupCVMixin:\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _UnsupportedGroupCVMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class ParameterGrid(builtins.object)\n",
      "     |  ParameterGrid(param_grid)\n",
      "     |  \n",
      "     |  Grid of parameters with a discrete number of values for each.\n",
      "     |  \n",
      "     |  Can be used to iterate over parameter value combinations with the\n",
      "     |  Python built-in function iter.\n",
      "     |  The order of the generated parameter combinations is deterministic.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <grid_search>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  param_grid : dict of str to sequence, or sequence of such\n",
      "     |      The parameter grid to explore, as a dictionary mapping estimator\n",
      "     |      parameters to sequences of allowed values.\n",
      "     |  \n",
      "     |      An empty dict signifies default parameters.\n",
      "     |  \n",
      "     |      A sequence of dicts signifies a sequence of grids to search, and is\n",
      "     |      useful to avoid exploring parameter combinations that make no sense\n",
      "     |      or have no effect. See the examples below.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.model_selection import ParameterGrid\n",
      "     |  >>> param_grid = {'a': [1, 2], 'b': [True, False]}\n",
      "     |  >>> list(ParameterGrid(param_grid)) == (\n",
      "     |  ...    [{'a': 1, 'b': True}, {'a': 1, 'b': False},\n",
      "     |  ...     {'a': 2, 'b': True}, {'a': 2, 'b': False}])\n",
      "     |  True\n",
      "     |  \n",
      "     |  >>> grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]\n",
      "     |  >>> list(ParameterGrid(grid)) == [{'kernel': 'linear'},\n",
      "     |  ...                               {'kernel': 'rbf', 'gamma': 1},\n",
      "     |  ...                               {'kernel': 'rbf', 'gamma': 10}]\n",
      "     |  True\n",
      "     |  >>> ParameterGrid(grid)[1] == {'kernel': 'rbf', 'gamma': 1}\n",
      "     |  True\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  GridSearchCV : Uses :class:`ParameterGrid` to perform a full parallelized\n",
      "     |      parameter search.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, ind)\n",
      "     |      Get the parameters that would be ``ind``th in iteration\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ind : int\n",
      "     |          The iteration index\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict of str to any\n",
      "     |          Equal to list(self)[ind]\n",
      "     |  \n",
      "     |  __init__(self, param_grid)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Iterate over the points in the grid.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : iterator over dict of str to any\n",
      "     |          Yields dictionaries mapping each estimator parameter to one of its\n",
      "     |          allowed values.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Number of points on the grid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class ParameterSampler(builtins.object)\n",
      "     |  ParameterSampler(param_distributions, n_iter, *, random_state=None)\n",
      "     |  \n",
      "     |  Generator on parameters sampled from given distributions.\n",
      "     |  \n",
      "     |  Non-deterministic iterable over random candidate combinations for hyper-\n",
      "     |  parameter search. If all parameters are presented as a list,\n",
      "     |  sampling without replacement is performed. If at least one parameter\n",
      "     |  is given as a distribution, sampling with replacement is used.\n",
      "     |  It is highly recommended to use continuous distributions for continuous\n",
      "     |  parameters.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <grid_search>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  param_distributions : dict\n",
      "     |      Dictionary with parameters names (`str`) as keys and distributions\n",
      "     |      or lists of parameters to try. Distributions must provide a ``rvs``\n",
      "     |      method for sampling (such as those from scipy.stats.distributions).\n",
      "     |      If a list is given, it is sampled uniformly.\n",
      "     |      If a list of dicts is given, first a dict is sampled uniformly, and\n",
      "     |      then a parameter is sampled using that dict as above.\n",
      "     |  \n",
      "     |  n_iter : int\n",
      "     |      Number of parameter settings that are produced.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Pseudo random number generator state used for random uniform sampling\n",
      "     |      from lists of possible values instead of scipy.stats distributions.\n",
      "     |      Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Returns\n",
      "     |  -------\n",
      "     |  params : dict of str to any\n",
      "     |      **Yields** dictionaries mapping each estimator parameter to\n",
      "     |      as sampled value.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.model_selection import ParameterSampler\n",
      "     |  >>> from scipy.stats.distributions import expon\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> param_grid = {'a':[1, 2], 'b': expon()}\n",
      "     |  >>> param_list = list(ParameterSampler(param_grid, n_iter=4,\n",
      "     |  ...                                    random_state=rng))\n",
      "     |  >>> rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items())\n",
      "     |  ...                 for d in param_list]\n",
      "     |  >>> rounded_list == [{'b': 0.89856, 'a': 1},\n",
      "     |  ...                  {'b': 0.923223, 'a': 1},\n",
      "     |  ...                  {'b': 1.878964, 'a': 2},\n",
      "     |  ...                  {'b': 1.038159, 'a': 2}]\n",
      "     |  True\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, param_distributions, n_iter, *, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Number of points that will be sampled.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class PredefinedSplit(BaseCrossValidator)\n",
      "     |  PredefinedSplit(test_fold)\n",
      "     |  \n",
      "     |  Predefined split cross-validator.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data into train/test sets using a\n",
      "     |  predefined scheme specified by the user with the ``test_fold`` parameter.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <predefined_split>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.16\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  test_fold : array-like of shape (n_samples,)\n",
      "     |      The entry ``test_fold[i]`` represents the index of the test set that\n",
      "     |      sample ``i`` belongs to. It is possible to exclude sample ``i`` from\n",
      "     |      any test set (i.e. include sample ``i`` in every training set) by\n",
      "     |      setting ``test_fold[i]`` equal to -1.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import PredefinedSplit\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> test_fold = [0, 1, -1, 1]\n",
      "     |  >>> ps = PredefinedSplit(test_fold)\n",
      "     |  >>> ps.get_n_splits()\n",
      "     |  2\n",
      "     |  >>> print(ps)\n",
      "     |  PredefinedSplit(test_fold=array([ 0,  1, -1,  1]))\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(ps.split()):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[1 2 3]\n",
      "     |    Test:  index=[0]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[0 2]\n",
      "     |    Test:  index=[1 3]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PredefinedSplit\n",
      "     |      BaseCrossValidator\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, test_fold)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X=None, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class RandomizedSearchCV(BaseSearchCV)\n",
      "     |  RandomizedSearchCV(estimator, param_distributions, *, n_iter=10, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None, error_score=nan, return_train_score=False)\n",
      "     |  \n",
      "     |  Randomized search on hyper parameters.\n",
      "     |  \n",
      "     |  RandomizedSearchCV implements a \"fit\" and a \"score\" method.\n",
      "     |  It also implements \"score_samples\", \"predict\", \"predict_proba\",\n",
      "     |  \"decision_function\", \"transform\" and \"inverse_transform\" if they are\n",
      "     |  implemented in the estimator used.\n",
      "     |  \n",
      "     |  The parameters of the estimator used to apply these methods are optimized\n",
      "     |  by cross-validated search over parameter settings.\n",
      "     |  \n",
      "     |  In contrast to GridSearchCV, not all parameter values are tried out, but\n",
      "     |  rather a fixed number of parameter settings is sampled from the specified\n",
      "     |  distributions. The number of parameter settings that are tried is\n",
      "     |  given by n_iter.\n",
      "     |  \n",
      "     |  If all parameters are presented as a list,\n",
      "     |  sampling without replacement is performed. If at least one parameter\n",
      "     |  is given as a distribution, sampling with replacement is used.\n",
      "     |  It is highly recommended to use continuous distributions for continuous\n",
      "     |  parameters.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <randomized_parameter_search>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.14\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : estimator object\n",
      "     |      An object of that type is instantiated for each grid point.\n",
      "     |      This is assumed to implement the scikit-learn estimator interface.\n",
      "     |      Either estimator needs to provide a ``score`` function,\n",
      "     |      or ``scoring`` must be passed.\n",
      "     |  \n",
      "     |  param_distributions : dict or list of dicts\n",
      "     |      Dictionary with parameters names (`str`) as keys and distributions\n",
      "     |      or lists of parameters to try. Distributions must provide a ``rvs``\n",
      "     |      method for sampling (such as those from scipy.stats.distributions).\n",
      "     |      If a list is given, it is sampled uniformly.\n",
      "     |      If a list of dicts is given, first a dict is sampled uniformly, and\n",
      "     |      then a parameter is sampled using that dict as above.\n",
      "     |  \n",
      "     |  n_iter : int, default=10\n",
      "     |      Number of parameter settings that are sampled. n_iter trades\n",
      "     |      off runtime vs quality of the solution.\n",
      "     |  \n",
      "     |  scoring : str, callable, list, tuple or dict, default=None\n",
      "     |      Strategy to evaluate the performance of the cross-validated model on\n",
      "     |      the test set.\n",
      "     |  \n",
      "     |      If `scoring` represents a single score, one can use:\n",
      "     |  \n",
      "     |      - a single string (see :ref:`scoring_string_names`);\n",
      "     |      - a callable (see :ref:`scoring_callable`) that returns a single value;\n",
      "     |      - `None`, the `estimator`'s\n",
      "     |        :ref:`default evaluation criterion <scoring_api_overview>` is used.\n",
      "     |  \n",
      "     |      If `scoring` represents multiple scores, one can use:\n",
      "     |  \n",
      "     |      - a list or tuple of unique strings;\n",
      "     |      - a callable returning a dictionary where the keys are the metric\n",
      "     |        names and the values are the metric scores;\n",
      "     |      - a dictionary with metric names as keys and callables as values.\n",
      "     |  \n",
      "     |      See :ref:`multimetric_grid_search` for an example.\n",
      "     |  \n",
      "     |      If None, the estimator's score method is used.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of jobs to run in parallel.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |      .. versionchanged:: v0.20\n",
      "     |         `n_jobs` default changed from 1 to None\n",
      "     |  \n",
      "     |  refit : bool, str, or callable, default=True\n",
      "     |      Refit an estimator using the best found parameters on the whole\n",
      "     |      dataset.\n",
      "     |  \n",
      "     |      For multiple metric evaluation, this needs to be a `str` denoting the\n",
      "     |      scorer that would be used to find the best parameters for refitting\n",
      "     |      the estimator at the end.\n",
      "     |  \n",
      "     |      Where there are considerations other than maximum score in\n",
      "     |      choosing a best estimator, ``refit`` can be set to a function which\n",
      "     |      returns the selected ``best_index_`` given the ``cv_results_``. In that\n",
      "     |      case, the ``best_estimator_`` and ``best_params_`` will be set\n",
      "     |      according to the returned ``best_index_`` while the ``best_score_``\n",
      "     |      attribute will not be available.\n",
      "     |  \n",
      "     |      The refitted estimator is made available at the ``best_estimator_``\n",
      "     |      attribute and permits using ``predict`` directly on this\n",
      "     |      ``RandomizedSearchCV`` instance.\n",
      "     |  \n",
      "     |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      "     |      ``best_score_`` and ``best_params_`` will only be available if\n",
      "     |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
      "     |      scorer.\n",
      "     |  \n",
      "     |      See ``scoring`` parameter to know more about multiple metric\n",
      "     |      evaluation.\n",
      "     |  \n",
      "     |      See :ref:`this example\n",
      "     |      <sphx_glr_auto_examples_model_selection_plot_grid_search_refit_callable.py>`\n",
      "     |      for an example of how to use ``refit=callable`` to balance model\n",
      "     |      complexity and cross-validated score.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |          Support for callable added.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross validation,\n",
      "     |      - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      "     |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "     |      other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "     |      with `shuffle=False` so the splits will be the same across calls.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  verbose : int\n",
      "     |      Controls the verbosity: the higher, the more messages.\n",
      "     |  \n",
      "     |      - >1 : the computation time for each fold and parameter candidate is\n",
      "     |        displayed;\n",
      "     |      - >2 : the score is also displayed;\n",
      "     |      - >3 : the fold and candidate parameter indexes are also displayed\n",
      "     |        together with the starting time of the computation.\n",
      "     |  \n",
      "     |  pre_dispatch : int, or str, default='2*n_jobs'\n",
      "     |      Controls the number of jobs that get dispatched during parallel\n",
      "     |      execution. Reducing this number can be useful to avoid an\n",
      "     |      explosion of memory consumption when more jobs get dispatched\n",
      "     |      than CPUs can process. This parameter can be:\n",
      "     |  \n",
      "     |      - None, in which case all the jobs are immediately created and spawned. Use\n",
      "     |        this for lightweight and fast-running jobs, to avoid delays due to on-demand\n",
      "     |        spawning of the jobs\n",
      "     |      - An int, giving the exact number of total jobs that are spawned\n",
      "     |      - A str, giving an expression as a function of n_jobs, as in '2*n_jobs'\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Pseudo random number generator state used for random uniform sampling\n",
      "     |      from lists of possible values instead of scipy.stats distributions.\n",
      "     |      Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  error_score : 'raise' or numeric, default=np.nan\n",
      "     |      Value to assign to the score if an error occurs in estimator fitting.\n",
      "     |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      "     |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      "     |      step, which will always raise the error.\n",
      "     |  \n",
      "     |  return_train_score : bool, default=False\n",
      "     |      If ``False``, the ``cv_results_`` attribute will not include training\n",
      "     |      scores.\n",
      "     |      Computing training scores is used to get insights on how different\n",
      "     |      parameter settings impact the overfitting/underfitting trade-off.\n",
      "     |      However computing the scores on the training set can be computationally\n",
      "     |      expensive and is not strictly required to select the parameters that\n",
      "     |      yield the best generalization performance.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.21\n",
      "     |          Default value was changed from ``True`` to ``False``\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_results_ : dict of numpy (masked) ndarrays\n",
      "     |      A dict with keys as column headers and values as columns, that can be\n",
      "     |      imported into a pandas ``DataFrame``.\n",
      "     |  \n",
      "     |      For instance the below given table\n",
      "     |  \n",
      "     |      +--------------+-------------+-------------------+---+---------------+\n",
      "     |      | param_kernel | param_gamma | split0_test_score |...|rank_test_score|\n",
      "     |      +==============+=============+===================+===+===============+\n",
      "     |      |    'rbf'     |     0.1     |       0.80        |...|       1       |\n",
      "     |      +--------------+-------------+-------------------+---+---------------+\n",
      "     |      |    'rbf'     |     0.2     |       0.84        |...|       3       |\n",
      "     |      +--------------+-------------+-------------------+---+---------------+\n",
      "     |      |    'rbf'     |     0.3     |       0.70        |...|       2       |\n",
      "     |      +--------------+-------------+-------------------+---+---------------+\n",
      "     |  \n",
      "     |      will be represented by a ``cv_results_`` dict of::\n",
      "     |  \n",
      "     |          {\n",
      "     |          'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],\n",
      "     |                                        mask = False),\n",
      "     |          'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False),\n",
      "     |          'split0_test_score'  : [0.80, 0.84, 0.70],\n",
      "     |          'split1_test_score'  : [0.82, 0.50, 0.70],\n",
      "     |          'mean_test_score'    : [0.81, 0.67, 0.70],\n",
      "     |          'std_test_score'     : [0.01, 0.24, 0.00],\n",
      "     |          'rank_test_score'    : [1, 3, 2],\n",
      "     |          'split0_train_score' : [0.80, 0.92, 0.70],\n",
      "     |          'split1_train_score' : [0.82, 0.55, 0.70],\n",
      "     |          'mean_train_score'   : [0.81, 0.74, 0.70],\n",
      "     |          'std_train_score'    : [0.01, 0.19, 0.00],\n",
      "     |          'mean_fit_time'      : [0.73, 0.63, 0.43],\n",
      "     |          'std_fit_time'       : [0.01, 0.02, 0.01],\n",
      "     |          'mean_score_time'    : [0.01, 0.06, 0.04],\n",
      "     |          'std_score_time'     : [0.00, 0.00, 0.00],\n",
      "     |          'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],\n",
      "     |          }\n",
      "     |  \n",
      "     |      NOTE\n",
      "     |  \n",
      "     |      The key ``'params'`` is used to store a list of parameter\n",
      "     |      settings dicts for all the parameter candidates.\n",
      "     |  \n",
      "     |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      "     |      ``std_score_time`` are all in seconds.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, the scores for all the scorers are\n",
      "     |      available in the ``cv_results_`` dict at the keys ending with that\n",
      "     |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      "     |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      "     |  \n",
      "     |  best_estimator_ : estimator\n",
      "     |      Estimator that was chosen by the search, i.e. estimator\n",
      "     |      which gave highest score (or smallest loss if specified)\n",
      "     |      on the left out data. Not available if ``refit=False``.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this attribute is present only if\n",
      "     |      ``refit`` is specified.\n",
      "     |  \n",
      "     |      See ``refit`` parameter for more information on allowed values.\n",
      "     |  \n",
      "     |  best_score_ : float\n",
      "     |      Mean cross-validated score of the best_estimator.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      "     |      ``False``. See ``refit`` parameter for more information.\n",
      "     |  \n",
      "     |      This attribute is not available if ``refit`` is a function.\n",
      "     |  \n",
      "     |  best_params_ : dict\n",
      "     |      Parameter setting that gave the best results on the hold out data.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      "     |      ``False``. See ``refit`` parameter for more information.\n",
      "     |  \n",
      "     |  best_index_ : int\n",
      "     |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      "     |      candidate parameter setting.\n",
      "     |  \n",
      "     |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      "     |      the parameter setting for the best model, that gives the highest\n",
      "     |      mean score (``search.best_score_``).\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      "     |      ``False``. See ``refit`` parameter for more information.\n",
      "     |  \n",
      "     |  scorer_ : function or a dict\n",
      "     |      Scorer function used on the held out data to choose the best\n",
      "     |      parameters for the model.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this attribute holds the validated\n",
      "     |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      "     |  \n",
      "     |  n_splits_ : int\n",
      "     |      The number of cross-validation splits (folds/iterations).\n",
      "     |  \n",
      "     |  refit_time_ : float\n",
      "     |      Seconds used for refitting the best model on the whole dataset.\n",
      "     |  \n",
      "     |      This is present only if ``refit`` is not False.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  multimetric_ : bool\n",
      "     |      Whether or not the scorers compute several metrics.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels. This is present only if ``refit`` is specified and\n",
      "     |      the underlying estimator is a classifier.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`. Only defined if\n",
      "     |      `best_estimator_` is defined (see the documentation for the `refit`\n",
      "     |      parameter for more details) and that `best_estimator_` exposes\n",
      "     |      `n_features_in_` when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Only defined if\n",
      "     |      `best_estimator_` is defined (see the documentation for the `refit`\n",
      "     |      parameter for more details) and that `best_estimator_` exposes\n",
      "     |      `feature_names_in_` when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  GridSearchCV : Does exhaustive search over a grid of parameters.\n",
      "     |  ParameterSampler : A generator over parameter settings, constructed from\n",
      "     |      param_distributions.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The parameters selected are those that maximize the score of the held-out\n",
      "     |  data, according to the scoring parameter.\n",
      "     |  \n",
      "     |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      "     |  parameter setting(and not `n_jobs` times). This is done for efficiency\n",
      "     |  reasons if individual jobs take very little time, but may raise errors if\n",
      "     |  the dataset is large and not enough memory is available.  A workaround in\n",
      "     |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      "     |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      "     |  n_jobs`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> from sklearn.model_selection import RandomizedSearchCV\n",
      "     |  >>> from scipy.stats import uniform\n",
      "     |  >>> iris = load_iris()\n",
      "     |  >>> logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
      "     |  ...                               random_state=0)\n",
      "     |  >>> distributions = dict(C=uniform(loc=0, scale=4),\n",
      "     |  ...                      penalty=['l2', 'l1'])\n",
      "     |  >>> clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
      "     |  >>> search = clf.fit(iris.data, iris.target)\n",
      "     |  >>> search.best_params_\n",
      "     |  {'C': np.float64(2.195...), 'penalty': 'l1'}\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomizedSearchCV\n",
      "     |      BaseSearchCV\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator, param_distributions, *, n_iter=10, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None, error_score=nan, return_train_score=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSearchCV:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Call decision_function on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``decision_function``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_score : ndarray of shape (n_samples,) or (n_samples, n_classes)                 or (n_samples, n_classes * (n_classes-1) / 2)\n",
      "     |          Result of the decision function for `X` based on the estimator with\n",
      "     |          the best found parameters.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, **params)\n",
      "     |      Run fit with all sets of parameters.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      \n",
      "     |      X : array-like of shape (n_samples, n_features) or (n_samples, n_samples)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features. For precomputed kernel or\n",
      "     |          distance matrix, the expected shape of X is (n_samples, n_samples).\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      "     |          Target relative to X for classification or regression;\n",
      "     |          None for unsupervised learning.\n",
      "     |      \n",
      "     |      **params : dict of str -> object\n",
      "     |          Parameters passed to the ``fit`` method of the estimator, the scorer,\n",
      "     |          and the CV splitter.\n",
      "     |      \n",
      "     |          If a fit parameter is an array-like whose length is equal to\n",
      "     |          `num_samples` then it will be split by cross-validation along with\n",
      "     |          `X` and `y`. For example, the :term:`sample_weight` parameter is\n",
      "     |          split because `len(sample_weights) = len(X)`. However, this behavior\n",
      "     |          does not apply to `groups` which is passed to the splitter configured\n",
      "     |          via the `cv` parameter of the constructor. Thus, `groups` is used\n",
      "     |          *to perform the split* and determines which samples are\n",
      "     |          assigned to the each side of the a split.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Instance of fitted estimator.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Call inverse_transform on the estimator with the best found params.\n",
      "     |      \n",
      "     |      Only available if the underlying estimator implements\n",
      "     |      ``inverse_transform`` and ``refit=True``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_original : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Result of the `inverse_transform` function for `X` based on the\n",
      "     |          estimator with the best found parameters.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Call predict on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          The predicted labels or values for `X` based on the estimator with\n",
      "     |          the best found parameters.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Call predict_log_proba on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict_log_proba``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Predicted class log-probabilities for `X` based on the estimator\n",
      "     |          with the best found parameters. The order of the classes\n",
      "     |          corresponds to that in the fitted attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Call predict_proba on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict_proba``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Predicted class probabilities for `X` based on the estimator with\n",
      "     |          the best found parameters. The order of the classes corresponds\n",
      "     |          to that in the fitted attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  score(self, X, y=None, **params)\n",
      "     |      Return the score on the given data, if the estimator has been refit.\n",
      "     |      \n",
      "     |      This uses the score defined by ``scoring`` where provided, and the\n",
      "     |      ``best_estimator_.score`` method otherwise.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      "     |          Target relative to X for classification or regression;\n",
      "     |          None for unsupervised learning.\n",
      "     |      \n",
      "     |      **params : dict\n",
      "     |          Parameters to be passed to the underlying scorer(s).\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |              Only available if `enable_metadata_routing=True`. See\n",
      "     |              :ref:`Metadata Routing User Guide <metadata_routing>` for more\n",
      "     |              details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          The score defined by ``scoring`` if provided, and the\n",
      "     |          ``best_estimator_.score`` method otherwise.\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Call score_samples on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``score_samples``.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.24\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : iterable\n",
      "     |          Data to predict on. Must fulfill input requirements\n",
      "     |          of the underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_score : ndarray of shape (n_samples,)\n",
      "     |          The ``best_estimator_.score_samples`` method.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Call transform on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if the underlying estimator supports ``transform`` and\n",
      "     |      ``refit=True``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          `X` transformed in the new space based on the estimator with\n",
      "     |          the best found parameters.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseSearchCV:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |      Class labels.\n",
      "     |      \n",
      "     |      Only available when `refit=True` and the estimator is a classifier.\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |      \n",
      "     |      Only available when `refit=True`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class RepeatedKFold(_UnsupportedGroupCVMixin, _RepeatedSplits)\n",
      "     |  RepeatedKFold(*, n_splits=5, n_repeats=10, random_state=None)\n",
      "     |  \n",
      "     |  Repeated K-Fold cross validator.\n",
      "     |  \n",
      "     |  Repeats K-Fold `n_repeats` times with different randomization in each repetition.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <repeated_k_fold>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |  n_repeats : int, default=10\n",
      "     |      Number of times cross-validator needs to be repeated.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of each repeated cross-validation instance.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import RepeatedKFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)\n",
      "     |  >>> rkf.get_n_splits(X, y)\n",
      "     |  4\n",
      "     |  >>> print(rkf)\n",
      "     |  RepeatedKFold(n_repeats=2, n_splits=2, random_state=2652124)\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(rkf.split(X)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}\")\n",
      "     |  ...\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[0 1]\n",
      "     |    Test:  index=[2 3]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[2 3]\n",
      "     |    Test:  index=[0 1]\n",
      "     |  Fold 2:\n",
      "     |    Train: index=[1 2]\n",
      "     |    Test:  index=[0 3]\n",
      "     |  Fold 3:\n",
      "     |    Train: index=[0 3]\n",
      "     |    Test:  index=[1 2]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Randomized CV splitters may return different results for each call of\n",
      "     |  split. You can make the results identical by setting `random_state`\n",
      "     |  to an integer.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RepeatedKFold\n",
      "     |      _UnsupportedGroupCVMixin\n",
      "     |      _RepeatedSplits\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_splits=5, n_repeats=10, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _UnsupportedGroupCVMixin:\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _UnsupportedGroupCVMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _RepeatedSplits:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |          ``np.zeros(n_samples)`` may be used as a placeholder.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |          ``np.zeros(n_samples)`` may be used as a placeholder.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class RepeatedStratifiedKFold(_UnsupportedGroupCVMixin, _RepeatedSplits)\n",
      "     |  RepeatedStratifiedKFold(*, n_splits=5, n_repeats=10, random_state=None)\n",
      "     |  \n",
      "     |  Repeated class-wise stratified K-Fold cross validator.\n",
      "     |  \n",
      "     |  Repeats Stratified K-Fold n times with different randomization in each\n",
      "     |  repetition.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <repeated_k_fold>`.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      Stratification on the class label solves an engineering problem rather\n",
      "     |      than a statistical one. See :ref:`stratification` for more details.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |  n_repeats : int, default=10\n",
      "     |      Number of times cross-validator needs to be repeated.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the generation of the random states for each repetition.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import RepeatedStratifiedKFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,\n",
      "     |  ...     random_state=36851234)\n",
      "     |  >>> rskf.get_n_splits(X, y)\n",
      "     |  4\n",
      "     |  >>> print(rskf)\n",
      "     |  RepeatedStratifiedKFold(n_repeats=2, n_splits=2, random_state=36851234)\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(rskf.split(X, y)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}\")\n",
      "     |  ...\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[1 2]\n",
      "     |    Test:  index=[0 3]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[0 3]\n",
      "     |    Test:  index=[1 2]\n",
      "     |  Fold 2:\n",
      "     |    Train: index=[1 3]\n",
      "     |    Test:  index=[0 2]\n",
      "     |  Fold 3:\n",
      "     |    Train: index=[0 2]\n",
      "     |    Test:  index=[1 3]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Randomized CV splitters may return different results for each call of\n",
      "     |  split. You can make the results identical by setting `random_state`\n",
      "     |  to an integer.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RepeatedKFold : Repeats K-Fold n times.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RepeatedStratifiedKFold\n",
      "     |      _UnsupportedGroupCVMixin\n",
      "     |      _RepeatedSplits\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_splits=5, n_repeats=10, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  split(self, X, y, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |          Note that providing ``y`` is sufficient to generate the splits and\n",
      "     |          hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
      "     |          ``X`` instead of actual training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |          Stratification is done based on the y labels.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Randomized CV splitters may return different results for each call of\n",
      "     |      split. You can make the results identical by setting `random_state`\n",
      "     |      to an integer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _UnsupportedGroupCVMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _RepeatedSplits:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |          ``np.zeros(n_samples)`` may be used as a placeholder.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |          ``np.zeros(n_samples)`` may be used as a placeholder.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class ShuffleSplit(_UnsupportedGroupCVMixin, BaseShuffleSplit)\n",
      "     |  ShuffleSplit(n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |  \n",
      "     |  Random permutation cross-validator.\n",
      "     |  \n",
      "     |  Yields indices to split data into training and test sets.\n",
      "     |  \n",
      "     |  Note: contrary to other cross-validation strategies, random splits\n",
      "     |  do not guarantee that test sets across all folds will be mutually exclusive,\n",
      "     |  and might include overlapping samples. However, this is still very likely for\n",
      "     |  sizeable datasets.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ShuffleSplit>`.\n",
      "     |  \n",
      "     |  For visualisation of cross-validation behaviour and\n",
      "     |  comparison between common scikit-learn split methods\n",
      "     |  refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=10\n",
      "     |      Number of re-shuffling & splitting iterations.\n",
      "     |  \n",
      "     |  test_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "     |      of the dataset to include in the test split. If int, represents the\n",
      "     |      absolute number of test samples. If None, the value is set to the\n",
      "     |      complement of the train size. If ``train_size`` is also None, it will\n",
      "     |      be set to 0.1.\n",
      "     |  \n",
      "     |  train_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the dataset to include in the train split. If\n",
      "     |      int, represents the absolute number of train samples. If None,\n",
      "     |      the value is automatically set to the complement of the test size.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of the training and testing indices produced.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import ShuffleSplit\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])\n",
      "     |  >>> y = np.array([1, 2, 1, 2, 1, 2])\n",
      "     |  >>> rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n",
      "     |  >>> rs.get_n_splits(X)\n",
      "     |  5\n",
      "     |  >>> print(rs)\n",
      "     |  ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(rs.split(X)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[1 3 0 4]\n",
      "     |    Test:  index=[5 2]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[4 0 2 5]\n",
      "     |    Test:  index=[1 3]\n",
      "     |  Fold 2:\n",
      "     |    Train: index=[1 2 4 0]\n",
      "     |    Test:  index=[3 5]\n",
      "     |  Fold 3:\n",
      "     |    Train: index=[3 4 1 0]\n",
      "     |    Test:  index=[5 2]\n",
      "     |  Fold 4:\n",
      "     |    Train: index=[3 5 1 0]\n",
      "     |    Test:  index=[2 4]\n",
      "     |  >>> # Specify train and test size\n",
      "     |  >>> rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,\n",
      "     |  ...                   random_state=0)\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(rs.split(X)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[1 3 0]\n",
      "     |    Test:  index=[5 2]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[4 0 2]\n",
      "     |    Test:  index=[1 3]\n",
      "     |  Fold 2:\n",
      "     |    Train: index=[1 2 4]\n",
      "     |    Test:  index=[3 5]\n",
      "     |  Fold 3:\n",
      "     |    Train: index=[3 4 1]\n",
      "     |    Test:  index=[5 2]\n",
      "     |  Fold 4:\n",
      "     |    Train: index=[3 5 1]\n",
      "     |    Test:  index=[2 4]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ShuffleSplit\n",
      "     |      _UnsupportedGroupCVMixin\n",
      "     |      BaseShuffleSplit\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _UnsupportedGroupCVMixin:\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _UnsupportedGroupCVMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class StratifiedGroupKFold(GroupsConsumerMixin, _BaseKFold)\n",
      "     |  StratifiedGroupKFold(n_splits=5, shuffle=False, random_state=None)\n",
      "     |  \n",
      "     |  Class-wise stratified K-Fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  This cross-validation object is a variation of StratifiedKFold attempts to\n",
      "     |  return stratified folds with non-overlapping groups. The folds are made by\n",
      "     |  preserving the percentage of samples for each class in `y` in a binary or\n",
      "     |  multiclass classification setting.\n",
      "     |  \n",
      "     |  Each group will appear exactly once in the test set across all folds (the\n",
      "     |  number of distinct groups has to be at least equal to the number of folds).\n",
      "     |  \n",
      "     |  The difference between :class:`GroupKFold`\n",
      "     |  and `StratifiedGroupKFold` is that\n",
      "     |  the former attempts to create balanced folds such that the number of\n",
      "     |  distinct groups is approximately the same in each fold, whereas\n",
      "     |  `StratifiedGroupKFold` attempts to create folds which preserve the\n",
      "     |  percentage of samples for each class as much as possible given the\n",
      "     |  constraint of non-overlapping groups between splits.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <stratified_group_k_fold>`.\n",
      "     |  \n",
      "     |  For visualisation of cross-validation behaviour and\n",
      "     |  comparison between common scikit-learn split methods\n",
      "     |  refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      Stratification on the class label solves an engineering problem rather\n",
      "     |      than a statistical one. See :ref:`stratification` for more details.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |  shuffle : bool, default=False\n",
      "     |      Whether to shuffle each class's samples before splitting into batches.\n",
      "     |      Note that the samples within each split will not be shuffled.\n",
      "     |      This implementation can only shuffle groups that have approximately the\n",
      "     |      same y distribution, no global shuffle will be performed.\n",
      "     |  \n",
      "     |  random_state : int or RandomState instance, default=None\n",
      "     |      When `shuffle` is True, `random_state` affects the ordering of the\n",
      "     |      indices, which controls the randomness of each fold for each class.\n",
      "     |      Otherwise, leave `random_state` as `None`.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import StratifiedGroupKFold\n",
      "     |  >>> X = np.ones((17, 2))\n",
      "     |  >>> y = np.array([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "     |  >>> groups = np.array([1, 1, 2, 2, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 7, 8, 8])\n",
      "     |  >>> sgkf = StratifiedGroupKFold(n_splits=3)\n",
      "     |  >>> sgkf.get_n_splits(X, y)\n",
      "     |  3\n",
      "     |  >>> print(sgkf)\n",
      "     |  StratifiedGroupKFold(n_splits=3, random_state=None, shuffle=False)\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(sgkf.split(X, y, groups)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}\")\n",
      "     |  ...     print(f\"         group={groups[train_index]}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}\")\n",
      "     |  ...     print(f\"         group={groups[test_index]}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[ 0  1  2  3  7  8  9 10 11 15 16]\n",
      "     |           group=[1 1 2 2 4 5 5 5 5 8 8]\n",
      "     |    Test:  index=[ 4  5  6 12 13 14]\n",
      "     |           group=[3 3 3 6 6 7]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[ 4  5  6  7  8  9 10 11 12 13 14]\n",
      "     |           group=[3 3 3 4 5 5 5 5 6 6 7]\n",
      "     |    Test:  index=[ 0  1  2  3 15 16]\n",
      "     |           group=[1 1 2 2 8 8]\n",
      "     |  Fold 2:\n",
      "     |    Train: index=[ 0  1  2  3  4  5  6 12 13 14 15 16]\n",
      "     |           group=[1 1 2 2 3 3 3 6 6 7 8 8]\n",
      "     |    Test:  index=[ 7  8  9 10 11]\n",
      "     |           group=[4 5 5 5 5]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The implementation is designed to:\n",
      "     |  \n",
      "     |  * Mimic the behavior of StratifiedKFold as much as possible for trivial\n",
      "     |    groups (e.g. when each group contains only one sample).\n",
      "     |  * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n",
      "     |    ``y = [1, 0]`` should not change the indices generated.\n",
      "     |  * Stratify based on samples as much as possible while keeping\n",
      "     |    non-overlapping groups constraint. That means that in some cases when\n",
      "     |    there is a small number of groups containing a large number of samples\n",
      "     |    the stratification will not be possible and the behavior will be close\n",
      "     |    to GroupKFold.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  StratifiedKFold: Takes class information into account to build folds which\n",
      "     |      retain class distributions (for binary or multiclass classification\n",
      "     |      tasks).\n",
      "     |  \n",
      "     |  GroupKFold: K-fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StratifiedGroupKFold\n",
      "     |      GroupsConsumerMixin\n",
      "     |      _BaseKFold\n",
      "     |      BaseCrossValidator\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5, shuffle=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_split_request(self: sklearn.model_selection._split.StratifiedGroupKFold, *, groups: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.model_selection._split.StratifiedGroupKFold from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``split`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``split`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``split``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      groups : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``groups`` parameter in ``split``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseKFold:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class StratifiedKFold(_BaseKFold)\n",
      "     |  StratifiedKFold(n_splits=5, *, shuffle=False, random_state=None)\n",
      "     |  \n",
      "     |  Class-wise stratified K-Fold cross-validator.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train/test sets.\n",
      "     |  \n",
      "     |  This cross-validation object is a variation of KFold that returns\n",
      "     |  stratified folds. The folds are made by preserving the percentage of\n",
      "     |  samples for each class in `y` in a binary or multiclass classification\n",
      "     |  setting.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <stratified_k_fold>`.\n",
      "     |  \n",
      "     |  For visualisation of cross-validation behaviour and\n",
      "     |  comparison between common scikit-learn split methods\n",
      "     |  refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      Stratification on the class label solves an engineering problem rather\n",
      "     |      than a statistical one. See :ref:`stratification` for more details.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``n_splits`` default value changed from 3 to 5.\n",
      "     |  \n",
      "     |  shuffle : bool, default=False\n",
      "     |      Whether to shuffle each class's samples before splitting into batches.\n",
      "     |      Note that the samples within each split will not be shuffled.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      When `shuffle` is True, `random_state` affects the ordering of the\n",
      "     |      indices, which controls the randomness of each fold for each class.\n",
      "     |      Otherwise, leave `random_state` as `None`.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import StratifiedKFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> skf = StratifiedKFold(n_splits=2)\n",
      "     |  >>> skf.get_n_splits(X, y)\n",
      "     |  2\n",
      "     |  >>> print(skf)\n",
      "     |  StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[1 3]\n",
      "     |    Test:  index=[0 2]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[0 2]\n",
      "     |    Test:  index=[1 3]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The implementation is designed to:\n",
      "     |  \n",
      "     |  * Generate test sets such that all contain the same distribution of\n",
      "     |    classes, or as close as possible.\n",
      "     |  * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n",
      "     |    ``y = [1, 0]`` should not change the indices generated.\n",
      "     |  * Preserve order dependencies in the dataset ordering, when\n",
      "     |    ``shuffle=False``: all samples from class k in some test set were\n",
      "     |    contiguous in y, or separated in y by samples from classes other than k.\n",
      "     |  * Generate test sets where the smallest and largest differ by at most one\n",
      "     |    sample.\n",
      "     |  \n",
      "     |  .. versionchanged:: 0.22\n",
      "     |      The previous implementation did not follow the last constraint.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StratifiedKFold\n",
      "     |      _BaseKFold\n",
      "     |      BaseCrossValidator\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5, *, shuffle=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  split(self, X, y, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |          Note that providing ``y`` is sufficient to generate the splits and\n",
      "     |          hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
      "     |          ``X`` instead of actual training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |          Stratification is done based on the y labels.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Randomized CV splitters may return different results for each call of\n",
      "     |      split. You can make the results identical by setting `random_state`\n",
      "     |      to an integer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseKFold:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class StratifiedShuffleSplit(BaseShuffleSplit)\n",
      "     |  StratifiedShuffleSplit(n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |  \n",
      "     |  Class-wise stratified ShuffleSplit cross-validator.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train/test sets.\n",
      "     |  \n",
      "     |  This cross-validation object is a merge of :class:`StratifiedKFold` and\n",
      "     |  :class:`ShuffleSplit`, which returns stratified randomized folds. The folds\n",
      "     |  are made by preserving the percentage of samples for each class in `y` in a\n",
      "     |  binary or multiclass classification setting.\n",
      "     |  \n",
      "     |  Note: like the :class:`ShuffleSplit` strategy, stratified random splits\n",
      "     |  do not guarantee that test sets across all folds will be mutually exclusive,\n",
      "     |  and might include overlapping samples. However, this is still very likely for\n",
      "     |  sizeable datasets.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <stratified_shuffle_split>`.\n",
      "     |  \n",
      "     |  For visualisation of cross-validation behaviour and\n",
      "     |  comparison between common scikit-learn split methods\n",
      "     |  refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      Stratification on the class label solves an engineering problem rather\n",
      "     |      than a statistical one. See :ref:`stratification` for more details.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=10\n",
      "     |      Number of re-shuffling & splitting iterations.\n",
      "     |  \n",
      "     |  test_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "     |      of the dataset to include in the test split. If int, represents the\n",
      "     |      absolute number of test samples. If None, the value is set to the\n",
      "     |      complement of the train size. If ``train_size`` is also None, it will\n",
      "     |      be set to 0.1.\n",
      "     |  \n",
      "     |  train_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the dataset to include in the train split. If\n",
      "     |      int, represents the absolute number of train samples. If None,\n",
      "     |      the value is automatically set to the complement of the test size.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of the training and testing indices produced.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import StratifiedShuffleSplit\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 0, 1, 1, 1])\n",
      "     |  >>> sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
      "     |  >>> sss.get_n_splits(X, y)\n",
      "     |  5\n",
      "     |  >>> print(sss)\n",
      "     |  StratifiedShuffleSplit(n_splits=5, random_state=0, ...)\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[5 2 3]\n",
      "     |    Test:  index=[4 1 0]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[5 1 4]\n",
      "     |    Test:  index=[0 2 3]\n",
      "     |  Fold 2:\n",
      "     |    Train: index=[5 0 2]\n",
      "     |    Test:  index=[4 3 1]\n",
      "     |  Fold 3:\n",
      "     |    Train: index=[4 1 0]\n",
      "     |    Test:  index=[2 3 5]\n",
      "     |  Fold 4:\n",
      "     |    Train: index=[0 5 1]\n",
      "     |    Test:  index=[3 4 2]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StratifiedShuffleSplit\n",
      "     |      BaseShuffleSplit\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  split(self, X, y, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |          Note that providing ``y`` is sufficient to generate the splits and\n",
      "     |          hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
      "     |          ``X`` instead of actual training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_labels)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |          Stratification is done based on the y labels.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Randomized CV splitters may return different results for each call of\n",
      "     |      split. You can make the results identical by setting `random_state`\n",
      "     |      to an integer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class TimeSeriesSplit(_BaseKFold)\n",
      "     |  TimeSeriesSplit(n_splits=5, *, max_train_size=None, test_size=None, gap=0)\n",
      "     |  \n",
      "     |  Time Series cross-validator.\n",
      "     |  \n",
      "     |  Provides train/test indices to split time-ordered data, where other\n",
      "     |  cross-validation methods are inappropriate, as they would lead to training\n",
      "     |  on future data and evaluating on past data.\n",
      "     |  To ensure comparable metrics across folds, samples must be equally spaced.\n",
      "     |  Once this condition is met, each test set covers the same time duration,\n",
      "     |  while the train set size accumulates data from previous splits.\n",
      "     |  \n",
      "     |  This cross-validation object is a variation of :class:`KFold`.\n",
      "     |  In the k-th split, it returns the first k folds as the train set and the\n",
      "     |  (k+1)-th fold as the test set.\n",
      "     |  \n",
      "     |  Note that, unlike standard cross-validation methods, successive\n",
      "     |  training sets are supersets of those that come before them.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <time_series_split>`.\n",
      "     |  \n",
      "     |  For visualisation of cross-validation behaviour and\n",
      "     |  comparison between common scikit-learn split methods\n",
      "     |  refer to :ref:`sphx_glr_auto_examples_model_selection_plot_cv_indices.py`\n",
      "     |  \n",
      "     |  .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of splits. Must be at least 2.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``n_splits`` default value changed from 3 to 5.\n",
      "     |  \n",
      "     |  max_train_size : int, default=None\n",
      "     |      Maximum size for a single training set.\n",
      "     |  \n",
      "     |  test_size : int, default=None\n",
      "     |      Used to limit the size of the test set. Defaults to\n",
      "     |      ``n_samples // (n_splits + 1)``, which is the maximum allowed value\n",
      "     |      with ``gap=0``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  gap : int, default=0\n",
      "     |      Number of samples to exclude from the end of each train set before\n",
      "     |      the test set.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import TimeSeriesSplit\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      "     |  >>> tscv = TimeSeriesSplit()\n",
      "     |  >>> print(tscv)\n",
      "     |  TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[0]\n",
      "     |    Test:  index=[1]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[0 1]\n",
      "     |    Test:  index=[2]\n",
      "     |  Fold 2:\n",
      "     |    Train: index=[0 1 2]\n",
      "     |    Test:  index=[3]\n",
      "     |  Fold 3:\n",
      "     |    Train: index=[0 1 2 3]\n",
      "     |    Test:  index=[4]\n",
      "     |  Fold 4:\n",
      "     |    Train: index=[0 1 2 3 4]\n",
      "     |    Test:  index=[5]\n",
      "     |  >>> # Fix test_size to 2 with 12 samples\n",
      "     |  >>> X = np.random.randn(12, 2)\n",
      "     |  >>> y = np.random.randint(0, 2, 12)\n",
      "     |  >>> tscv = TimeSeriesSplit(n_splits=3, test_size=2)\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[0 1 2 3 4 5]\n",
      "     |    Test:  index=[6 7]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[0 1 2 3 4 5 6 7]\n",
      "     |    Test:  index=[8 9]\n",
      "     |  Fold 2:\n",
      "     |    Train: index=[0 1 2 3 4 5 6 7 8 9]\n",
      "     |    Test:  index=[10 11]\n",
      "     |  >>> # Add in a 2 period gap\n",
      "     |  >>> tscv = TimeSeriesSplit(n_splits=3, test_size=2, gap=2)\n",
      "     |  >>> for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
      "     |  ...     print(f\"Fold {i}:\")\n",
      "     |  ...     print(f\"  Train: index={train_index}\")\n",
      "     |  ...     print(f\"  Test:  index={test_index}\")\n",
      "     |  Fold 0:\n",
      "     |    Train: index=[0 1 2 3]\n",
      "     |    Test:  index=[6 7]\n",
      "     |  Fold 1:\n",
      "     |    Train: index=[0 1 2 3 4 5]\n",
      "     |    Test:  index=[8 9]\n",
      "     |  Fold 2:\n",
      "     |    Train: index=[0 1 2 3 4 5 6 7]\n",
      "     |    Test:  index=[10 11]\n",
      "     |  \n",
      "     |  For a more extended example see\n",
      "     |  :ref:`sphx_glr_auto_examples_applications_plot_cyclical_feature_engineering.py`.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The training set has size ``i * n_samples // (n_splits + 1)\n",
      "     |  + n_samples % (n_splits + 1)`` in the ``i`` th split,\n",
      "     |  with a test set of size ``n_samples//(n_splits + 1)`` by default,\n",
      "     |  where ``n_samples`` is the number of samples. Note that this\n",
      "     |  formula is only valid when ``test_size`` and ``max_train_size`` are\n",
      "     |  left to their default values.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TimeSeriesSplit\n",
      "     |      _BaseKFold\n",
      "     |      BaseCrossValidator\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5, *, max_train_size=None, test_size=None, gap=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseKFold:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class TunedThresholdClassifierCV(BaseThresholdClassifier)\n",
      "     |  TunedThresholdClassifierCV(estimator, *, scoring='balanced_accuracy', response_method='auto', thresholds=100, cv=None, refit=True, n_jobs=None, random_state=None, store_cv_results=False)\n",
      "     |  \n",
      "     |  Classifier that post-tunes the decision threshold using cross-validation.\n",
      "     |  \n",
      "     |  This estimator post-tunes the decision threshold (cut-off point) that is\n",
      "     |  used for converting posterior probability estimates (i.e. output of\n",
      "     |  `predict_proba`) or decision scores (i.e. output of `decision_function`)\n",
      "     |  into a class label. The tuning is done by optimizing a binary metric,\n",
      "     |  potentially constrained by a another metric.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <TunedThresholdClassifierCV>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.5\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : estimator instance\n",
      "     |      The classifier, fitted or not, for which we want to optimize\n",
      "     |      the decision threshold used during `predict`.\n",
      "     |  \n",
      "     |  scoring : str or callable, default=\"balanced_accuracy\"\n",
      "     |      The objective metric to be optimized. Can be one of:\n",
      "     |  \n",
      "     |      - str: string associated to a scoring function for binary classification,\n",
      "     |        see :ref:`scoring_string_names` for options.\n",
      "     |      - callable: a scorer callable object (e.g., function) with signature\n",
      "     |        ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n",
      "     |  \n",
      "     |  response_method : {\"auto\", \"decision_function\", \"predict_proba\"}, default=\"auto\"\n",
      "     |      Methods by the classifier `estimator` corresponding to the\n",
      "     |      decision function for which we want to find a threshold. It can be:\n",
      "     |  \n",
      "     |      * if `\"auto\"`, it will try to invoke, for each classifier,\n",
      "     |        `\"predict_proba\"` or `\"decision_function\"` in that order.\n",
      "     |      * otherwise, one of `\"predict_proba\"` or `\"decision_function\"`.\n",
      "     |        If the method is not implemented by the classifier, it will raise an\n",
      "     |        error.\n",
      "     |  \n",
      "     |  thresholds : int or array-like, default=100\n",
      "     |      The number of decision threshold to use when discretizing the output of the\n",
      "     |      classifier `method`. Pass an array-like to manually specify the thresholds\n",
      "     |      to use.\n",
      "     |  \n",
      "     |  cv : int, float, cross-validation generator, iterable or \"prefit\", default=None\n",
      "     |      Determines the cross-validation splitting strategy to train classifier.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      * `None`, to use the default 5-fold stratified K-fold cross validation;\n",
      "     |      * An integer number, to specify the number of folds in a stratified k-fold;\n",
      "     |      * A float number, to specify a single shuffle split. The floating number should\n",
      "     |        be in (0, 1) and represent the size of the validation set;\n",
      "     |      * An object to be used as a cross-validation generator;\n",
      "     |      * An iterable yielding train, test splits;\n",
      "     |      * `\"prefit\"`, to bypass the cross-validation.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. warning::\n",
      "     |          Using `cv=\"prefit\"` and passing the same dataset for fitting `estimator`\n",
      "     |          and tuning the cut-off point is subject to undesired overfitting. You can\n",
      "     |          refer to :ref:`TunedThresholdClassifierCV_no_cv` for an example.\n",
      "     |  \n",
      "     |          This option should only be used when the set used to fit `estimator` is\n",
      "     |          different from the one used to tune the cut-off point (by calling\n",
      "     |          :meth:`TunedThresholdClassifierCV.fit`).\n",
      "     |  \n",
      "     |  refit : bool, default=True\n",
      "     |      Whether or not to refit the classifier on the entire training set once\n",
      "     |      the decision threshold has been found.\n",
      "     |      Note that forcing `refit=False` on cross-validation having more\n",
      "     |      than a single split will raise an error. Similarly, `refit=True` in\n",
      "     |      conjunction with `cv=\"prefit\"` will raise an error.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel. When `cv` represents a\n",
      "     |      cross-validation strategy, the fitting and scoring on each data split\n",
      "     |      is done in parallel. ``None`` means 1 unless in a\n",
      "     |      :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      "     |      processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of cross-validation when `cv` is a float.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  store_cv_results : bool, default=False\n",
      "     |      Whether to store all scores and thresholds computed during the cross-validation\n",
      "     |      process.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : estimator instance\n",
      "     |      The fitted classifier used when predicting.\n",
      "     |  \n",
      "     |  best_threshold_ : float\n",
      "     |      The new decision threshold.\n",
      "     |  \n",
      "     |  best_score_ : float or None\n",
      "     |      The optimal score of the objective metric, evaluated at `best_threshold_`.\n",
      "     |  \n",
      "     |  cv_results_ : dict or None\n",
      "     |      A dictionary containing the scores and thresholds computed during the\n",
      "     |      cross-validation process. Only exist if `store_cv_results=True`. The\n",
      "     |      keys are `\"thresholds\"` and `\"scores\"`.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The class labels.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`. Only defined if the\n",
      "     |      underlying estimator exposes such an attribute when fit.\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Only defined if the\n",
      "     |      underlying estimator exposes such an attribute when fit.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.model_selection.FixedThresholdClassifier : Classifier that uses a\n",
      "     |      constant threshold.\n",
      "     |  sklearn.calibration.CalibratedClassifierCV : Estimator that calibrates\n",
      "     |      probabilities.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      "     |  >>> from sklearn.metrics import classification_report\n",
      "     |  >>> from sklearn.model_selection import TunedThresholdClassifierCV, train_test_split\n",
      "     |  >>> X, y = make_classification(\n",
      "     |  ...     n_samples=1_000, weights=[0.9, 0.1], class_sep=0.8, random_state=42\n",
      "     |  ... )\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...     X, y, stratify=y, random_state=42\n",
      "     |  ... )\n",
      "     |  >>> classifier = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n",
      "     |  >>> print(classification_report(y_test, classifier.predict(X_test)))\n",
      "     |                precision    recall  f1-score   support\n",
      "     |  <BLANKLINE>\n",
      "     |             0       0.94      0.99      0.96       224\n",
      "     |             1       0.80      0.46      0.59        26\n",
      "     |  <BLANKLINE>\n",
      "     |      accuracy                           0.93       250\n",
      "     |     macro avg       0.87      0.72      0.77       250\n",
      "     |  weighted avg       0.93      0.93      0.92       250\n",
      "     |  <BLANKLINE>\n",
      "     |  >>> classifier_tuned = TunedThresholdClassifierCV(\n",
      "     |  ...     classifier, scoring=\"balanced_accuracy\"\n",
      "     |  ... ).fit(X_train, y_train)\n",
      "     |  >>> print(\n",
      "     |  ...     f\"Cut-off point found at {classifier_tuned.best_threshold_:.3f}\"\n",
      "     |  ... )\n",
      "     |  Cut-off point found at 0.342\n",
      "     |  >>> print(classification_report(y_test, classifier_tuned.predict(X_test)))\n",
      "     |                precision    recall  f1-score   support\n",
      "     |  <BLANKLINE>\n",
      "     |             0       0.96      0.95      0.96       224\n",
      "     |             1       0.61      0.65      0.63        26\n",
      "     |  <BLANKLINE>\n",
      "     |      accuracy                           0.92       250\n",
      "     |     macro avg       0.78      0.80      0.79       250\n",
      "     |  weighted avg       0.92      0.92      0.92       250\n",
      "     |  <BLANKLINE>\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TunedThresholdClassifierCV\n",
      "     |      BaseThresholdClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator, *, scoring='balanced_accuracy', response_method='auto', thresholds=100, cv=None, refit=True, n_jobs=None, random_state=None, store_cv_results=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict the target of new samples.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The samples, as accepted by `estimator.predict`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      class_labels : ndarray of shape (n_samples,)\n",
      "     |          The predicted class.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseThresholdClassifier:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Decision function for samples in `X` using the fitted estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      decisions : ndarray of shape (n_samples,)\n",
      "     |          The decision function computed the fitted estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, **params)\n",
      "     |      Fit the classifier.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      **params : dict\n",
      "     |          Parameters to pass to the `fit` method of the underlying\n",
      "     |          classifier.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict logarithm class probabilities for `X` using the fitted estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      log_probabilities : ndarray of shape (n_samples, n_classes)\n",
      "     |          The logarithm class probabilities of the input samples.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for `X` using the fitted estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      probabilities : ndarray of shape (n_samples, n_classes)\n",
      "     |          The class probabilities of the input samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseThresholdClassifier:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |      Classes labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class ValidationCurveDisplay(_BaseCurveDisplay)\n",
      "     |  ValidationCurveDisplay(*, param_name, param_range, train_scores, test_scores, score_name=None)\n",
      "     |  \n",
      "     |  Validation Curve visualization.\n",
      "     |  \n",
      "     |  It is recommended to use\n",
      "     |  :meth:`~sklearn.model_selection.ValidationCurveDisplay.from_estimator` to\n",
      "     |  create a :class:`~sklearn.model_selection.ValidationCurveDisplay` instance.\n",
      "     |  All parameters are stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>` for general information\n",
      "     |  about the visualization API and :ref:`detailed documentation\n",
      "     |  <validation_curve>` regarding the validation curve visualization.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  param_name : str\n",
      "     |      Name of the parameter that has been varied.\n",
      "     |  \n",
      "     |  param_range : array-like of shape (n_ticks,)\n",
      "     |      The values of the parameter that have been evaluated.\n",
      "     |  \n",
      "     |  train_scores : ndarray of shape (n_ticks, n_cv_folds)\n",
      "     |      Scores on training sets.\n",
      "     |  \n",
      "     |  test_scores : ndarray of shape (n_ticks, n_cv_folds)\n",
      "     |      Scores on test set.\n",
      "     |  \n",
      "     |  score_name : str, default=None\n",
      "     |      The name of the score used in `validation_curve`. It will override the name\n",
      "     |      inferred from the `scoring` parameter. If `score` is `None`, we use `\"Score\"` if\n",
      "     |      `negate_score` is `False` and `\"Negative score\"` otherwise. If `scoring` is a\n",
      "     |      string or a callable, we infer the name. We replace `_` by spaces and capitalize\n",
      "     |      the first letter. We remove `neg_` and replace it by `\"Negative\"` if\n",
      "     |      `negate_score` is `False` or just remove it otherwise.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with the validation curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the validation curve.\n",
      "     |  \n",
      "     |  errorbar_ : list of matplotlib Artist or None\n",
      "     |      When the `std_display_style` is `\"errorbar\"`, this is a list of\n",
      "     |      `matplotlib.container.ErrorbarContainer` objects. If another style is\n",
      "     |      used, `errorbar_` is `None`.\n",
      "     |  \n",
      "     |  lines_ : list of matplotlib Artist or None\n",
      "     |      When the `std_display_style` is `\"fill_between\"`, this is a list of\n",
      "     |      `matplotlib.lines.Line2D` objects corresponding to the mean train and\n",
      "     |      test scores. If another style is used, `line_` is `None`.\n",
      "     |  \n",
      "     |  fill_between_ : list of matplotlib Artist or None\n",
      "     |      When the `std_display_style` is `\"fill_between\"`, this is a list of\n",
      "     |      `matplotlib.collections.PolyCollection` objects. If another style is\n",
      "     |      used, `fill_between_` is `None`.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.model_selection.validation_curve : Compute the validation curve.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.model_selection import ValidationCurveDisplay, validation_curve\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> X, y = make_classification(n_samples=1_000, random_state=0)\n",
      "     |  >>> logistic_regression = LogisticRegression()\n",
      "     |  >>> param_name, param_range = \"C\", np.logspace(-8, 3, 10)\n",
      "     |  >>> train_scores, test_scores = validation_curve(\n",
      "     |  ...     logistic_regression, X, y, param_name=param_name, param_range=param_range\n",
      "     |  ... )\n",
      "     |  >>> display = ValidationCurveDisplay(\n",
      "     |  ...     param_name=param_name, param_range=param_range,\n",
      "     |  ...     train_scores=train_scores, test_scores=test_scores, score_name=\"Score\"\n",
      "     |  ... )\n",
      "     |  >>> display.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ValidationCurveDisplay\n",
      "     |      _BaseCurveDisplay\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, param_name, param_range, train_scores, test_scores, score_name=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, negate_score=False, score_name=None, score_type='both', std_display_style='fill_between', line_kw=None, fill_between_kw=None, errorbar_kw=None)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      negate_score : bool, default=False\n",
      "     |          Whether or not to negate the scores obtained through\n",
      "     |          :func:`~sklearn.model_selection.validation_curve`. This is\n",
      "     |          particularly useful when using the error denoted by `neg_*` in\n",
      "     |          `scikit-learn`.\n",
      "     |      \n",
      "     |      score_name : str, default=None\n",
      "     |          The name of the score used to decorate the y-axis of the plot. It will\n",
      "     |          override the name inferred from the `scoring` parameter. If `score` is\n",
      "     |          `None`, we use `\"Score\"` if `negate_score` is `False` and `\"Negative score\"`\n",
      "     |          otherwise. If `scoring` is a string or a callable, we infer the name. We\n",
      "     |          replace `_` by spaces and capitalize the first letter. We remove `neg_` and\n",
      "     |          replace it by `\"Negative\"` if `negate_score` is\n",
      "     |          `False` or just remove it otherwise.\n",
      "     |      \n",
      "     |      score_type : {\"test\", \"train\", \"both\"}, default=\"both\"\n",
      "     |          The type of score to plot. Can be one of `\"test\"`, `\"train\"`, or\n",
      "     |          `\"both\"`.\n",
      "     |      \n",
      "     |      std_display_style : {\"errorbar\", \"fill_between\"} or None, default=\"fill_between\"\n",
      "     |          The style used to display the score standard deviation around the\n",
      "     |          mean score. If None, no standard deviation representation is\n",
      "     |          displayed.\n",
      "     |      \n",
      "     |      line_kw : dict, default=None\n",
      "     |          Additional keyword arguments passed to the `plt.plot` used to draw\n",
      "     |          the mean score.\n",
      "     |      \n",
      "     |      fill_between_kw : dict, default=None\n",
      "     |          Additional keyword arguments passed to the `plt.fill_between` used\n",
      "     |          to draw the score standard deviation.\n",
      "     |      \n",
      "     |      errorbar_kw : dict, default=None\n",
      "     |          Additional keyword arguments passed to the `plt.errorbar` used to\n",
      "     |          draw mean score and standard deviation score.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.model_selection.ValidationCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, param_name, param_range, groups=None, cv=None, scoring=None, n_jobs=None, pre_dispatch='all', verbose=0, error_score=nan, fit_params=None, ax=None, negate_score=False, score_name=None, score_type='both', std_display_style='fill_between', line_kw=None, fill_between_kw=None, errorbar_kw=None)\n",
      "     |      Create a validation curve display from an estimator.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <visualizations>` for general\n",
      "     |      information about the visualization API and :ref:`detailed\n",
      "     |      documentation <validation_curve>` regarding the validation curve\n",
      "     |      visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : object type that implements the \"fit\" and \"predict\" methods\n",
      "     |          An object of that type which is cloned for each validation.\n",
      "     |      \n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n",
      "     |          Target relative to X for classification or regression;\n",
      "     |          None for unsupervised learning.\n",
      "     |      \n",
      "     |      param_name : str\n",
      "     |          Name of the parameter that will be varied.\n",
      "     |      \n",
      "     |      param_range : array-like of shape (n_values,)\n",
      "     |          The values of the parameter that will be evaluated.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "     |          instance (e.g., :class:`GroupKFold`).\n",
      "     |      \n",
      "     |      cv : int, cross-validation generator or an iterable, default=None\n",
      "     |          Determines the cross-validation splitting strategy.\n",
      "     |          Possible inputs for cv are:\n",
      "     |      \n",
      "     |          - None, to use the default 5-fold cross validation,\n",
      "     |          - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "     |          - :term:`CV splitter`,\n",
      "     |          - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |      \n",
      "     |          For int/None inputs, if the estimator is a classifier and `y` is\n",
      "     |          either binary or multiclass,\n",
      "     |          :class:`~sklearn.model_selection.StratifiedKFold` is used. In all\n",
      "     |          other cases, :class:`~sklearn.model_selection.KFold` is used. These\n",
      "     |          splitters are instantiated with `shuffle=False` so the splits will\n",
      "     |          be the same across calls.\n",
      "     |      \n",
      "     |          Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |          cross-validation strategies that can be used here.\n",
      "     |      \n",
      "     |      scoring : str or callable, default=None\n",
      "     |          Scoring method to use when computing the validation curve. Options:\n",
      "     |      \n",
      "     |          - str: see :ref:`scoring_string_names` for options.\n",
      "     |          - callable: a scorer callable object (e.g., function) with signature\n",
      "     |            ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n",
      "     |          - `None`: the `estimator`'s\n",
      "     |            :ref:`default evaluation criterion <scoring_api_overview>` is used.\n",
      "     |      \n",
      "     |      n_jobs : int, default=None\n",
      "     |          Number of jobs to run in parallel. Training the estimator and\n",
      "     |          computing the score are parallelized over the different training\n",
      "     |          and test sets. `None` means 1 unless in a\n",
      "     |          :obj:`joblib.parallel_backend` context. `-1` means using all\n",
      "     |          processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "     |      \n",
      "     |      pre_dispatch : int or str, default='all'\n",
      "     |          Number of predispatched jobs for parallel execution (default is\n",
      "     |          all). The option can reduce the allocated memory. The str can\n",
      "     |          be an expression like '2*n_jobs'.\n",
      "     |      \n",
      "     |      verbose : int, default=0\n",
      "     |          Controls the verbosity: the higher, the more messages.\n",
      "     |      \n",
      "     |      error_score : 'raise' or numeric, default=np.nan\n",
      "     |          Value to assign to the score if an error occurs in estimator\n",
      "     |          fitting. If set to 'raise', the error is raised. If a numeric value\n",
      "     |          is given, FitFailedWarning is raised.\n",
      "     |      \n",
      "     |      fit_params : dict, default=None\n",
      "     |          Parameters to pass to the fit method of the estimator.\n",
      "     |      \n",
      "     |      ax : matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      negate_score : bool, default=False\n",
      "     |          Whether or not to negate the scores obtained through\n",
      "     |          :func:`~sklearn.model_selection.validation_curve`. This is\n",
      "     |          particularly useful when using the error denoted by `neg_*` in\n",
      "     |          `scikit-learn`.\n",
      "     |      \n",
      "     |      score_name : str, default=None\n",
      "     |          The name of the score used to decorate the y-axis of the plot. It will\n",
      "     |          override the name inferred from the `scoring` parameter. If `score` is\n",
      "     |          `None`, we use `\"Score\"` if `negate_score` is `False` and `\"Negative score\"`\n",
      "     |          otherwise. If `scoring` is a string or a callable, we infer the name. We\n",
      "     |          replace `_` by spaces and capitalize the first letter. We remove `neg_` and\n",
      "     |          replace it by `\"Negative\"` if `negate_score` is\n",
      "     |          `False` or just remove it otherwise.\n",
      "     |      \n",
      "     |      score_type : {\"test\", \"train\", \"both\"}, default=\"both\"\n",
      "     |          The type of score to plot. Can be one of `\"test\"`, `\"train\"`, or\n",
      "     |          `\"both\"`.\n",
      "     |      \n",
      "     |      std_display_style : {\"errorbar\", \"fill_between\"} or None, default=\"fill_between\"\n",
      "     |          The style used to display the score standard deviation around the\n",
      "     |          mean score. If `None`, no representation of the standard deviation\n",
      "     |          is displayed.\n",
      "     |      \n",
      "     |      line_kw : dict, default=None\n",
      "     |          Additional keyword arguments passed to the `plt.plot` used to draw\n",
      "     |          the mean score.\n",
      "     |      \n",
      "     |      fill_between_kw : dict, default=None\n",
      "     |          Additional keyword arguments passed to the `plt.fill_between` used\n",
      "     |          to draw the score standard deviation.\n",
      "     |      \n",
      "     |      errorbar_kw : dict, default=None\n",
      "     |          Additional keyword arguments passed to the `plt.errorbar` used to\n",
      "     |          draw mean score and standard deviation score.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.model_selection.ValidationCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import numpy as np\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.model_selection import ValidationCurveDisplay\n",
      "     |      >>> from sklearn.linear_model import LogisticRegression\n",
      "     |      >>> X, y = make_classification(n_samples=1_000, random_state=0)\n",
      "     |      >>> logistic_regression = LogisticRegression()\n",
      "     |      >>> param_name, param_range = \"C\", np.logspace(-8, 3, 10)\n",
      "     |      >>> ValidationCurveDisplay.from_estimator(\n",
      "     |      ...     logistic_regression, X, y, param_name=param_name,\n",
      "     |      ...     param_range=param_range,\n",
      "     |      ... )\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _BaseCurveDisplay:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "FUNCTIONS\n",
      "    __getattr__(name)\n",
      "        # TODO: remove this check once the estimator is no longer experimental.\n",
      "    \n",
      "    check_cv(cv=5, y=None, *, classifier=False)\n",
      "        Input checker utility for building a cross-validator.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cv : int, cross-validation generator, iterable or None, default=5\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - integer, to specify the number of folds.\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable that generates (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For integer/None inputs, if classifier is True and ``y`` is either\n",
      "            binary or multiclass, :class:`StratifiedKFold` is used. In all other\n",
      "            cases, :class:`KFold` is used.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value changed from 3-fold to 5-fold.\n",
      "        \n",
      "        y : array-like, default=None\n",
      "            The target variable for supervised learning problems.\n",
      "        \n",
      "        classifier : bool, default=False\n",
      "            Whether the task is a classification task, in which case\n",
      "            stratified KFold will be used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        checked_cv : a cross-validator instance.\n",
      "            The return value is a cross-validator which generates the train/test\n",
      "            splits via the ``split`` method.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.model_selection import check_cv\n",
      "        >>> check_cv(cv=5, y=None, classifier=False)\n",
      "        KFold(...)\n",
      "        >>> check_cv(cv=5, y=[1, 1, 0, 0, 0, 0], classifier=True)\n",
      "        StratifiedKFold(...)\n",
      "    \n",
      "    cross_val_predict(estimator, X, y=None, *, groups=None, cv=None, n_jobs=None, verbose=0, params=None, pre_dispatch='2*n_jobs', method='predict')\n",
      "        Generate cross-validated estimates for each input data point.\n",
      "        \n",
      "        The data is split according to the cv parameter. Each sample belongs\n",
      "        to exactly one test set, and its prediction is computed with an\n",
      "        estimator fitted on the corresponding training set.\n",
      "        \n",
      "        Passing these predictions into an evaluation metric may not be a valid\n",
      "        way to measure generalization performance. Results can differ from\n",
      "        :func:`cross_validate` and :func:`cross_val_score` unless all tests sets\n",
      "        have equal size and the metric decomposes over samples.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator\n",
      "            The estimator instance to use to fit the data. It must implement a `fit`\n",
      "            method and the method given by the `method` parameter.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to fit. Can be, for example a list, or an array at least 2d.\n",
      "        \n",
      "        y : {array-like, sparse matrix} of shape (n_samples,) or (n_samples, n_outputs),             default=None\n",
      "            The target variable to try to predict in the case of\n",
      "            supervised learning.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Group labels for the samples used while splitting the dataset into\n",
      "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "            instance (e.g., :class:`GroupKFold`).\n",
      "        \n",
      "            .. versionchanged:: 1.4\n",
      "                ``groups`` can only be passed if metadata routing is not enabled\n",
      "                via ``sklearn.set_config(enable_metadata_routing=True)``. When routing\n",
      "                is enabled, pass ``groups`` alongside other metadata via the ``params``\n",
      "                argument instead. E.g.:\n",
      "                ``cross_val_predict(..., params={'groups': groups})``.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable that generates (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and\n",
      "            predicting are parallelized over the cross-validation splits.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            The verbosity level.\n",
      "        \n",
      "        params : dict, default=None\n",
      "            Parameters to pass to the underlying estimator's ``fit`` and the CV\n",
      "            splitter.\n",
      "        \n",
      "            .. versionadded:: 1.4\n",
      "        \n",
      "        pre_dispatch : int or str, default='2*n_jobs'\n",
      "            Controls the number of jobs that get dispatched during parallel\n",
      "            execution. Reducing this number can be useful to avoid an\n",
      "            explosion of memory consumption when more jobs get dispatched\n",
      "            than CPUs can process. This parameter can be:\n",
      "        \n",
      "            - None, in which case all the jobs are immediately created and spawned. Use\n",
      "              this for lightweight and fast-running jobs, to avoid delays due to on-demand\n",
      "              spawning of the jobs\n",
      "            - An int, giving the exact number of total jobs that are spawned\n",
      "            - A str, giving an expression as a function of n_jobs, as in '2*n_jobs'\n",
      "        \n",
      "        method : {'predict', 'predict_proba', 'predict_log_proba',               'decision_function'}, default='predict'\n",
      "            The method to be invoked by `estimator`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        predictions : ndarray\n",
      "            This is the result of calling `method`. Shape:\n",
      "        \n",
      "            - When `method` is 'predict' and in special case where `method` is\n",
      "              'decision_function' and the target is binary: (n_samples,)\n",
      "            - When `method` is one of {'predict_proba', 'predict_log_proba',\n",
      "              'decision_function'} (unless special case above):\n",
      "              (n_samples, n_classes)\n",
      "            - If `estimator` is :term:`multioutput`, an extra dimension\n",
      "              'n_outputs' is added to the end of each shape above.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        cross_val_score : Calculate score for each CV split.\n",
      "        cross_validate : Calculate one or more scores and timings for each CV\n",
      "            split.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In the case that one or more classes are absent in a training portion, a\n",
      "        default score needs to be assigned to all instances for that class if\n",
      "        ``method`` produces columns per class, as in {'decision_function',\n",
      "        'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is\n",
      "        0.  In order to ensure finite output, we approximate negative infinity by\n",
      "        the minimum finite float value for the dtype in other cases.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import datasets, linear_model\n",
      "        >>> from sklearn.model_selection import cross_val_predict\n",
      "        >>> diabetes = datasets.load_diabetes()\n",
      "        >>> X = diabetes.data[:150]\n",
      "        >>> y = diabetes.target[:150]\n",
      "        >>> lasso = linear_model.Lasso()\n",
      "        >>> y_pred = cross_val_predict(lasso, X, y, cv=3)\n",
      "        \n",
      "        For a detailed example of using ``cross_val_predict`` to visualize\n",
      "        prediction errors, please see\n",
      "        :ref:`sphx_glr_auto_examples_model_selection_plot_cv_predict.py`.\n",
      "    \n",
      "    cross_val_score(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, params=None, pre_dispatch='2*n_jobs', error_score=nan)\n",
      "        Evaluate a score by cross-validation.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to fit. Can be for example a list, or an array.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n",
      "            The target variable to try to predict in the case of\n",
      "            supervised learning.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Group labels for the samples used while splitting the dataset into\n",
      "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "            instance (e.g., :class:`GroupKFold`).\n",
      "        \n",
      "            .. versionchanged:: 1.4\n",
      "                ``groups`` can only be passed if metadata routing is not enabled\n",
      "                via ``sklearn.set_config(enable_metadata_routing=True)``. When routing\n",
      "                is enabled, pass ``groups`` alongside other metadata via the ``params``\n",
      "                argument instead. E.g.:\n",
      "                ``cross_val_score(..., params={'groups': groups})``.\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            Strategy to evaluate the performance of the `estimator` across cross-validation\n",
      "            splits.\n",
      "        \n",
      "            - str: see :ref:`scoring_string_names` for options.\n",
      "            - callable: a scorer callable object (e.g., function) with signature\n",
      "              ``scorer(estimator, X, y)``, which should return only a single value.\n",
      "              See :ref:`scoring_callable` for details.\n",
      "            - `None`: the `estimator`'s\n",
      "              :ref:`default evaluation criterion <scoring_api_overview>` is used.\n",
      "        \n",
      "            Similar to the use of `scoring` in :func:`cross_validate` but only a\n",
      "            single metric is permitted.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - `None`, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable that generates (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For `int`/`None` inputs, if the estimator is a classifier and `y` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                `cv` default value if `None` changed from 3-fold to 5-fold.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and computing\n",
      "            the score are parallelized over the cross-validation splits.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            The verbosity level.\n",
      "        \n",
      "        params : dict, default=None\n",
      "            Parameters to pass to the underlying estimator's ``fit``, the scorer,\n",
      "            and the CV splitter.\n",
      "        \n",
      "            .. versionadded:: 1.4\n",
      "        \n",
      "        pre_dispatch : int or str, default='2*n_jobs'\n",
      "            Controls the number of jobs that get dispatched during parallel\n",
      "            execution. Reducing this number can be useful to avoid an\n",
      "            explosion of memory consumption when more jobs get dispatched\n",
      "            than CPUs can process. This parameter can be:\n",
      "        \n",
      "            - ``None``, in which case all the jobs are immediately created and spawned. Use\n",
      "              this for lightweight and fast-running jobs, to avoid delays due to on-demand\n",
      "              spawning of the jobs\n",
      "            - An int, giving the exact number of total jobs that are spawned\n",
      "            - A str, giving an expression as a function of n_jobs, as in '2*n_jobs'\n",
      "        \n",
      "        error_score : 'raise' or numeric, default=np.nan\n",
      "            Value to assign to the score if an error occurs in estimator fitting.\n",
      "            If set to 'raise', the error is raised.\n",
      "            If a numeric value is given, FitFailedWarning is raised.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scores : ndarray of float of shape=(len(list(cv)),)\n",
      "            Array of scores of the estimator for each run of the cross validation.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        cross_validate : To run cross-validation on multiple metrics and also to\n",
      "            return train scores, fit times and score times.\n",
      "        \n",
      "        cross_val_predict : Get predictions from each split of cross-validation for\n",
      "            diagnostic purposes.\n",
      "        \n",
      "        sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n",
      "            loss function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import datasets, linear_model\n",
      "        >>> from sklearn.model_selection import cross_val_score\n",
      "        >>> diabetes = datasets.load_diabetes()\n",
      "        >>> X = diabetes.data[:150]\n",
      "        >>> y = diabetes.target[:150]\n",
      "        >>> lasso = linear_model.Lasso()\n",
      "        >>> print(cross_val_score(lasso, X, y, cv=3))\n",
      "        [0.3315057  0.08022103 0.03531816]\n",
      "    \n",
      "    cross_validate(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, params=None, pre_dispatch='2*n_jobs', return_train_score=False, return_estimator=False, return_indices=False, error_score=nan)\n",
      "        Evaluate metric(s) by cross-validation and also record fit/score times.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <multimetric_cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to fit. Can be for example a list, or an array.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None\n",
      "            The target variable to try to predict in the case of\n",
      "            supervised learning.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Group labels for the samples used while splitting the dataset into\n",
      "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "            instance (e.g., :class:`GroupKFold`).\n",
      "        \n",
      "            .. versionchanged:: 1.4\n",
      "                ``groups`` can only be passed if metadata routing is not enabled\n",
      "                via ``sklearn.set_config(enable_metadata_routing=True)``. When routing\n",
      "                is enabled, pass ``groups`` alongside other metadata via the ``params``\n",
      "                argument instead. E.g.:\n",
      "                ``cross_validate(..., params={'groups': groups})``.\n",
      "        \n",
      "        scoring : str, callable, list, tuple, or dict, default=None\n",
      "            Strategy to evaluate the performance of the `estimator` across cross-validation\n",
      "            splits.\n",
      "        \n",
      "            If `scoring` represents a single score, one can use:\n",
      "        \n",
      "            - a single string (see :ref:`scoring_string_names`);\n",
      "            - a callable (see :ref:`scoring_callable`) that returns a single value.\n",
      "            - `None`, the `estimator`'s\n",
      "              :ref:`default evaluation criterion <scoring_api_overview>` is used.\n",
      "        \n",
      "            If `scoring` represents multiple scores, one can use:\n",
      "        \n",
      "            - a list or tuple of unique strings;\n",
      "            - a callable returning a dictionary where the keys are the metric\n",
      "              names and the values are the metric scores;\n",
      "            - a dictionary with metric names as keys and callables a values.\n",
      "        \n",
      "            See :ref:`multimetric_grid_search` for an example.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and computing\n",
      "            the score are parallelized over the cross-validation splits.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            The verbosity level.\n",
      "        \n",
      "        params : dict, default=None\n",
      "            Parameters to pass to the underlying estimator's ``fit``, the scorer,\n",
      "            and the CV splitter.\n",
      "        \n",
      "            .. versionadded:: 1.4\n",
      "        \n",
      "        pre_dispatch : int or str, default='2*n_jobs'\n",
      "            Controls the number of jobs that get dispatched during parallel\n",
      "            execution. Reducing this number can be useful to avoid an\n",
      "            explosion of memory consumption when more jobs get dispatched\n",
      "            than CPUs can process. This parameter can be:\n",
      "        \n",
      "            - An int, giving the exact number of total jobs that are spawned\n",
      "            - A str, giving an expression as a function of n_jobs, as in '2*n_jobs'\n",
      "        \n",
      "        return_train_score : bool, default=False\n",
      "            Whether to include train scores.\n",
      "            Computing training scores is used to get insights on how different\n",
      "            parameter settings impact the overfitting/underfitting trade-off.\n",
      "            However computing the scores on the training set can be computationally\n",
      "            expensive and is not strictly required to select the parameters that\n",
      "            yield the best generalization performance.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "            .. versionchanged:: 0.21\n",
      "                Default value was changed from ``True`` to ``False``\n",
      "        \n",
      "        return_estimator : bool, default=False\n",
      "            Whether to return the estimators fitted on each split.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        return_indices : bool, default=False\n",
      "            Whether to return the train-test indices selected for each split.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "        \n",
      "        error_score : 'raise' or numeric, default=np.nan\n",
      "            Value to assign to the score if an error occurs in estimator fitting.\n",
      "            If set to 'raise', the error is raised.\n",
      "            If a numeric value is given, FitFailedWarning is raised.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scores : dict of float arrays of shape (n_splits,)\n",
      "            Array of scores of the estimator for each run of the cross validation.\n",
      "        \n",
      "            A dict of arrays containing the score/time arrays for each scorer is\n",
      "            returned. The possible keys for this ``dict`` are:\n",
      "        \n",
      "            ``test_score``\n",
      "                The score array for test scores on each cv split.\n",
      "                Suffix ``_score`` in ``test_score`` changes to a specific\n",
      "                metric like ``test_r2`` or ``test_auc`` if there are\n",
      "                multiple scoring metrics in the scoring parameter.\n",
      "            ``train_score``\n",
      "                The score array for train scores on each cv split.\n",
      "                Suffix ``_score`` in ``train_score`` changes to a specific\n",
      "                metric like ``train_r2`` or ``train_auc`` if there are\n",
      "                multiple scoring metrics in the scoring parameter.\n",
      "                This is available only if ``return_train_score`` parameter\n",
      "                is ``True``.\n",
      "            ``fit_time``\n",
      "                The time for fitting the estimator on the train\n",
      "                set for each cv split.\n",
      "            ``score_time``\n",
      "                The time for scoring the estimator on the test set for each\n",
      "                cv split. (Note: time for scoring on the train set is not\n",
      "                included even if ``return_train_score`` is set to ``True``).\n",
      "            ``estimator``\n",
      "                The estimator objects for each cv split.\n",
      "                This is available only if ``return_estimator`` parameter\n",
      "                is set to ``True``.\n",
      "            ``indices``\n",
      "                The train/test positional indices for each cv split. A dictionary\n",
      "                is returned where the keys are either `\"train\"` or `\"test\"`\n",
      "                and the associated values are a list of integer-dtyped NumPy\n",
      "                arrays with the indices. Available only if `return_indices=True`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        cross_val_score : Run cross-validation for single metric evaluation.\n",
      "        \n",
      "        cross_val_predict : Get predictions from each split of cross-validation for\n",
      "            diagnostic purposes.\n",
      "        \n",
      "        sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n",
      "            loss function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import datasets, linear_model\n",
      "        >>> from sklearn.model_selection import cross_validate\n",
      "        >>> from sklearn.metrics import make_scorer\n",
      "        >>> from sklearn.metrics import confusion_matrix\n",
      "        >>> from sklearn.svm import LinearSVC\n",
      "        >>> diabetes = datasets.load_diabetes()\n",
      "        >>> X = diabetes.data[:150]\n",
      "        >>> y = diabetes.target[:150]\n",
      "        >>> lasso = linear_model.Lasso()\n",
      "        \n",
      "        Single metric evaluation using ``cross_validate``\n",
      "        \n",
      "        >>> cv_results = cross_validate(lasso, X, y, cv=3)\n",
      "        >>> sorted(cv_results.keys())\n",
      "        ['fit_time', 'score_time', 'test_score']\n",
      "        >>> cv_results['test_score']\n",
      "        array([0.3315057 , 0.08022103, 0.03531816])\n",
      "        \n",
      "        Multiple metric evaluation using ``cross_validate``\n",
      "        (please refer the ``scoring`` parameter doc for more information)\n",
      "        \n",
      "        >>> scores = cross_validate(lasso, X, y, cv=3,\n",
      "        ...                         scoring=('r2', 'neg_mean_squared_error'),\n",
      "        ...                         return_train_score=True)\n",
      "        >>> print(scores['test_neg_mean_squared_error'])\n",
      "        [-3635.5 -3573.3 -6114.7]\n",
      "        >>> print(scores['train_r2'])\n",
      "        [0.28009951 0.3908844  0.22784907]\n",
      "    \n",
      "    learning_curve(estimator, X, y, *, groups=None, train_sizes=array([0.1  , 0.325, 0.55 , 0.775, 1.   ]), cv=None, scoring=None, exploit_incremental_learning=False, n_jobs=None, pre_dispatch='all', verbose=0, shuffle=False, random_state=None, error_score=nan, return_times=False, fit_params=None, params=None)\n",
      "        Learning curve.\n",
      "        \n",
      "        Determines cross-validated training and test scores for different training\n",
      "        set sizes.\n",
      "        \n",
      "        A cross-validation generator splits the whole dataset k times in training\n",
      "        and test data. Subsets of the training set with varying sizes will be used\n",
      "        to train the estimator and a score for each training subset size and the\n",
      "        test set will be computed. Afterwards, the scores will be averaged over\n",
      "        all k runs for each training subset size.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <learning_curve>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : object type that implements the \"fit\" method\n",
      "            An object of that type which is cloned for each validation. It must\n",
      "            also implement \"predict\" unless `scoring` is a callable that doesn't\n",
      "            rely on \"predict\" to compute a score.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Training vector, where `n_samples` is the number of samples and\n",
      "            `n_features` is the number of features.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n",
      "            Target relative to X for classification or regression;\n",
      "            None for unsupervised learning.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Group labels for the samples used while splitting the dataset into\n",
      "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "            instance (e.g., :class:`GroupKFold`).\n",
      "        \n",
      "            .. versionchanged:: 1.6\n",
      "                ``groups`` can only be passed if metadata routing is not enabled\n",
      "                via ``sklearn.set_config(enable_metadata_routing=True)``. When routing\n",
      "                is enabled, pass ``groups`` alongside other metadata via the ``params``\n",
      "                argument instead. E.g.:\n",
      "                ``learning_curve(..., params={'groups': groups})``.\n",
      "        \n",
      "        train_sizes : array-like of shape (n_ticks,),             default=np.linspace(0.1, 1.0, 5)\n",
      "            Relative or absolute numbers of training examples that will be used to\n",
      "            generate the learning curve. If the dtype is float, it is regarded as a\n",
      "            fraction of the maximum size of the training set (that is determined\n",
      "            by the selected validation method), i.e. it has to be within (0, 1].\n",
      "            Otherwise it is interpreted as absolute sizes of the training sets.\n",
      "            Note that for classification the number of samples usually has to\n",
      "            be big enough to contain at least one sample from each class.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            Scoring method to use to evaluate the training and test sets.\n",
      "        \n",
      "            - str: see :ref:`scoring_string_names` for options.\n",
      "            - callable: a scorer callable object (e.g., function) with signature\n",
      "              ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n",
      "            - `None`: the `estimator`'s\n",
      "              :ref:`default evaluation criterion <scoring_api_overview>` is used.\n",
      "        \n",
      "        exploit_incremental_learning : bool, default=False\n",
      "            If the estimator supports incremental learning, this will be\n",
      "            used to speed up fitting for different training set sizes.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and computing\n",
      "            the score are parallelized over the different training and test sets.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        pre_dispatch : int or str, default='all'\n",
      "            Number of predispatched jobs for parallel execution (default is\n",
      "            all). The option can reduce the allocated memory. The str can\n",
      "            be an expression like '2*n_jobs'.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Controls the verbosity: the higher, the more messages.\n",
      "        \n",
      "        shuffle : bool, default=False\n",
      "            Whether to shuffle training data before taking prefixes of it\n",
      "            based on``train_sizes``.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Used when ``shuffle`` is True. Pass an int for reproducible\n",
      "            output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        error_score : 'raise' or numeric, default=np.nan\n",
      "            Value to assign to the score if an error occurs in estimator fitting.\n",
      "            If set to 'raise', the error is raised.\n",
      "            If a numeric value is given, FitFailedWarning is raised.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        return_times : bool, default=False\n",
      "            Whether to return the fit and score times.\n",
      "        \n",
      "        fit_params : dict, default=None\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "            .. deprecated:: 1.6\n",
      "                This parameter is deprecated and will be removed in version 1.8. Use\n",
      "                ``params`` instead.\n",
      "        \n",
      "        params : dict, default=None\n",
      "            Parameters to pass to the `fit` method of the estimator and to the scorer.\n",
      "        \n",
      "            - If `enable_metadata_routing=False` (default): Parameters directly passed to\n",
      "              the `fit` method of the estimator.\n",
      "        \n",
      "            - If `enable_metadata_routing=True`: Parameters safely routed to the `fit`\n",
      "              method of the estimator. See :ref:`Metadata Routing User Guide\n",
      "              <metadata_routing>` for more details.\n",
      "        \n",
      "            .. versionadded:: 1.6\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        train_sizes_abs : array of shape (n_unique_ticks,)\n",
      "            Numbers of training examples that has been used to generate the\n",
      "            learning curve. Note that the number of ticks might be less\n",
      "            than n_ticks because duplicate entries will be removed.\n",
      "        \n",
      "        train_scores : array of shape (n_ticks, n_cv_folds)\n",
      "            Scores on training sets.\n",
      "        \n",
      "        test_scores : array of shape (n_ticks, n_cv_folds)\n",
      "            Scores on test set.\n",
      "        \n",
      "        fit_times : array of shape (n_ticks, n_cv_folds)\n",
      "            Times spent for fitting in seconds. Only present if ``return_times``\n",
      "            is True.\n",
      "        \n",
      "        score_times : array of shape (n_ticks, n_cv_folds)\n",
      "            Times spent for scoring in seconds. Only present if ``return_times``\n",
      "            is True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        LearningCurveDisplay.from_estimator : Plot a learning curve using an\n",
      "            estimator and data.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets import make_classification\n",
      "        >>> from sklearn.tree import DecisionTreeClassifier\n",
      "        >>> from sklearn.model_selection import learning_curve\n",
      "        >>> X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
      "        >>> tree = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
      "        >>> train_size_abs, train_scores, test_scores = learning_curve(\n",
      "        ...     tree, X, y, train_sizes=[0.3, 0.6, 0.9]\n",
      "        ... )\n",
      "        >>> for train_size, cv_train_scores, cv_test_scores in zip(\n",
      "        ...     train_size_abs, train_scores, test_scores\n",
      "        ... ):\n",
      "        ...     print(f\"{train_size} samples were used to train the model\")\n",
      "        ...     print(f\"The average train accuracy is {cv_train_scores.mean():.2f}\")\n",
      "        ...     print(f\"The average test accuracy is {cv_test_scores.mean():.2f}\")\n",
      "        24 samples were used to train the model\n",
      "        The average train accuracy is 1.00\n",
      "        The average test accuracy is 0.85\n",
      "        48 samples were used to train the model\n",
      "        The average train accuracy is 1.00\n",
      "        The average test accuracy is 0.90\n",
      "        72 samples were used to train the model\n",
      "        The average train accuracy is 1.00\n",
      "        The average test accuracy is 0.93\n",
      "    \n",
      "    permutation_test_score(estimator, X, y, *, groups=None, cv=None, n_permutations=100, n_jobs=None, random_state=0, verbose=0, scoring=None, fit_params=None, params=None)\n",
      "        Evaluate the significance of a cross-validated score with permutations.\n",
      "        \n",
      "        Permutes targets to generate 'randomized data' and compute the empirical\n",
      "        p-value against the null hypothesis that features and targets are\n",
      "        independent.\n",
      "        \n",
      "        The p-value represents the fraction of randomized data sets where the\n",
      "        estimator performed as well or better than on the original data. A small\n",
      "        p-value suggests that there is a real dependency between features and\n",
      "        targets which has been used by the estimator to give good predictions.\n",
      "        A large p-value may be due to lack of real dependency between features\n",
      "        and targets or the estimator was not able to use the dependency to\n",
      "        give good predictions.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <permutation_test_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        X : array-like of shape at least 2D\n",
      "            The data to fit.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n",
      "            The target variable to try to predict in the case of\n",
      "            supervised learning.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Labels to constrain permutation within groups, i.e. ``y`` values\n",
      "            are permuted among samples with the same group identifier.\n",
      "            When not specified, ``y`` values are permuted among all samples.\n",
      "        \n",
      "            When a grouped cross-validator is used, the group labels are\n",
      "            also passed on to the ``split`` method of the cross-validator. The\n",
      "            cross-validator uses them for grouping the samples  while splitting\n",
      "            the dataset into train/test set.\n",
      "        \n",
      "            .. versionchanged:: 1.6\n",
      "                ``groups`` can only be passed if metadata routing is not enabled\n",
      "                via ``sklearn.set_config(enable_metadata_routing=True)``. When routing\n",
      "                is enabled, pass ``groups`` alongside other metadata via the ``params``\n",
      "                argument instead. E.g.:\n",
      "                ``permutation_test_score(..., params={'groups': groups})``.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - `None`, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For `int`/`None` inputs, if the estimator is a classifier and `y` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                `cv` default value if `None` changed from 3-fold to 5-fold.\n",
      "        \n",
      "        n_permutations : int, default=100\n",
      "            Number of times to permute ``y``.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and computing\n",
      "            the cross-validated score are parallelized over the permutations.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=0\n",
      "            Pass an int for reproducible output for permutation of\n",
      "            ``y`` values among samples. See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            The verbosity level.\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            Scoring method to use to evaluate the predictions on the validation set.\n",
      "        \n",
      "            - str: see :ref:`scoring_string_names` for options.\n",
      "            - callable: a scorer callable object (e.g., function) with signature\n",
      "              ``scorer(estimator, X, y)``, which should return only a single value.\n",
      "              See :ref:`scoring_callable` for details.\n",
      "            - `None`: the `estimator`'s\n",
      "              :ref:`default evaluation criterion <scoring_api_overview>` is used.\n",
      "        \n",
      "        fit_params : dict, default=None\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "            .. deprecated:: 1.6\n",
      "                This parameter is deprecated and will be removed in version 1.6. Use\n",
      "                ``params`` instead.\n",
      "        \n",
      "        params : dict, default=None\n",
      "            Parameters to pass to the `fit` method of the estimator, the scorer\n",
      "            and the cv splitter.\n",
      "        \n",
      "            - If `enable_metadata_routing=False` (default): Parameters directly passed to\n",
      "              the `fit` method of the estimator.\n",
      "        \n",
      "            - If `enable_metadata_routing=True`: Parameters safely routed to the `fit`\n",
      "              method of the estimator, `cv` object and `scorer`. See :ref:`Metadata Routing\n",
      "              User Guide <metadata_routing>` for more details.\n",
      "        \n",
      "            .. versionadded:: 1.6\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The true score without permuting targets.\n",
      "        \n",
      "        permutation_scores : array of shape (n_permutations,)\n",
      "            The scores obtained for each permutations.\n",
      "        \n",
      "        pvalue : float\n",
      "            The p-value, which approximates the probability that the score would\n",
      "            be obtained by chance. This is calculated as:\n",
      "        \n",
      "            `(C + 1) / (n_permutations + 1)`\n",
      "        \n",
      "            Where C is the number of permutations whose score >= the true score.\n",
      "        \n",
      "            The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function implements Test 1 in:\n",
      "        \n",
      "        Ojala and Garriga. `Permutation Tests for Studying Classifier Performance\n",
      "        <http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf>`_. The\n",
      "        Journal of Machine Learning Research (2010) vol. 11\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets import make_classification\n",
      "        >>> from sklearn.linear_model import LogisticRegression\n",
      "        >>> from sklearn.model_selection import permutation_test_score\n",
      "        >>> X, y = make_classification(random_state=0)\n",
      "        >>> estimator = LogisticRegression()\n",
      "        >>> score, permutation_scores, pvalue = permutation_test_score(\n",
      "        ...     estimator, X, y, random_state=0\n",
      "        ... )\n",
      "        >>> print(f\"Original Score: {score:.3f}\")\n",
      "        Original Score: 0.810\n",
      "        >>> print(\n",
      "        ...     f\"Permutation Scores: {permutation_scores.mean():.3f} +/- \"\n",
      "        ...     f\"{permutation_scores.std():.3f}\"\n",
      "        ... )\n",
      "        Permutation Scores: 0.505 +/- 0.057\n",
      "        >>> print(f\"P-value: {pvalue:.3f}\")\n",
      "        P-value: 0.010\n",
      "    \n",
      "    train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
      "        Split arrays or matrices into random train and test subsets.\n",
      "        \n",
      "        Quick utility that wraps input validation,\n",
      "        ``next(ShuffleSplit().split(X, y))``, and application to input data\n",
      "        into a single call for splitting (and optionally subsampling) data into a\n",
      "        one-liner.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        *arrays : sequence of indexables with same length / shape[0]\n",
      "            Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "            matrices or pandas dataframes.\n",
      "        \n",
      "        test_size : float or int, default=None\n",
      "            If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "            of the dataset to include in the test split. If int, represents the\n",
      "            absolute number of test samples. If None, the value is set to the\n",
      "            complement of the train size. If ``train_size`` is also None, it will\n",
      "            be set to 0.25.\n",
      "        \n",
      "        train_size : float or int, default=None\n",
      "            If float, should be between 0.0 and 1.0 and represent the\n",
      "            proportion of the dataset to include in the train split. If\n",
      "            int, represents the absolute number of train samples. If None,\n",
      "            the value is automatically set to the complement of the test size.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Controls the shuffling applied to the data before applying the split.\n",
      "            Pass an int for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        shuffle : bool, default=True\n",
      "            Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "            then stratify must be None.\n",
      "        \n",
      "        stratify : array-like, default=None\n",
      "            If not None, data is split in a stratified fashion, using this as\n",
      "            the class labels.\n",
      "            Read more in the :ref:`User Guide <stratification>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        splitting : list, length=2 * len(arrays)\n",
      "            List containing train-test split of inputs.\n",
      "        \n",
      "            .. versionadded:: 0.16\n",
      "                If the input is sparse, the output will be a\n",
      "                ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "                input type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.model_selection import train_test_split\n",
      "        >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "        >>> X\n",
      "        array([[0, 1],\n",
      "               [2, 3],\n",
      "               [4, 5],\n",
      "               [6, 7],\n",
      "               [8, 9]])\n",
      "        >>> list(y)\n",
      "        [0, 1, 2, 3, 4]\n",
      "        \n",
      "        >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "        ...     X, y, test_size=0.33, random_state=42)\n",
      "        ...\n",
      "        >>> X_train\n",
      "        array([[4, 5],\n",
      "               [0, 1],\n",
      "               [6, 7]])\n",
      "        >>> y_train\n",
      "        [2, 0, 3]\n",
      "        >>> X_test\n",
      "        array([[2, 3],\n",
      "               [8, 9]])\n",
      "        >>> y_test\n",
      "        [1, 4]\n",
      "        \n",
      "        >>> train_test_split(y, shuffle=False)\n",
      "        [[0, 1, 2], [3, 4]]\n",
      "        \n",
      "        >>> from sklearn import datasets\n",
      "        >>> iris = datasets.load_iris(as_frame=True)\n",
      "        >>> X, y = iris['data'], iris['target']\n",
      "        >>> X.head()\n",
      "            sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "        0                5.1               3.5                1.4               0.2\n",
      "        1                4.9               3.0                1.4               0.2\n",
      "        2                4.7               3.2                1.3               0.2\n",
      "        3                4.6               3.1                1.5               0.2\n",
      "        4                5.0               3.6                1.4               0.2\n",
      "        >>> y.head()\n",
      "        0    0\n",
      "        1    0\n",
      "        2    0\n",
      "        3    0\n",
      "        4    0\n",
      "        ...\n",
      "        \n",
      "        >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "        ... X, y, test_size=0.33, random_state=42)\n",
      "        ...\n",
      "        >>> X_train.head()\n",
      "            sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "        96                 5.7               2.9                4.2               1.3\n",
      "        105                7.6               3.0                6.6               2.1\n",
      "        66                 5.6               3.0                4.5               1.5\n",
      "        0                  5.1               3.5                1.4               0.2\n",
      "        122                7.7               2.8                6.7               2.0\n",
      "        >>> y_train.head()\n",
      "        96     1\n",
      "        105    2\n",
      "        66     1\n",
      "        0      0\n",
      "        122    2\n",
      "        ...\n",
      "        >>> X_test.head()\n",
      "            sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "        73                 6.1               2.8                4.7               1.2\n",
      "        18                 5.7               3.8                1.7               0.3\n",
      "        118                7.7               2.6                6.9               2.3\n",
      "        78                 6.0               2.9                4.5               1.5\n",
      "        76                 6.8               2.8                4.8               1.4\n",
      "        >>> y_test.head()\n",
      "        73     1\n",
      "        18     0\n",
      "        118    2\n",
      "        78     1\n",
      "        76     1\n",
      "        ...\n",
      "    \n",
      "    validation_curve(estimator, X, y, *, param_name, param_range, groups=None, cv=None, scoring=None, n_jobs=None, pre_dispatch='all', verbose=0, error_score=nan, fit_params=None, params=None)\n",
      "        Validation curve.\n",
      "        \n",
      "        Determine training and test scores for varying parameter values.\n",
      "        \n",
      "        Compute scores for an estimator with different values of a specified\n",
      "        parameter. This is similar to grid search with one parameter. However, this\n",
      "        will also compute training scores and is merely a utility for plotting the\n",
      "        results.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <validation_curve>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : object type that implements the \"fit\" method\n",
      "            An object of that type which is cloned for each validation. It must\n",
      "            also implement \"predict\" unless `scoring` is a callable that doesn't\n",
      "            rely on \"predict\" to compute a score.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Training vector, where `n_samples` is the number of samples and\n",
      "            `n_features` is the number of features.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n",
      "            Target relative to X for classification or regression;\n",
      "            None for unsupervised learning.\n",
      "        \n",
      "        param_name : str\n",
      "            Name of the parameter that will be varied.\n",
      "        \n",
      "        param_range : array-like of shape (n_values,)\n",
      "            The values of the parameter that will be evaluated.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Group labels for the samples used while splitting the dataset into\n",
      "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "            instance (e.g., :class:`GroupKFold`).\n",
      "        \n",
      "            .. versionchanged:: 1.6\n",
      "                ``groups`` can only be passed if metadata routing is not enabled\n",
      "                via ``sklearn.set_config(enable_metadata_routing=True)``. When routing\n",
      "                is enabled, pass ``groups`` alongside other metadata via the ``params``\n",
      "                argument instead. E.g.:\n",
      "                ``validation_curve(..., params={'groups': groups})``.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            Scoring method to use to evaluate the training and test sets.\n",
      "        \n",
      "            - str: see :ref:`scoring_string_names` for options.\n",
      "            - callable: a scorer callable object (e.g., function) with signature\n",
      "              ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n",
      "            - `None`: the `estimator`'s\n",
      "              :ref:`default evaluation criterion <scoring_api_overview>` is used.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and computing\n",
      "            the score are parallelized over the combinations of each parameter\n",
      "            value and each cross-validation split.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        pre_dispatch : int or str, default='all'\n",
      "            Number of predispatched jobs for parallel execution (default is\n",
      "            all). The option can reduce the allocated memory. The str can\n",
      "            be an expression like '2*n_jobs'.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Controls the verbosity: the higher, the more messages.\n",
      "        \n",
      "        error_score : 'raise' or numeric, default=np.nan\n",
      "            Value to assign to the score if an error occurs in estimator fitting.\n",
      "            If set to 'raise', the error is raised.\n",
      "            If a numeric value is given, FitFailedWarning is raised.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        fit_params : dict, default=None\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "            .. deprecated:: 1.6\n",
      "                This parameter is deprecated and will be removed in version 1.8. Use\n",
      "                ``params`` instead.\n",
      "        \n",
      "        params : dict, default=None\n",
      "            Parameters to pass to the estimator, scorer and cross-validation object.\n",
      "        \n",
      "            - If `enable_metadata_routing=False` (default): Parameters directly passed to\n",
      "              the `fit` method of the estimator.\n",
      "        \n",
      "            - If `enable_metadata_routing=True`: Parameters safely routed to the `fit`\n",
      "              method of the estimator, to the scorer and to the cross-validation object.\n",
      "              See :ref:`Metadata Routing User Guide <metadata_routing>` for more details.\n",
      "        \n",
      "            .. versionadded:: 1.6\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        train_scores : array of shape (n_ticks, n_cv_folds)\n",
      "            Scores on training sets.\n",
      "        \n",
      "        test_scores : array of shape (n_ticks, n_cv_folds)\n",
      "            Scores on test set.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ValidationCurveDisplay.from_estimator : Plot the validation curve\n",
      "            given an estimator, the data, and the parameter to vary.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        See :ref:`sphx_glr_auto_examples_model_selection_plot_train_error_vs_test_error.py`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.datasets import make_classification\n",
      "        >>> from sklearn.model_selection import validation_curve\n",
      "        >>> from sklearn.linear_model import LogisticRegression\n",
      "        >>> X, y = make_classification(n_samples=1_000, random_state=0)\n",
      "        >>> logistic_regression = LogisticRegression()\n",
      "        >>> param_name, param_range = \"C\", np.logspace(-8, 3, 10)\n",
      "        >>> train_scores, test_scores = validation_curve(\n",
      "        ...     logistic_regression, X, y, param_name=param_name, param_range=param_range\n",
      "        ... )\n",
      "        >>> print(f\"The average train accuracy is {train_scores.mean():.2f}\")\n",
      "        The average train accuracy is 0.81\n",
      "        >>> print(f\"The average test accuracy is {test_scores.mean():.2f}\")\n",
      "        The average test accuracy is 0.81\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BaseCrossValidator', 'BaseShuffleSplit', 'FixedThresholdCl...\n",
      "\n",
      "FILE\n",
      "    c:\\project\\bigdata_cert\\.venv\\lib\\site-packages\\sklearn\\model_selection\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "help(model_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "71f3b00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.metrics in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.metrics - Score functions, performance metrics, pairwise metrics and distance computations.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _base\n",
      "    _classification\n",
      "    _dist_metrics\n",
      "    _pairwise_distances_reduction (package)\n",
      "    _pairwise_fast\n",
      "    _plot (package)\n",
      "    _ranking\n",
      "    _regression\n",
      "    _scorer\n",
      "    cluster (package)\n",
      "    pairwise\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        sklearn.metrics._dist_metrics.DistanceMetric\n",
      "        sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay\n",
      "        sklearn.metrics._plot.regression.PredictionErrorDisplay\n",
      "    sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin(builtins.object)\n",
      "        sklearn.metrics._plot.det_curve.DetCurveDisplay\n",
      "        sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay\n",
      "        sklearn.metrics._plot.roc_curve.RocCurveDisplay\n",
      "    \n",
      "    class ConfusionMatrixDisplay(builtins.object)\n",
      "     |  ConfusionMatrixDisplay(confusion_matrix, *, display_labels=None)\n",
      "     |  \n",
      "     |  Confusion Matrix visualization.\n",
      "     |  \n",
      "     |  It is recommended to use\n",
      "     |  :func:`~sklearn.metrics.ConfusionMatrixDisplay.from_estimator` or\n",
      "     |  :func:`~sklearn.metrics.ConfusionMatrixDisplay.from_predictions` to\n",
      "     |  create a :class:`ConfusionMatrixDisplay`. All parameters are stored as\n",
      "     |  attributes.\n",
      "     |  \n",
      "     |  For general information regarding `scikit-learn` visualization tools, see\n",
      "     |  the :ref:`Visualization Guide <visualizations>`.\n",
      "     |  For guidance on interpreting these plots, refer to the\n",
      "     |  :ref:`Model Evaluation Guide <confusion_matrix>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  confusion_matrix : ndarray of shape (n_classes, n_classes)\n",
      "     |      Confusion matrix.\n",
      "     |  \n",
      "     |  display_labels : ndarray of shape (n_classes,), default=None\n",
      "     |      Display labels for plot. If None, display labels are set from 0 to\n",
      "     |      `n_classes - 1`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  im_ : matplotlib AxesImage\n",
      "     |      Image representing the confusion matrix.\n",
      "     |  \n",
      "     |  text_ : ndarray of shape (n_classes, n_classes), dtype=matplotlib Text,             or None\n",
      "     |      Array of matplotlib axes. `None` if `include_values` is false.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with confusion matrix.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the confusion matrix.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  confusion_matrix : Compute Confusion Matrix to evaluate the accuracy of a\n",
      "     |      classification.\n",
      "     |  ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "     |      given an estimator, the data, and the label.\n",
      "     |  ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "     |      given the true and predicted labels.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
      "     |  ...                                                     random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0)\n",
      "     |  >>> clf.fit(X_train, y_train)\n",
      "     |  SVC(random_state=0)\n",
      "     |  >>> predictions = clf.predict(X_test)\n",
      "     |  >>> cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
      "     |  >>> disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
      "     |  ...                               display_labels=clf.classes_)\n",
      "     |  >>> disp.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, confusion_matrix, *, display_labels=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, *, include_values=True, cmap='viridis', xticks_rotation='horizontal', values_format=None, ax=None, colorbar=True, im_kw=None, text_kw=None)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      include_values : bool, default=True\n",
      "     |          Includes values in confusion matrix.\n",
      "     |      \n",
      "     |      cmap : str or matplotlib Colormap, default='viridis'\n",
      "     |          Colormap recognized by matplotlib.\n",
      "     |      \n",
      "     |      xticks_rotation : {'vertical', 'horizontal'} or float,                          default='horizontal'\n",
      "     |          Rotation of xtick labels.\n",
      "     |      \n",
      "     |      values_format : str, default=None\n",
      "     |          Format specification for values in confusion matrix. If `None`,\n",
      "     |          the format specification is 'd' or '.2g' whichever is shorter.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      colorbar : bool, default=True\n",
      "     |          Whether or not to add a colorbar to the plot.\n",
      "     |      \n",
      "     |      im_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.imshow` call.\n",
      "     |      \n",
      "     |      text_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.text` call.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.2\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "     |          Returns a :class:`~sklearn.metrics.ConfusionMatrixDisplay` instance\n",
      "     |          that contains all the information to plot the confusion matrix.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, labels=None, sample_weight=None, normalize=None, display_labels=None, include_values=True, xticks_rotation='horizontal', values_format=None, cmap='viridis', ax=None, colorbar=True, im_kw=None, text_kw=None)\n",
      "     |      Plot Confusion Matrix given an estimator and some data.\n",
      "     |      \n",
      "     |      For general information regarding `scikit-learn` visualization tools, see\n",
      "     |      the :ref:`Visualization Guide <visualizations>`.\n",
      "     |      For guidance on interpreting these plots, refer to the\n",
      "     |      :ref:`Model Evaluation Guide <confusion_matrix>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      labels : array-like of shape (n_classes,), default=None\n",
      "     |          List of labels to index the confusion matrix. This may be used to\n",
      "     |          reorder or select a subset of labels. If `None` is given, those\n",
      "     |          that appear at least once in `y_true` or `y_pred` are used in\n",
      "     |          sorted order.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      normalize : {'true', 'pred', 'all'}, default=None\n",
      "     |          Either to normalize the counts display in the matrix:\n",
      "     |      \n",
      "     |          - if `'true'`, the confusion matrix is normalized over the true\n",
      "     |            conditions (e.g. rows);\n",
      "     |          - if `'pred'`, the confusion matrix is normalized over the\n",
      "     |            predicted conditions (e.g. columns);\n",
      "     |          - if `'all'`, the confusion matrix is normalized by the total\n",
      "     |            number of samples;\n",
      "     |          - if `None` (default), the confusion matrix will not be normalized.\n",
      "     |      \n",
      "     |      display_labels : array-like of shape (n_classes,), default=None\n",
      "     |          Target names used for plotting. By default, `labels` will be used\n",
      "     |          if it is defined, otherwise the unique labels of `y_true` and\n",
      "     |          `y_pred` will be used.\n",
      "     |      \n",
      "     |      include_values : bool, default=True\n",
      "     |          Includes values in confusion matrix.\n",
      "     |      \n",
      "     |      xticks_rotation : {'vertical', 'horizontal'} or float,                 default='horizontal'\n",
      "     |          Rotation of xtick labels.\n",
      "     |      \n",
      "     |      values_format : str, default=None\n",
      "     |          Format specification for values in confusion matrix. If `None`, the\n",
      "     |          format specification is 'd' or '.2g' whichever is shorter.\n",
      "     |      \n",
      "     |      cmap : str or matplotlib Colormap, default='viridis'\n",
      "     |          Colormap recognized by matplotlib.\n",
      "     |      \n",
      "     |      ax : matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      colorbar : bool, default=True\n",
      "     |          Whether or not to add a colorbar to the plot.\n",
      "     |      \n",
      "     |      im_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.imshow` call.\n",
      "     |      \n",
      "     |      text_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.text` call.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.2\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "     |          given the true and predicted labels.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import ConfusionMatrixDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0)\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      SVC(random_state=0)\n",
      "     |      >>> ConfusionMatrixDisplay.from_estimator(\n",
      "     |      ...     clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |      \n",
      "     |      For a detailed example of using a confusion matrix to evaluate a\n",
      "     |      Support Vector Classifier, please see\n",
      "     |      :ref:`sphx_glr_auto_examples_model_selection_plot_confusion_matrix.py`\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None, display_labels=None, include_values=True, xticks_rotation='horizontal', values_format=None, cmap='viridis', ax=None, colorbar=True, im_kw=None, text_kw=None)\n",
      "     |      Plot Confusion Matrix given true and predicted labels.\n",
      "     |      \n",
      "     |      For general information regarding `scikit-learn` visualization tools, see\n",
      "     |      the :ref:`Visualization Guide <visualizations>`.\n",
      "     |      For guidance on interpreting these plots, refer to the\n",
      "     |      :ref:`Model Evaluation Guide <confusion_matrix>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True labels.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          The predicted labels given by the method `predict` of an\n",
      "     |          classifier.\n",
      "     |      \n",
      "     |      labels : array-like of shape (n_classes,), default=None\n",
      "     |          List of labels to index the confusion matrix. This may be used to\n",
      "     |          reorder or select a subset of labels. If `None` is given, those\n",
      "     |          that appear at least once in `y_true` or `y_pred` are used in\n",
      "     |          sorted order.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      normalize : {'true', 'pred', 'all'}, default=None\n",
      "     |          Either to normalize the counts display in the matrix:\n",
      "     |      \n",
      "     |          - if `'true'`, the confusion matrix is normalized over the true\n",
      "     |            conditions (e.g. rows);\n",
      "     |          - if `'pred'`, the confusion matrix is normalized over the\n",
      "     |            predicted conditions (e.g. columns);\n",
      "     |          - if `'all'`, the confusion matrix is normalized by the total\n",
      "     |            number of samples;\n",
      "     |          - if `None` (default), the confusion matrix will not be normalized.\n",
      "     |      \n",
      "     |      display_labels : array-like of shape (n_classes,), default=None\n",
      "     |          Target names used for plotting. By default, `labels` will be used\n",
      "     |          if it is defined, otherwise the unique labels of `y_true` and\n",
      "     |          `y_pred` will be used.\n",
      "     |      \n",
      "     |      include_values : bool, default=True\n",
      "     |          Includes values in confusion matrix.\n",
      "     |      \n",
      "     |      xticks_rotation : {'vertical', 'horizontal'} or float,                 default='horizontal'\n",
      "     |          Rotation of xtick labels.\n",
      "     |      \n",
      "     |      values_format : str, default=None\n",
      "     |          Format specification for values in confusion matrix. If `None`, the\n",
      "     |          format specification is 'd' or '.2g' whichever is shorter.\n",
      "     |      \n",
      "     |      cmap : str or matplotlib Colormap, default='viridis'\n",
      "     |          Colormap recognized by matplotlib.\n",
      "     |      \n",
      "     |      ax : matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      colorbar : bool, default=True\n",
      "     |          Whether or not to add a colorbar to the plot.\n",
      "     |      \n",
      "     |      im_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.imshow` call.\n",
      "     |      \n",
      "     |      text_kw : dict, default=None\n",
      "     |          Dict with keywords passed to `matplotlib.pyplot.text` call.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.2\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "     |          given an estimator, the data, and the label.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import ConfusionMatrixDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0)\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      SVC(random_state=0)\n",
      "     |      >>> y_pred = clf.predict(X_test)\n",
      "     |      >>> ConfusionMatrixDisplay.from_predictions(\n",
      "     |      ...    y_test, y_pred)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class DetCurveDisplay(sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin)\n",
      "     |  DetCurveDisplay(*, fpr, fnr, estimator_name=None, pos_label=None)\n",
      "     |  \n",
      "     |  Detection Error Tradeoff (DET) curve visualization.\n",
      "     |  \n",
      "     |  It is recommended to use :func:`~sklearn.metrics.DetCurveDisplay.from_estimator`\n",
      "     |  or :func:`~sklearn.metrics.DetCurveDisplay.from_predictions` to create a\n",
      "     |  visualizer. All parameters are stored as attributes.\n",
      "     |  \n",
      "     |  For general information regarding `scikit-learn` visualization tools, see\n",
      "     |  the :ref:`Visualization Guide <visualizations>`.\n",
      "     |  For guidance on interpreting these plots, refer to the\n",
      "     |  :ref:`Model Evaluation Guide <det_curve>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fpr : ndarray\n",
      "     |      False positive rate.\n",
      "     |  \n",
      "     |  fnr : ndarray\n",
      "     |      False negative rate.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : int, float, bool or str, default=None\n",
      "     |      The label of the positive class.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      DET Curve.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with DET Curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  det_curve : Compute error rates for different probability thresholds.\n",
      "     |  DetCurveDisplay.from_estimator : Plot DET curve given an estimator and\n",
      "     |      some data.\n",
      "     |  DetCurveDisplay.from_predictions : Plot DET curve given the true and\n",
      "     |      predicted labels.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import det_curve, DetCurveDisplay\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(n_samples=1000, random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...     X, y, test_size=0.4, random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |  >>> y_pred = clf.decision_function(X_test)\n",
      "     |  >>> fpr, fnr, _ = det_curve(y_test, y_pred)\n",
      "     |  >>> display = DetCurveDisplay(\n",
      "     |  ...     fpr=fpr, fnr=fnr, estimator_name=\"SVC\"\n",
      "     |  ... )\n",
      "     |  >>> display.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DetCurveDisplay\n",
      "     |      sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fpr, fnr, estimator_name=None, pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, **kwargs)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of DET curve for labeling. If `None`, use `estimator_name` if\n",
      "     |          it is not `None`, otherwise no labeling is shown.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.DetCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, sample_weight=None, drop_intermediate=True, response_method='auto', pos_label=None, name=None, ax=None, **kwargs)\n",
      "     |      Plot DET curve given an estimator and data.\n",
      "     |      \n",
      "     |      For general information regarding `scikit-learn` visualization tools, see\n",
      "     |      the :ref:`Visualization Guide <visualizations>`.\n",
      "     |      For guidance on interpreting these plots, refer to the\n",
      "     |      :ref:`Model Evaluation Guide <det_curve>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      drop_intermediate : bool, default=True\n",
      "     |          Whether to drop thresholds where true positives (tp) do not change\n",
      "     |          from the previous or subsequent threshold. All points with the same\n",
      "     |          tp value have the same `fnr` and thus same y coordinate.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |      response_method : {'predict_proba', 'decision_function', 'auto'}                 default='auto'\n",
      "     |          Specifies whether to use :term:`predict_proba` or\n",
      "     |          :term:`decision_function` as the predicted target response. If set\n",
      "     |          to 'auto', :term:`predict_proba` is tried first and if it does not\n",
      "     |          exist :term:`decision_function` is tried next.\n",
      "     |      \n",
      "     |      pos_label : int, float, bool or str, default=None\n",
      "     |          The label of the positive class. When `pos_label=None`, if `y_true`\n",
      "     |          is in {-1, 1} or {0, 1}, `pos_label` is set to 1, otherwise an\n",
      "     |          error will be raised.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of DET curve for labeling. If `None`, use the name of the\n",
      "     |          estimator.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.DetCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      det_curve : Compute error rates for different probability thresholds.\n",
      "     |      DetCurveDisplay.from_predictions : Plot DET curve given the true and\n",
      "     |          predicted labels.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import DetCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(n_samples=1000, random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, test_size=0.4, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> DetCurveDisplay.from_estimator(\n",
      "     |      ...    clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, sample_weight=None, drop_intermediate=True, pos_label=None, name=None, ax=None, **kwargs)\n",
      "     |      Plot the DET curve given the true and predicted labels.\n",
      "     |      \n",
      "     |      For general information regarding `scikit-learn` visualization tools, see\n",
      "     |      the :ref:`Visualization Guide <visualizations>`.\n",
      "     |      For guidance on interpreting these plots, refer to the\n",
      "     |      :ref:`Model Evaluation Guide <det_curve>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True labels.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          Target scores, can either be probability estimates of the positive\n",
      "     |          class, confidence values, or non-thresholded measure of decisions\n",
      "     |          (as returned by `decision_function` on some classifiers).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      drop_intermediate : bool, default=True\n",
      "     |          Whether to drop thresholds where true positives (tp) do not change\n",
      "     |          from the previous or subsequent threshold. All points with the same\n",
      "     |          tp value have the same `fnr` and thus same y coordinate.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |      pos_label : int, float, bool or str, default=None\n",
      "     |          The label of the positive class. When `pos_label=None`, if `y_true`\n",
      "     |          is in {-1, 1} or {0, 1}, `pos_label` is set to 1, otherwise an\n",
      "     |          error will be raised.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of DET curve for labeling. If `None`, name will be set to\n",
      "     |          `\"Classifier\"`.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.DetCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      det_curve : Compute error rates for different probability thresholds.\n",
      "     |      DetCurveDisplay.from_estimator : Plot DET curve given an estimator and\n",
      "     |          some data.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import DetCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(n_samples=1000, random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, test_size=0.4, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> y_pred = clf.decision_function(X_test)\n",
      "     |      >>> DetCurveDisplay.from_predictions(\n",
      "     |      ...    y_test, y_pred)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class DistanceMetric(builtins.object)\n",
      "     |  Uniform interface for fast distance metric functions.\n",
      "     |  \n",
      "     |  The `DistanceMetric` class provides a convenient way to compute pairwise distances\n",
      "     |  between samples. It supports various distance metrics, such as Euclidean distance,\n",
      "     |  Manhattan distance, and more.\n",
      "     |  \n",
      "     |  The `pairwise` method can be used to compute pairwise distances between samples in\n",
      "     |  the input arrays. It returns a distance matrix representing the distances between\n",
      "     |  all pairs of samples.\n",
      "     |  \n",
      "     |  The :meth:`get_metric` method allows you to retrieve a specific metric using its\n",
      "     |  string identifier.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.metrics import DistanceMetric\n",
      "     |  >>> dist = DistanceMetric.get_metric('euclidean')\n",
      "     |  >>> X = [[1, 2], [3, 4], [5, 6]]\n",
      "     |  >>> Y = [[7, 8], [9, 10]]\n",
      "     |  >>> dist.pairwise(X,Y)\n",
      "     |  array([[7.81..., 10.63...]\n",
      "     |         [5.65...,  8.48...]\n",
      "     |         [1.41...,  4.24...]])\n",
      "     |  \n",
      "     |  .. rubric:: Available Metrics\n",
      "     |  \n",
      "     |  The following lists the string metric identifiers and the associated\n",
      "     |  distance metric classes:\n",
      "     |  \n",
      "     |  **Metrics intended for real-valued vector spaces:**\n",
      "     |  \n",
      "     |  ==============  ====================  ========  ===============================\n",
      "     |  identifier      class name            args      distance function\n",
      "     |  --------------  --------------------  --------  -------------------------------\n",
      "     |  \"euclidean\"     EuclideanDistance     -         ``sqrt(sum((x - y)^2))``\n",
      "     |  \"manhattan\"     ManhattanDistance     -         ``sum(|x - y|)``\n",
      "     |  \"chebyshev\"     ChebyshevDistance     -         ``max(|x - y|)``\n",
      "     |  \"minkowski\"     MinkowskiDistance     p, w      ``sum(w * |x - y|^p)^(1/p)``\n",
      "     |  \"seuclidean\"    SEuclideanDistance    V         ``sqrt(sum((x - y)^2 / V))``\n",
      "     |  \"mahalanobis\"   MahalanobisDistance   V or VI   ``sqrt((x - y)' V^-1 (x - y))``\n",
      "     |  ==============  ====================  ========  ===============================\n",
      "     |  \n",
      "     |  **Metrics intended for two-dimensional vector spaces:**  Note that the haversine\n",
      "     |  distance metric requires data in the form of [latitude, longitude] and both\n",
      "     |  inputs and outputs are in units of radians.\n",
      "     |  \n",
      "     |  ============  ==================  ===============================================================\n",
      "     |  identifier    class name          distance function\n",
      "     |  ------------  ------------------  ---------------------------------------------------------------\n",
      "     |  \"haversine\"   HaversineDistance   ``2 arcsin(sqrt(sin^2(0.5*dx) + cos(x1)cos(x2)sin^2(0.5*dy)))``\n",
      "     |  ============  ==================  ===============================================================\n",
      "     |  \n",
      "     |  \n",
      "     |  **Metrics intended for integer-valued vector spaces:**  Though intended\n",
      "     |  for integer-valued vectors, these are also valid metrics in the case of\n",
      "     |  real-valued vectors.\n",
      "     |  \n",
      "     |  =============  ====================  ========================================\n",
      "     |  identifier     class name            distance function\n",
      "     |  -------------  --------------------  ----------------------------------------\n",
      "     |  \"hamming\"      HammingDistance       ``N_unequal(x, y) / N_tot``\n",
      "     |  \"canberra\"     CanberraDistance      ``sum(|x - y| / (|x| + |y|))``\n",
      "     |  \"braycurtis\"   BrayCurtisDistance    ``sum(|x - y|) / (sum(|x|) + sum(|y|))``\n",
      "     |  =============  ====================  ========================================\n",
      "     |  \n",
      "     |  **Metrics intended for boolean-valued vector spaces:**  Any nonzero entry\n",
      "     |  is evaluated to \"True\".  In the listings below, the following\n",
      "     |  abbreviations are used:\n",
      "     |  \n",
      "     |  - N: number of dimensions\n",
      "     |  - NTT: number of dims in which both values are True\n",
      "     |  - NTF: number of dims in which the first value is True, second is False\n",
      "     |  - NFT: number of dims in which the first value is False, second is True\n",
      "     |  - NFF: number of dims in which both values are False\n",
      "     |  - NNEQ: number of non-equal dimensions, NNEQ = NTF + NFT\n",
      "     |  - NNZ: number of nonzero dimensions, NNZ = NTF + NFT + NTT\n",
      "     |  \n",
      "     |  =================  =======================  ===============================\n",
      "     |  identifier         class name               distance function\n",
      "     |  -----------------  -----------------------  -------------------------------\n",
      "     |  \"jaccard\"          JaccardDistance          NNEQ / NNZ\n",
      "     |  \"matching\"         MatchingDistance         NNEQ / N\n",
      "     |  \"dice\"             DiceDistance             NNEQ / (NTT + NNZ)\n",
      "     |  \"kulsinski\"        KulsinskiDistance        (NNEQ + N - NTT) / (NNEQ + N)\n",
      "     |  \"rogerstanimoto\"   RogersTanimotoDistance   2 * NNEQ / (N + NNEQ)\n",
      "     |  \"russellrao\"       RussellRaoDistance       (N - NTT) / N\n",
      "     |  \"sokalmichener\"    SokalMichenerDistance    2 * NNEQ / (N + NNEQ)\n",
      "     |  \"sokalsneath\"      SokalSneathDistance      NNEQ / (NNEQ + 0.5 * NTT)\n",
      "     |  =================  =======================  ===============================\n",
      "     |  \n",
      "     |  **User-defined distance:**\n",
      "     |  \n",
      "     |  ===========    ===============    =======\n",
      "     |  identifier     class name         args\n",
      "     |  -----------    ---------------    -------\n",
      "     |  \"pyfunc\"       PyFuncDistance     func\n",
      "     |  ===========    ===============    =======\n",
      "     |  \n",
      "     |  Here ``func`` is a function which takes two one-dimensional numpy\n",
      "     |  arrays, and returns a distance.  Note that in order to be used within\n",
      "     |  the BallTree, the distance must be a true metric:\n",
      "     |  i.e. it must satisfy the following properties\n",
      "     |  \n",
      "     |  1) Non-negativity: d(x, y) >= 0\n",
      "     |  2) Identity: d(x, y) = 0 if and only if x == y\n",
      "     |  3) Symmetry: d(x, y) = d(y, x)\n",
      "     |  4) Triangle Inequality: d(x, y) + d(y, z) >= d(x, z)\n",
      "     |  \n",
      "     |  Because of the Python object overhead involved in calling the python\n",
      "     |  function, this will be fairly slow, but it will have the same\n",
      "     |  scaling as other distances.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __reduce__ = __reduce_cython__(...)\n",
      "     |  \n",
      "     |  __reduce_cython__(self)\n",
      "     |  \n",
      "     |  __setstate__ = __setstate_cython__(...)\n",
      "     |  \n",
      "     |  __setstate_cython__(self, __pyx_state)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  get_metric(metric, dtype=<class 'numpy.float64'>, **kwargs)\n",
      "     |      Get the given distance metric from the string identifier.\n",
      "     |      \n",
      "     |      See the docstring of DistanceMetric for a list of available metrics.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      metric : str or class name\n",
      "     |          The string identifier or class name of the desired distance metric.\n",
      "     |          See the documentation of the `DistanceMetric` class for a list of\n",
      "     |          available metrics.\n",
      "     |      \n",
      "     |      dtype : {np.float32, np.float64}, default=np.float64\n",
      "     |          The data type of the input on which the metric will be applied.\n",
      "     |          This affects the precision of the computed distances.\n",
      "     |          By default, it is set to `np.float64`.\n",
      "     |      \n",
      "     |      **kwargs\n",
      "     |          Additional keyword arguments that will be passed to the requested metric.\n",
      "     |          These arguments can be used to customize the behavior of the specific\n",
      "     |          metric.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      metric_obj : instance of the requested metric\n",
      "     |          An instance of the requested distance metric class.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class PrecisionRecallDisplay(sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin)\n",
      "     |  PrecisionRecallDisplay(precision, recall, *, average_precision=None, estimator_name=None, pos_label=None, prevalence_pos_label=None)\n",
      "     |  \n",
      "     |  Precision Recall visualization.\n",
      "     |  \n",
      "     |  It is recommended to use\n",
      "     |  :func:`~sklearn.metrics.PrecisionRecallDisplay.from_estimator` or\n",
      "     |  :func:`~sklearn.metrics.PrecisionRecallDisplay.from_predictions` to create\n",
      "     |  a :class:`~sklearn.metrics.PrecisionRecallDisplay`. All parameters are\n",
      "     |  stored as attributes.\n",
      "     |  \n",
      "     |  For general information regarding `scikit-learn` visualization tools, see\n",
      "     |  the :ref:`Visualization Guide <visualizations>`.\n",
      "     |  For guidance on interpreting these plots, refer to the :ref:`Model\n",
      "     |  Evaluation Guide <precision_recall_f_measure_metrics>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  precision : ndarray\n",
      "     |      Precision values.\n",
      "     |  \n",
      "     |  recall : ndarray\n",
      "     |      Recall values.\n",
      "     |  \n",
      "     |  average_precision : float, default=None\n",
      "     |      Average precision. If None, the average precision is not shown.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, then the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : int, float, bool or str, default=None\n",
      "     |      The class considered as the positive class. If None, the class will not\n",
      "     |      be shown in the legend.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  prevalence_pos_label : float, default=None\n",
      "     |      The prevalence of the positive label. It is used for plotting the\n",
      "     |      chance level line. If None, the chance level line will not be plotted\n",
      "     |      even if `plot_chance_level` is set to True when plotting.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      Precision recall curve.\n",
      "     |  \n",
      "     |  chance_level_ : matplotlib Artist or None\n",
      "     |      The chance level line. It is `None` if the chance level is not plotted.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with precision recall curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  precision_recall_curve : Compute precision-recall pairs for different\n",
      "     |      probability thresholds.\n",
      "     |  PrecisionRecallDisplay.from_estimator : Plot Precision Recall Curve given\n",
      "     |      a binary classifier.\n",
      "     |  PrecisionRecallDisplay.from_predictions : Plot Precision Recall Curve\n",
      "     |      using predictions from a binary classifier.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The average precision (cf. :func:`~sklearn.metrics.average_precision_score`) in\n",
      "     |  scikit-learn is computed without any interpolation. To be consistent with\n",
      "     |  this metric, the precision-recall curve is plotted without any\n",
      "     |  interpolation as well (step-wise style).\n",
      "     |  \n",
      "     |  You can change this style by passing the keyword argument\n",
      "     |  `drawstyle=\"default\"` in :meth:`plot`, :meth:`from_estimator`, or\n",
      "     |  :meth:`from_predictions`. However, the curve will not be strictly\n",
      "     |  consistent with the reported average precision.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import (precision_recall_curve,\n",
      "     |  ...                              PrecisionRecallDisplay)\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
      "     |  ...                                                     random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0)\n",
      "     |  >>> clf.fit(X_train, y_train)\n",
      "     |  SVC(random_state=0)\n",
      "     |  >>> predictions = clf.predict(X_test)\n",
      "     |  >>> precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
      "     |  >>> disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
      "     |  >>> disp.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PrecisionRecallDisplay\n",
      "     |      sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, precision, recall, *, average_precision=None, estimator_name=None, pos_label=None, prevalence_pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, plot_chance_level=False, chance_level_kw=None, despine=False, **kwargs)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Extra keyword arguments will be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : Matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of precision recall curve for labeling. If `None`, use\n",
      "     |          `estimator_name` if not `None`, otherwise no labeling is shown.\n",
      "     |      \n",
      "     |      plot_chance_level : bool, default=False\n",
      "     |          Whether to plot the chance level. The chance level is the prevalence\n",
      "     |          of the positive label computed from the data passed during\n",
      "     |          :meth:`from_estimator` or :meth:`from_predictions` call.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      chance_level_kw : dict, default=None\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot` for rendering\n",
      "     |          the chance level line.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      despine : bool, default=False\n",
      "     |          Whether to remove the top and right spines from the plot.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.6\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The average precision (cf. :func:`~sklearn.metrics.average_precision_score`)\n",
      "     |      in scikit-learn is computed without any interpolation. To be consistent\n",
      "     |      with this metric, the precision-recall curve is plotted without any\n",
      "     |      interpolation as well (step-wise style).\n",
      "     |      \n",
      "     |      You can change this style by passing the keyword argument\n",
      "     |      `drawstyle=\"default\"`. However, the curve will not be strictly\n",
      "     |      consistent with the reported average precision.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, sample_weight=None, drop_intermediate=False, response_method='auto', pos_label=None, name=None, ax=None, plot_chance_level=False, chance_level_kw=None, despine=False, **kwargs)\n",
      "     |      Plot precision-recall curve given an estimator and some data.\n",
      "     |      \n",
      "     |      For general information regarding `scikit-learn` visualization tools, see\n",
      "     |      the :ref:`Visualization Guide <visualizations>`.\n",
      "     |      For guidance on interpreting these plots, refer to the :ref:`Model\n",
      "     |      Evaluation Guide <precision_recall_f_measure_metrics>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      drop_intermediate : bool, default=False\n",
      "     |          Whether to drop some suboptimal thresholds which would not appear\n",
      "     |          on a plotted precision-recall curve. This is useful in order to\n",
      "     |          create lighter precision-recall curves.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      response_method : {'predict_proba', 'decision_function', 'auto'},             default='auto'\n",
      "     |          Specifies whether to use :term:`predict_proba` or\n",
      "     |          :term:`decision_function` as the target response. If set to 'auto',\n",
      "     |          :term:`predict_proba` is tried first and if it does not exist\n",
      "     |          :term:`decision_function` is tried next.\n",
      "     |      \n",
      "     |      pos_label : int, float, bool or str, default=None\n",
      "     |          The class considered as the positive class when computing the\n",
      "     |          precision and recall metrics. By default, `estimators.classes_[1]`\n",
      "     |          is considered as the positive class.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name for labeling curve. If `None`, no name is used.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "     |      \n",
      "     |      plot_chance_level : bool, default=False\n",
      "     |          Whether to plot the chance level. The chance level is the prevalence\n",
      "     |          of the positive label computed from the data passed during\n",
      "     |          :meth:`from_estimator` or :meth:`from_predictions` call.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      chance_level_kw : dict, default=None\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot` for rendering\n",
      "     |          the chance level line.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      despine : bool, default=False\n",
      "     |          Whether to remove the top and right spines from the plot.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.6\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      PrecisionRecallDisplay.from_predictions : Plot precision-recall curve\n",
      "     |          using estimated probabilities or output of decision function.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The average precision (cf. :func:`~sklearn.metrics.average_precision_score`)\n",
      "     |      in scikit-learn is computed without any interpolation. To be consistent\n",
      "     |      with this metric, the precision-recall curve is plotted without any\n",
      "     |      interpolation as well (step-wise style).\n",
      "     |      \n",
      "     |      You can change this style by passing the keyword argument\n",
      "     |      `drawstyle=\"default\"`. However, the curve will not be strictly\n",
      "     |      consistent with the reported average precision.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import PrecisionRecallDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.linear_model import LogisticRegression\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = LogisticRegression()\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      LogisticRegression()\n",
      "     |      >>> PrecisionRecallDisplay.from_estimator(\n",
      "     |      ...    clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, sample_weight=None, drop_intermediate=False, pos_label=None, name=None, ax=None, plot_chance_level=False, chance_level_kw=None, despine=False, **kwargs)\n",
      "     |      Plot precision-recall curve given binary class predictions.\n",
      "     |      \n",
      "     |      For general information regarding `scikit-learn` visualization tools, see\n",
      "     |      the :ref:`Visualization Guide <visualizations>`.\n",
      "     |      For guidance on interpreting these plots, refer to the :ref:`Model\n",
      "     |      Evaluation Guide <precision_recall_f_measure_metrics>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True binary labels.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          Estimated probabilities or output of decision function.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      drop_intermediate : bool, default=False\n",
      "     |          Whether to drop some suboptimal thresholds which would not appear\n",
      "     |          on a plotted precision-recall curve. This is useful in order to\n",
      "     |          create lighter precision-recall curves.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      pos_label : int, float, bool or str, default=None\n",
      "     |          The class considered as the positive class when computing the\n",
      "     |          precision and recall metrics.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name for labeling curve. If `None`, name will be set to\n",
      "     |          `\"Classifier\"`.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "     |      \n",
      "     |      plot_chance_level : bool, default=False\n",
      "     |          Whether to plot the chance level. The chance level is the prevalence\n",
      "     |          of the positive label computed from the data passed during\n",
      "     |          :meth:`from_estimator` or :meth:`from_predictions` call.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      chance_level_kw : dict, default=None\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot` for rendering\n",
      "     |          the chance level line.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      despine : bool, default=False\n",
      "     |          Whether to remove the top and right spines from the plot.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.6\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      PrecisionRecallDisplay.from_estimator : Plot precision-recall curve\n",
      "     |          using an estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The average precision (cf. :func:`~sklearn.metrics.average_precision_score`)\n",
      "     |      in scikit-learn is computed without any interpolation. To be consistent\n",
      "     |      with this metric, the precision-recall curve is plotted without any\n",
      "     |      interpolation as well (step-wise style).\n",
      "     |      \n",
      "     |      You can change this style by passing the keyword argument\n",
      "     |      `drawstyle=\"default\"`. However, the curve will not be strictly\n",
      "     |      consistent with the reported average precision.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import PrecisionRecallDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.linear_model import LogisticRegression\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...         X, y, random_state=0)\n",
      "     |      >>> clf = LogisticRegression()\n",
      "     |      >>> clf.fit(X_train, y_train)\n",
      "     |      LogisticRegression()\n",
      "     |      >>> y_pred = clf.predict_proba(X_test)[:, 1]\n",
      "     |      >>> PrecisionRecallDisplay.from_predictions(\n",
      "     |      ...    y_test, y_pred)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class PredictionErrorDisplay(builtins.object)\n",
      "     |  PredictionErrorDisplay(*, y_true, y_pred)\n",
      "     |  \n",
      "     |  Visualization of the prediction error of a regression model.\n",
      "     |  \n",
      "     |  This tool can display \"residuals vs predicted\" or \"actual vs predicted\"\n",
      "     |  using scatter plots to qualitatively assess the behavior of a regressor,\n",
      "     |  preferably on held-out data points.\n",
      "     |  \n",
      "     |  See the details in the docstrings of\n",
      "     |  :func:`~sklearn.metrics.PredictionErrorDisplay.from_estimator` or\n",
      "     |  :func:`~sklearn.metrics.PredictionErrorDisplay.from_predictions` to\n",
      "     |  create a visualizer. All parameters are stored as attributes.\n",
      "     |  \n",
      "     |  For general information regarding `scikit-learn` visualization tools, read\n",
      "     |  more in the :ref:`Visualization Guide <visualizations>`.\n",
      "     |  For details regarding interpreting these plots, refer to the\n",
      "     |  :ref:`Model Evaluation Guide <visualization_regression_evaluation>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.2\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  y_true : ndarray of shape (n_samples,)\n",
      "     |      True values.\n",
      "     |  \n",
      "     |  y_pred : ndarray of shape (n_samples,)\n",
      "     |      Prediction values.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      Optimal line representing `y_true == y_pred`. Therefore, it is a\n",
      "     |      diagonal line for `kind=\"predictions\"` and a horizontal line for\n",
      "     |      `kind=\"residuals\"`.\n",
      "     |  \n",
      "     |  errors_lines_ : matplotlib Artist or None\n",
      "     |      Residual lines. If `with_errors=False`, then it is set to `None`.\n",
      "     |  \n",
      "     |  scatter_ : matplotlib Artist\n",
      "     |      Scatter data points.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with the different matplotlib axis.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the scatter and lines.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  PredictionErrorDisplay.from_estimator : Prediction error visualization\n",
      "     |      given an estimator and some data.\n",
      "     |  PredictionErrorDisplay.from_predictions : Prediction error visualization\n",
      "     |      given the true and predicted targets.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> from sklearn.datasets import load_diabetes\n",
      "     |  >>> from sklearn.linear_model import Ridge\n",
      "     |  >>> from sklearn.metrics import PredictionErrorDisplay\n",
      "     |  >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |  >>> ridge = Ridge().fit(X, y)\n",
      "     |  >>> y_pred = ridge.predict(X)\n",
      "     |  >>> display = PredictionErrorDisplay(y_true=y, y_pred=y_pred)\n",
      "     |  >>> display.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, y_true, y_pred)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, kind='residual_vs_predicted', scatter_kwargs=None, line_kwargs=None)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Extra keyword arguments will be passed to matplotlib's ``plot``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      kind : {\"actual_vs_predicted\", \"residual_vs_predicted\"},                 default=\"residual_vs_predicted\"\n",
      "     |          The type of plot to draw:\n",
      "     |      \n",
      "     |          - \"actual_vs_predicted\" draws the observed values (y-axis) vs.\n",
      "     |            the predicted values (x-axis).\n",
      "     |          - \"residual_vs_predicted\" draws the residuals, i.e. difference\n",
      "     |            between observed and predicted values, (y-axis) vs. the predicted\n",
      "     |            values (x-axis).\n",
      "     |      \n",
      "     |      scatter_kwargs : dict, default=None\n",
      "     |          Dictionary with keywords passed to the `matplotlib.pyplot.scatter`\n",
      "     |          call.\n",
      "     |      \n",
      "     |      line_kwargs : dict, default=None\n",
      "     |          Dictionary with keyword passed to the `matplotlib.pyplot.plot`\n",
      "     |          call to draw the optimal line.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PredictionErrorDisplay`\n",
      "     |      \n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, kind='residual_vs_predicted', subsample=1000, random_state=None, ax=None, scatter_kwargs=None, line_kwargs=None)\n",
      "     |      Plot the prediction error given a regressor and some data.\n",
      "     |      \n",
      "     |      For general information regarding `scikit-learn` visualization tools,\n",
      "     |      read more in the :ref:`Visualization Guide <visualizations>`.\n",
      "     |      For details regarding interpreting these plots, refer to the\n",
      "     |      :ref:`Model Evaluation Guide <visualization_regression_evaluation>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted regressor or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a regressor.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      kind : {\"actual_vs_predicted\", \"residual_vs_predicted\"},                 default=\"residual_vs_predicted\"\n",
      "     |          The type of plot to draw:\n",
      "     |      \n",
      "     |          - \"actual_vs_predicted\" draws the observed values (y-axis) vs.\n",
      "     |            the predicted values (x-axis).\n",
      "     |          - \"residual_vs_predicted\" draws the residuals, i.e. difference\n",
      "     |            between observed and predicted values, (y-axis) vs. the predicted\n",
      "     |            values (x-axis).\n",
      "     |      \n",
      "     |      subsample : float, int or None, default=1_000\n",
      "     |          Sampling the samples to be shown on the scatter plot. If `float`,\n",
      "     |          it should be between 0 and 1 and represents the proportion of the\n",
      "     |          original dataset. If `int`, it represents the number of samples\n",
      "     |          display on the scatter plot. If `None`, no subsampling will be\n",
      "     |          applied. by default, 1000 samples or less will be displayed.\n",
      "     |      \n",
      "     |      random_state : int or RandomState, default=None\n",
      "     |          Controls the randomness when `subsample` is not `None`.\n",
      "     |          See :term:`Glossary <random_state>` for details.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      scatter_kwargs : dict, default=None\n",
      "     |          Dictionary with keywords passed to the `matplotlib.pyplot.scatter`\n",
      "     |          call.\n",
      "     |      \n",
      "     |      line_kwargs : dict, default=None\n",
      "     |          Dictionary with keyword passed to the `matplotlib.pyplot.plot`\n",
      "     |          call to draw the optimal line.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PredictionErrorDisplay`\n",
      "     |          Object that stores the computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      PredictionErrorDisplay : Prediction error visualization for regression.\n",
      "     |      PredictionErrorDisplay.from_predictions : Prediction error visualization\n",
      "     |          given the true and predicted targets.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import load_diabetes\n",
      "     |      >>> from sklearn.linear_model import Ridge\n",
      "     |      >>> from sklearn.metrics import PredictionErrorDisplay\n",
      "     |      >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |      >>> ridge = Ridge().fit(X, y)\n",
      "     |      >>> disp = PredictionErrorDisplay.from_estimator(ridge, X, y)\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_pred, *, kind='residual_vs_predicted', subsample=1000, random_state=None, ax=None, scatter_kwargs=None, line_kwargs=None)\n",
      "     |      Plot the prediction error given the true and predicted targets.\n",
      "     |      \n",
      "     |      For general information regarding `scikit-learn` visualization tools,\n",
      "     |      read more in the :ref:`Visualization Guide <visualizations>`.\n",
      "     |      For details regarding interpreting these plots, refer to the\n",
      "     |      :ref:`Model Evaluation Guide <visualization_regression_evaluation>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.2\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True target values.\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          Predicted target values.\n",
      "     |      \n",
      "     |      kind : {\"actual_vs_predicted\", \"residual_vs_predicted\"},                 default=\"residual_vs_predicted\"\n",
      "     |          The type of plot to draw:\n",
      "     |      \n",
      "     |          - \"actual_vs_predicted\" draws the observed values (y-axis) vs.\n",
      "     |            the predicted values (x-axis).\n",
      "     |          - \"residual_vs_predicted\" draws the residuals, i.e. difference\n",
      "     |            between observed and predicted values, (y-axis) vs. the predicted\n",
      "     |            values (x-axis).\n",
      "     |      \n",
      "     |      subsample : float, int or None, default=1_000\n",
      "     |          Sampling the samples to be shown on the scatter plot. If `float`,\n",
      "     |          it should be between 0 and 1 and represents the proportion of the\n",
      "     |          original dataset. If `int`, it represents the number of samples\n",
      "     |          display on the scatter plot. If `None`, no subsampling will be\n",
      "     |          applied. by default, 1000 samples or less will be displayed.\n",
      "     |      \n",
      "     |      random_state : int or RandomState, default=None\n",
      "     |          Controls the randomness when `subsample` is not `None`.\n",
      "     |          See :term:`Glossary <random_state>` for details.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      scatter_kwargs : dict, default=None\n",
      "     |          Dictionary with keywords passed to the `matplotlib.pyplot.scatter`\n",
      "     |          call.\n",
      "     |      \n",
      "     |      line_kwargs : dict, default=None\n",
      "     |          Dictionary with keyword passed to the `matplotlib.pyplot.plot`\n",
      "     |          call to draw the optimal line.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PredictionErrorDisplay`\n",
      "     |          Object that stores the computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      PredictionErrorDisplay : Prediction error visualization for regression.\n",
      "     |      PredictionErrorDisplay.from_estimator : Prediction error visualization\n",
      "     |          given an estimator and some data.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import load_diabetes\n",
      "     |      >>> from sklearn.linear_model import Ridge\n",
      "     |      >>> from sklearn.metrics import PredictionErrorDisplay\n",
      "     |      >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |      >>> ridge = Ridge().fit(X, y)\n",
      "     |      >>> y_pred = ridge.predict(X)\n",
      "     |      >>> disp = PredictionErrorDisplay.from_predictions(y_true=y, y_pred=y_pred)\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "    \n",
      "    class RocCurveDisplay(sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin)\n",
      "     |  RocCurveDisplay(*, fpr, tpr, roc_auc=None, name=None, pos_label=None, estimator_name='deprecated')\n",
      "     |  \n",
      "     |  ROC Curve visualization.\n",
      "     |  \n",
      "     |  It is recommended to use\n",
      "     |  :func:`~sklearn.metrics.RocCurveDisplay.from_estimator` or\n",
      "     |  :func:`~sklearn.metrics.RocCurveDisplay.from_predictions` or\n",
      "     |  :func:`~sklearn.metrics.RocCurveDisplay.from_cv_results` to create\n",
      "     |  a :class:`~sklearn.metrics.RocCurveDisplay`. All parameters are\n",
      "     |  stored as attributes.\n",
      "     |  \n",
      "     |  For general information regarding `scikit-learn` visualization tools, see\n",
      "     |  the :ref:`Visualization Guide <visualizations>`.\n",
      "     |  For guidance on interpreting these plots, refer to the :ref:`Model\n",
      "     |  Evaluation Guide <roc_metrics>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fpr : ndarray or list of ndarrays\n",
      "     |      False positive rates. Each ndarray should contain values for a single curve.\n",
      "     |      If plotting multiple curves, list should be of same length as `tpr`.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.7\n",
      "     |          Now accepts a list for plotting multiple curves.\n",
      "     |  \n",
      "     |  tpr : ndarray or list of ndarrays\n",
      "     |      True positive rates. Each ndarray should contain values for a single curve.\n",
      "     |      If plotting multiple curves, list should be of same length as `fpr`.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.7\n",
      "     |          Now accepts a list for plotting multiple curves.\n",
      "     |  \n",
      "     |  roc_auc : float or list of floats, default=None\n",
      "     |      Area under ROC curve, used for labeling each curve in the legend.\n",
      "     |      If plotting multiple curves, should be a list of the same length as `fpr`\n",
      "     |      and `tpr`. If `None`, ROC AUC scores are not shown in the legend.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.7\n",
      "     |          Now accepts a list for plotting multiple curves.\n",
      "     |  \n",
      "     |  name : str or list of str, default=None\n",
      "     |      Name for labeling legend entries. The number of legend entries is determined\n",
      "     |      by the `curve_kwargs` passed to `plot`, and is not affected by `name`.\n",
      "     |      To label each curve, provide a list of strings. To avoid labeling\n",
      "     |      individual curves that have the same appearance, this cannot be used in\n",
      "     |      conjunction with `curve_kwargs` being a dictionary or None. If a\n",
      "     |      string is provided, it will be used to either label the single legend entry\n",
      "     |      or if there are multiple legend entries, label each individual curve with\n",
      "     |      the same name. If still `None`, no name is shown in the legend.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.7\n",
      "     |  \n",
      "     |  pos_label : int, float, bool or str, default=None\n",
      "     |      The class considered as the positive class when computing the roc auc\n",
      "     |      metrics. By default, `estimators.classes_[1]` is considered\n",
      "     |      as the positive class.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, the estimator name is not shown.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.7\n",
      "     |          `estimator_name` is deprecated and will be removed in 1.9. Use `name`\n",
      "     |          instead.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist or list of matplotlib Artists\n",
      "     |      ROC Curves.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.7\n",
      "     |          This attribute can now be a list of Artists, for when multiple curves\n",
      "     |          are plotted.\n",
      "     |  \n",
      "     |  chance_level_ : matplotlib Artist or None\n",
      "     |      The chance level line. It is `None` if the chance level is not plotted.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with ROC Curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "     |  RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n",
      "     |      (ROC) curve given an estimator and some data.\n",
      "     |  RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n",
      "     |      (ROC) curve given the true and predicted values.\n",
      "     |  roc_auc_score : Compute the area under the ROC curve.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import metrics\n",
      "     |  >>> y_true = np.array([0, 0, 1, 1])\n",
      "     |  >>> y_score = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "     |  >>> fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score)\n",
      "     |  >>> roc_auc = metrics.auc(fpr, tpr)\n",
      "     |  >>> display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
      "     |  ...                                   name='example estimator')\n",
      "     |  >>> display.plot()\n",
      "     |  <...>\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RocCurveDisplay\n",
      "     |      sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fpr, tpr, roc_auc=None, name=None, pos_label=None, estimator_name='deprecated')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, curve_kwargs=None, plot_chance_level=False, chance_level_kw=None, despine=False, **kwargs)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str or list of str, default=None\n",
      "     |          Name for labeling legend entries. The number of legend entries\n",
      "     |          is determined by `curve_kwargs`, and is not affected by `name`.\n",
      "     |          To label each curve, provide a list of strings. To avoid labeling\n",
      "     |          individual curves that have the same appearance, this cannot be used in\n",
      "     |          conjunction with `curve_kwargs` being a dictionary or None. If a\n",
      "     |          string is provided, it will be used to either label the single legend entry\n",
      "     |          or if there are multiple legend entries, label each individual curve with\n",
      "     |          the same name. If `None`, set to `name` provided at `RocCurveDisplay`\n",
      "     |          initialization. If still `None`, no name is shown in the legend.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |      curve_kwargs : dict or list of dict, default=None\n",
      "     |          Keywords arguments to be passed to matplotlib's `plot` function\n",
      "     |          to draw individual ROC curves. For single curve plotting, should be\n",
      "     |          a dictionary. For multi-curve plotting, if a list is provided the\n",
      "     |          parameters are applied to the ROC curves of each CV fold\n",
      "     |          sequentially and a legend entry is added for each curve.\n",
      "     |          If a single dictionary is provided, the same parameters are applied\n",
      "     |          to all ROC curves and a single legend entry for all curves is added,\n",
      "     |          labeled with the mean ROC AUC score.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |      plot_chance_level : bool, default=False\n",
      "     |          Whether to plot the chance level.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      chance_level_kw : dict, default=None\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot` for rendering\n",
      "     |          the chance level line.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      despine : bool, default=False\n",
      "     |          Whether to remove the top and right spines from the plot.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.6\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |          .. deprecated:: 1.7\n",
      "     |              kwargs is deprecated and will be removed in 1.9. Pass matplotlib\n",
      "     |              arguments to `curve_kwargs` as a dictionary instead.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.RocCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_cv_results(cv_results, X, y, *, sample_weight=None, drop_intermediate=True, response_method='auto', pos_label=None, ax=None, name=None, curve_kwargs=None, plot_chance_level=False, chance_level_kwargs=None, despine=False)\n",
      "     |      Create a multi-fold ROC curve display given cross-validation results.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.7\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      cv_results : dict\n",
      "     |          Dictionary as returned by :func:`~sklearn.model_selection.cross_validate`\n",
      "     |          using `return_estimator=True` and `return_indices=True` (i.e., dictionary\n",
      "     |          should contain the keys \"estimator\" and \"indices\").\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      drop_intermediate : bool, default=True\n",
      "     |          Whether to drop some suboptimal thresholds which would not appear\n",
      "     |          on a plotted ROC curve. This is useful in order to create lighter\n",
      "     |          ROC curves.\n",
      "     |      \n",
      "     |      response_method : {'predict_proba', 'decision_function', 'auto'}                 default='auto'\n",
      "     |          Specifies whether to use :term:`predict_proba` or\n",
      "     |          :term:`decision_function` as the target response. If set to 'auto',\n",
      "     |          :term:`predict_proba` is tried first and if it does not exist\n",
      "     |          :term:`decision_function` is tried next.\n",
      "     |      \n",
      "     |      pos_label : int, float, bool or str, default=None\n",
      "     |          The class considered as the positive class when computing the ROC AUC\n",
      "     |          metrics. By default, `estimators.classes_[1]` is considered\n",
      "     |          as the positive class.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str or list of str, default=None\n",
      "     |          Name for labeling legend entries. The number of legend entries\n",
      "     |          is determined by `curve_kwargs`, and is not affected by `name`.\n",
      "     |          To label each curve, provide a list of strings. To avoid labeling\n",
      "     |          individual curves that have the same appearance, this cannot be used in\n",
      "     |          conjunction with `curve_kwargs` being a dictionary or None. If a\n",
      "     |          string is provided, it will be used to either label the single legend entry\n",
      "     |          or if there are multiple legend entries, label each individual curve with\n",
      "     |          the same name. If `None`, no name is shown in the legend.\n",
      "     |      \n",
      "     |      curve_kwargs : dict or list of dict, default=None\n",
      "     |          Keywords arguments to be passed to matplotlib's `plot` function\n",
      "     |          to draw individual ROC curves. If a list is provided the\n",
      "     |          parameters are applied to the ROC curves of each CV fold\n",
      "     |          sequentially and a legend entry is added for each curve.\n",
      "     |          If a single dictionary is provided, the same parameters are applied\n",
      "     |          to all ROC curves and a single legend entry for all curves is added,\n",
      "     |          labeled with the mean ROC AUC score.\n",
      "     |      \n",
      "     |      plot_chance_level : bool, default=False\n",
      "     |          Whether to plot the chance level.\n",
      "     |      \n",
      "     |      chance_level_kwargs : dict, default=None\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot` for rendering\n",
      "     |          the chance level line.\n",
      "     |      \n",
      "     |      despine : bool, default=False\n",
      "     |          Whether to remove the top and right spines from the plot.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.RocCurveDisplay`\n",
      "     |          The multi-fold ROC curve display.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "     |          RocCurveDisplay.from_estimator : ROC Curve visualization given an\n",
      "     |          estimator and some data.\n",
      "     |      RocCurveDisplay.from_predictions : ROC Curve visualization given the\n",
      "     |          probabilities of scores of a classifier.\n",
      "     |      roc_auc_score : Compute the area under the ROC curve.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import RocCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import cross_validate\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0)\n",
      "     |      >>> cv_results = cross_validate(\n",
      "     |      ...     clf, X, y, cv=3, return_estimator=True, return_indices=True)\n",
      "     |      >>> RocCurveDisplay.from_cv_results(cv_results, X, y)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_estimator(estimator, X, y, *, sample_weight=None, drop_intermediate=True, response_method='auto', pos_label=None, name=None, ax=None, curve_kwargs=None, plot_chance_level=False, chance_level_kw=None, despine=False, **kwargs)\n",
      "     |      Create a ROC Curve display from an estimator.\n",
      "     |      \n",
      "     |      For general information regarding `scikit-learn` visualization tools,\n",
      "     |      see the :ref:`Visualization Guide <visualizations>`.\n",
      "     |      For guidance on interpreting these plots, refer to the :ref:`Model\n",
      "     |      Evaluation Guide <roc_metrics>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      estimator : estimator instance\n",
      "     |          Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "     |          in which the last estimator is a classifier.\n",
      "     |      \n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input values.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      drop_intermediate : bool, default=True\n",
      "     |          Whether to drop thresholds where the resulting point is collinear\n",
      "     |          with its neighbors in ROC space. This has no effect on the ROC AUC\n",
      "     |          or visual shape of the curve, but reduces the number of plotted\n",
      "     |          points.\n",
      "     |      \n",
      "     |      response_method : {'predict_proba', 'decision_function', 'auto'}                 default='auto'\n",
      "     |          Specifies whether to use :term:`predict_proba` or\n",
      "     |          :term:`decision_function` as the target response. If set to 'auto',\n",
      "     |          :term:`predict_proba` is tried first and if it does not exist\n",
      "     |          :term:`decision_function` is tried next.\n",
      "     |      \n",
      "     |      pos_label : int, float, bool or str, default=None\n",
      "     |          The class considered as the positive class when computing the ROC AUC.\n",
      "     |          By default, `estimators.classes_[1]` is considered\n",
      "     |          as the positive class.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of ROC Curve for labeling. If `None`, use the name of the\n",
      "     |          estimator.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "     |      \n",
      "     |      curve_kwargs : dict, default=None\n",
      "     |          Keywords arguments to be passed to matplotlib's `plot` function.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |      plot_chance_level : bool, default=False\n",
      "     |          Whether to plot the chance level.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      chance_level_kw : dict, default=None\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot` for rendering\n",
      "     |          the chance level line.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      despine : bool, default=False\n",
      "     |          Whether to remove the top and right spines from the plot.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.6\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |          .. deprecated:: 1.7\n",
      "     |              kwargs is deprecated and will be removed in 1.9. Pass matplotlib\n",
      "     |              arguments to `curve_kwargs` as a dictionary instead.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.RocCurveDisplay`\n",
      "     |          The ROC Curve display.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "     |      RocCurveDisplay.from_predictions : ROC Curve visualization given the\n",
      "     |          probabilities of scores of a classifier.\n",
      "     |      roc_auc_score : Compute the area under the ROC curve.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import RocCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> RocCurveDisplay.from_estimator(\n",
      "     |      ...    clf, X_test, y_test)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  from_predictions(y_true, y_score=None, *, sample_weight=None, drop_intermediate=True, pos_label=None, name=None, ax=None, curve_kwargs=None, plot_chance_level=False, chance_level_kw=None, despine=False, y_pred='deprecated', **kwargs)\n",
      "     |      Plot ROC curve given the true and predicted values.\n",
      "     |      \n",
      "     |      For general information regarding `scikit-learn` visualization tools,\n",
      "     |      see the :ref:`Visualization Guide <visualizations>`.\n",
      "     |      For guidance on interpreting these plots, refer to the :ref:`Model\n",
      "     |      Evaluation Guide <roc_metrics>`.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.0\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      y_true : array-like of shape (n_samples,)\n",
      "     |          True labels.\n",
      "     |      \n",
      "     |      y_score : array-like of shape (n_samples,)\n",
      "     |          Target scores, can either be probability estimates of the positive\n",
      "     |          class, confidence values, or non-thresholded measure of decisions\n",
      "     |          (as returned by “decision_function” on some classifiers).\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |              `y_pred` has been renamed to `y_score`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      drop_intermediate : bool, default=True\n",
      "     |          Whether to drop thresholds where the resulting point is collinear\n",
      "     |          with its neighbors in ROC space. This has no effect on the ROC AUC\n",
      "     |          or visual shape of the curve, but reduces the number of plotted\n",
      "     |          points.\n",
      "     |      \n",
      "     |      pos_label : int, float, bool or str, default=None\n",
      "     |          The label of the positive class when computing the ROC AUC.\n",
      "     |          When `pos_label=None`, if `y_true` is in {-1, 1} or {0, 1}, `pos_label`\n",
      "     |          is set to 1, otherwise an error will be raised.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of ROC curve for legend labeling. If `None`, name will be set to\n",
      "     |          `\"Classifier\"`.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      curve_kwargs : dict, default=None\n",
      "     |          Keywords arguments to be passed to matplotlib's `plot` function.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |      plot_chance_level : bool, default=False\n",
      "     |          Whether to plot the chance level.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      chance_level_kw : dict, default=None\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot` for rendering\n",
      "     |          the chance level line.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      despine : bool, default=False\n",
      "     |          Whether to remove the top and right spines from the plot.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.6\n",
      "     |      \n",
      "     |      y_pred : array-like of shape (n_samples,)\n",
      "     |          Target scores, can either be probability estimates of the positive\n",
      "     |          class, confidence values, or non-thresholded measure of decisions\n",
      "     |          (as returned by “decision_function” on some classifiers).\n",
      "     |      \n",
      "     |          .. deprecated:: 1.7\n",
      "     |              `y_pred` is deprecated and will be removed in 1.9. Use\n",
      "     |              `y_score` instead.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Additional keywords arguments passed to matplotlib `plot` function.\n",
      "     |      \n",
      "     |          .. deprecated:: 1.7\n",
      "     |              kwargs is deprecated and will be removed in 1.9. Pass matplotlib\n",
      "     |              arguments to `curve_kwargs` as a dictionary instead.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.RocCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "     |      RocCurveDisplay.from_estimator : ROC Curve visualization given an\n",
      "     |          estimator and some data.\n",
      "     |      roc_auc_score : Compute the area under the ROC curve.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> from sklearn.datasets import make_classification\n",
      "     |      >>> from sklearn.metrics import RocCurveDisplay\n",
      "     |      >>> from sklearn.model_selection import train_test_split\n",
      "     |      >>> from sklearn.svm import SVC\n",
      "     |      >>> X, y = make_classification(random_state=0)\n",
      "     |      >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |      ...     X, y, random_state=0)\n",
      "     |      >>> clf = SVC(random_state=0).fit(X_train, y_train)\n",
      "     |      >>> y_score = clf.decision_function(X_test)\n",
      "     |      >>> RocCurveDisplay.from_predictions(y_test, y_score)\n",
      "     |      <...>\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "\n",
      "FUNCTIONS\n",
      "    accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)\n",
      "        Accuracy classification score.\n",
      "        \n",
      "        In multilabel classification, this function computes subset accuracy:\n",
      "        the set of labels predicted for a sample must *exactly* match the\n",
      "        corresponding set of labels in y_true.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <accuracy_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If ``False``, return the number of correctly classified samples.\n",
      "            Otherwise, return the fraction of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or int\n",
      "            If ``normalize == True``, return the fraction of correctly\n",
      "            classified samples (float), else returns the number of correctly\n",
      "            classified samples (int).\n",
      "        \n",
      "            The best performance is 1 with ``normalize == True`` and the number\n",
      "            of samples with ``normalize == False``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        balanced_accuracy_score : Compute the balanced accuracy to deal with\n",
      "            imbalanced datasets.\n",
      "        jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "        hamming_loss : Compute the average Hamming loss or Hamming distance between\n",
      "            two sets of samples.\n",
      "        zero_one_loss : Compute the Zero-one classification loss. By default, the\n",
      "            function will return the percentage of imperfectly predicted subsets.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import accuracy_score\n",
      "        >>> y_pred = [0, 2, 1, 3]\n",
      "        >>> y_true = [0, 1, 2, 3]\n",
      "        >>> accuracy_score(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> accuracy_score(y_true, y_pred, normalize=False)\n",
      "        2.0\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "    \n",
      "    adjusted_mutual_info_score(labels_true, labels_pred, *, average_method='arithmetic')\n",
      "        Adjusted Mutual Information between two clusterings.\n",
      "        \n",
      "        Adjusted Mutual Information (AMI) is an adjustment of the Mutual\n",
      "        Information (MI) score to account for chance. It accounts for the fact that\n",
      "        the MI is generally higher for two clusterings with a larger number of\n",
      "        clusters, regardless of whether there is actually more information shared.\n",
      "        For two clusterings :math:`U` and :math:`V`, the AMI is given as::\n",
      "        \n",
      "            AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching :math:`U` (``label_true``)\n",
      "        with :math:`V` (``labels_pred``) will return the same score value. This can\n",
      "        be useful to measure the agreement of two independent label assignments\n",
      "        strategies on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Be mindful that this function is an order of magnitude slower than other\n",
      "        metrics, such as the Adjusted Rand Index.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets, called :math:`U` in\n",
      "            the above formula.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets, called :math:`V` in\n",
      "            the above formula.\n",
      "        \n",
      "        average_method : {'min', 'geometric', 'arithmetic', 'max'}, default='arithmetic'\n",
      "            How to compute the normalizer in the denominator.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "               The default value of ``average_method`` changed from 'max' to\n",
      "               'arithmetic'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ami: float (upperlimited by 1.0)\n",
      "           The AMI returns a value of 1 when the two partitions are identical\n",
      "           (ie perfectly matched). Random partitions (independent labellings) have\n",
      "           an expected AMI around 0 on average hence can be negative. The value is\n",
      "           in adjusted nats (based on the natural logarithm).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_rand_score : Adjusted Rand Index.\n",
      "        mutual_info_score : Mutual Information (not adjusted for chance).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for\n",
      "           Clusterings Comparison: Variants, Properties, Normalization and\n",
      "           Correction for Chance, JMLR\n",
      "           <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Adjusted Mutual Information\n",
      "           <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the AMI is null::\n",
      "        \n",
      "          >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "    \n",
      "    adjusted_rand_score(labels_true, labels_pred)\n",
      "        Rand index adjusted for chance.\n",
      "        \n",
      "        The Rand Index computes a similarity measure between two clusterings\n",
      "        by considering all pairs of samples and counting pairs that are\n",
      "        assigned in the same or different clusters in the predicted and\n",
      "        true clusterings.\n",
      "        \n",
      "        The raw RI score is then \"adjusted for chance\" into the ARI score\n",
      "        using the following scheme::\n",
      "        \n",
      "            ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
      "        \n",
      "        The adjusted Rand index is thus ensured to have a value close to\n",
      "        0.0 for random labeling independently of the number of clusters and\n",
      "        samples and exactly 1.0 when the clusterings are identical (up to\n",
      "        a permutation). The adjusted Rand index is bounded below by -0.5 for\n",
      "        especially discordant clusterings.\n",
      "        \n",
      "        ARI is a symmetric measure::\n",
      "        \n",
      "            adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <adjusted_rand_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=int\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=int\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ARI : float\n",
      "           Similarity score between -0.5 and 1.0. Random labelings have an ARI\n",
      "           close to 0.0. 1.0 stands for perfect match.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_mutual_info_score : Adjusted Mutual Information.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,\n",
      "          Journal of Classification 1985\n",
      "          https://link.springer.com/article/10.1007%2FBF01908075\n",
      "        \n",
      "        .. [Steinley2004] D. Steinley, Properties of the Hubert-Arabie\n",
      "          adjusted Rand index, Psychological Methods 2004\n",
      "        \n",
      "        .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index\n",
      "        \n",
      "        .. [Chacon] :doi:`Minimum adjusted Rand index for two clusterings of a given size,\n",
      "          2022, J. E. Chacón and A. I. Rastrojo <10.1007/s11634-022-00491-w>`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have a score of 1 even\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_rand_score\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may not always be pure, hence penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          0.57\n",
      "        \n",
      "        ARI is symmetric, so labelings that have pure clusters with members\n",
      "        coming from the same classes but unnecessary splits are penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])\n",
      "          0.57\n",
      "        \n",
      "        If classes members are completely split across different clusters, the\n",
      "        assignment is totally incomplete, hence the ARI is very low::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        ARI may take a negative value for especially discordant labelings that\n",
      "        are a worse choice than the expected value of random labels::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 1, 0, 1])\n",
      "          -0.5\n",
      "        \n",
      "        See :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`\n",
      "        for a more detailed example.\n",
      "    \n",
      "    auc(x, y)\n",
      "        Compute Area Under the Curve (AUC) using the trapezoidal rule.\n",
      "        \n",
      "        This is a general function, given points on a curve.  For computing the\n",
      "        area under the ROC-curve, see :func:`roc_auc_score`.  For an alternative\n",
      "        way to summarize a precision-recall curve, see\n",
      "        :func:`average_precision_score`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array-like of shape (n,)\n",
      "            X coordinates. These must be either monotonic increasing or monotonic\n",
      "            decreasing.\n",
      "        y : array-like of shape (n,)\n",
      "            Y coordinates.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "            Area Under the Curve.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        average_precision_score : Compute average precision from prediction scores.\n",
      "        precision_recall_curve : Compute precision-recall pairs for different\n",
      "            probability thresholds.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y_true = np.array([1, 1, 2, 2])\n",
      "        >>> y_score = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score, pos_label=2)\n",
      "        >>> metrics.auc(fpr, tpr)\n",
      "        0.75\n",
      "    \n",
      "    average_precision_score(y_true, y_score, *, average='macro', pos_label=1, sample_weight=None)\n",
      "        Compute average precision (AP) from prediction scores.\n",
      "        \n",
      "        AP summarizes a precision-recall curve as the weighted mean of precisions\n",
      "        achieved at each threshold, with the increase in recall from the previous\n",
      "        threshold used as the weight:\n",
      "        \n",
      "        .. math::\n",
      "            \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\n",
      "        \n",
      "        where :math:`P_n` and :math:`R_n` are the precision and recall at the nth\n",
      "        threshold [1]_. This implementation is not interpolated and is different\n",
      "        from computing the area under the precision-recall curve with the\n",
      "        trapezoidal rule, which uses linear interpolation and can be too\n",
      "        optimistic.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            True binary labels or binary label indicators.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by :term:`decision_function` on some classifiers).\n",
      "            For :term:`decision_function` scores, values greater than or equal to\n",
      "            zero should indicate the positive class.\n",
      "        \n",
      "        average : {'micro', 'samples', 'weighted', 'macro'} or None,             default='macro'\n",
      "            If ``None``, the scores for each class are returned. Otherwise,\n",
      "            this determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=1\n",
      "            The label of the positive class. Only applied to binary ``y_true``.\n",
      "            For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        average_precision : float\n",
      "            Average precision score.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        precision_recall_curve : Compute precision-recall pairs for different\n",
      "            probability thresholds.\n",
      "        PrecisionRecallDisplay.from_estimator : Plot the precision recall curve\n",
      "            using an estimator and data.\n",
      "        PrecisionRecallDisplay.from_predictions : Plot the precision recall curve\n",
      "            using true and predicted labels.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionchanged:: 0.19\n",
      "          Instead of linearly interpolating between operating points, precisions\n",
      "          are weighted by the change in recall since the last operating point.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Average precision\n",
      "               <https://en.wikipedia.org/w/index.php?title=Information_retrieval&\n",
      "               oldid=793358396#Average_precision>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import average_precision_score\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> average_precision_score(y_true, y_scores)\n",
      "        0.83\n",
      "        >>> y_true = np.array([0, 0, 1, 1, 2, 2])\n",
      "        >>> y_scores = np.array([\n",
      "        ...     [0.7, 0.2, 0.1],\n",
      "        ...     [0.4, 0.3, 0.3],\n",
      "        ...     [0.1, 0.8, 0.1],\n",
      "        ...     [0.2, 0.3, 0.5],\n",
      "        ...     [0.4, 0.4, 0.2],\n",
      "        ...     [0.1, 0.2, 0.7],\n",
      "        ... ])\n",
      "        >>> average_precision_score(y_true, y_scores)\n",
      "        0.77\n",
      "    \n",
      "    balanced_accuracy_score(y_true, y_pred, *, sample_weight=None, adjusted=False)\n",
      "        Compute the balanced accuracy.\n",
      "        \n",
      "        The balanced accuracy in binary and multiclass classification problems to\n",
      "        deal with imbalanced datasets. It is defined as the average of recall\n",
      "        obtained on each class.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0 when ``adjusted=False``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        adjusted : bool, default=False\n",
      "            When true, the result is adjusted for chance, so that random\n",
      "            performance would score 0, while keeping perfect performance at a score\n",
      "            of 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        balanced_accuracy : float\n",
      "            Balanced accuracy score.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        average_precision_score : Compute average precision (AP) from prediction\n",
      "            scores.\n",
      "        precision_score : Compute the precision score.\n",
      "        recall_score : Compute the recall score.\n",
      "        roc_auc_score : Compute Area Under the Receiver Operating Characteristic\n",
      "            Curve (ROC AUC) from prediction scores.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Some literature promotes alternative definitions of balanced accuracy. Our\n",
      "        definition is equivalent to :func:`accuracy_score` with class-balanced\n",
      "        sample weights, and shares desirable properties with the binary case.\n",
      "        See the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).\n",
      "               The balanced accuracy and its posterior distribution.\n",
      "               Proceedings of the 20th International Conference on Pattern\n",
      "               Recognition, 3121-24.\n",
      "        .. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).\n",
      "               `Fundamentals of Machine Learning for Predictive Data Analytics:\n",
      "               Algorithms, Worked Examples, and Case Studies\n",
      "               <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import balanced_accuracy_score\n",
      "        >>> y_true = [0, 1, 0, 0, 1, 0]\n",
      "        >>> y_pred = [0, 1, 0, 0, 0, 1]\n",
      "        >>> balanced_accuracy_score(y_true, y_pred)\n",
      "        0.625\n",
      "    \n",
      "    brier_score_loss(y_true, y_proba, *, sample_weight=None, pos_label=None, labels=None, scale_by_half='auto')\n",
      "        Compute the Brier score loss.\n",
      "        \n",
      "        The smaller the Brier score loss, the better, hence the naming with \"loss\".\n",
      "        The Brier score measures the mean squared difference between the predicted\n",
      "        probability and the actual outcome. The Brier score is a strictly proper scoring\n",
      "        rule.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <brier_score_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            True targets.\n",
      "        \n",
      "        y_proba : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Predicted probabilities. If `y_proba.shape = (n_samples,)`\n",
      "            the probabilities provided are assumed to be that of the\n",
      "            positive class. If `y_proba.shape = (n_samples, n_classes)`\n",
      "            the columns in `y_proba` are assumed to correspond to the\n",
      "            labels in alphabetical order, as done by\n",
      "            :class:`~sklearn.preprocessing.LabelBinarizer`.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=None\n",
      "            Label of the positive class when `y_proba.shape = (n_samples,)`.\n",
      "            If not provided, `pos_label` will be inferred in the\n",
      "            following manner:\n",
      "        \n",
      "            * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;\n",
      "            * else if `y_true` contains string, an error will be raised and\n",
      "              `pos_label` should be explicitly specified;\n",
      "            * otherwise, `pos_label` defaults to the greater label,\n",
      "              i.e. `np.unique(y_true)[-1]`.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            Class labels when `y_proba.shape = (n_samples, n_classes)`.\n",
      "            If not provided, labels will be inferred from `y_true`.\n",
      "        \n",
      "            .. versionadded:: 1.7\n",
      "        \n",
      "        scale_by_half : bool or \"auto\", default=\"auto\"\n",
      "            When True, scale the Brier score by 1/2 to lie in the [0, 1] range instead\n",
      "            of the [0, 2] range. The default \"auto\" option implements the rescaling to\n",
      "            [0, 1] only for binary classification (as customary) but keeps the\n",
      "            original [0, 2] range for multiclass classification.\n",
      "        \n",
      "            .. versionadded:: 1.7\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            Brier score loss.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        For :math:`N` observations labeled from :math:`C` possible classes, the Brier\n",
      "        score is defined as:\n",
      "        \n",
      "        .. math::\n",
      "            \\frac{1}{N}\\sum_{i=1}^{N}\\sum_{c=1}^{C}(y_{ic} - \\hat{p}_{ic})^{2}\n",
      "        \n",
      "        where :math:`y_{ic}` is 1 if observation `i` belongs to class `c`,\n",
      "        otherwise 0 and :math:`\\hat{p}_{ic}` is the predicted probability for\n",
      "        observation `i` to belong to class `c`.\n",
      "        The Brier score then ranges between :math:`[0, 2]`.\n",
      "        \n",
      "        In binary classification tasks the Brier score is usually divided by\n",
      "        two and then ranges between :math:`[0, 1]`. It can be alternatively\n",
      "        written as:\n",
      "        \n",
      "        .. math::\n",
      "            \\frac{1}{N}\\sum_{i=1}^{N}(y_{i} - \\hat{p}_{i})^{2}\n",
      "        \n",
      "        where :math:`y_{i}` is the binary target and :math:`\\hat{p}_{i}`\n",
      "        is the predicted probability of the positive class.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Brier score\n",
      "                <https://en.wikipedia.org/wiki/Brier_score>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import brier_score_loss\n",
      "        >>> y_true = np.array([0, 1, 1, 0])\n",
      "        >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n",
      "        >>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n",
      "        >>> brier_score_loss(y_true, y_prob)\n",
      "        0.0375\n",
      "        >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)\n",
      "        0.0375\n",
      "        >>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n",
      "        0.0375\n",
      "        >>> brier_score_loss(y_true, np.array(y_prob) > 0.5)\n",
      "        0.0\n",
      "        >>> brier_score_loss(y_true, y_prob, scale_by_half=False)\n",
      "        0.075\n",
      "        >>> brier_score_loss(\n",
      "        ...    [\"eggs\", \"ham\", \"spam\"],\n",
      "        ...    [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.2, 0.2, 0.6]],\n",
      "        ...    labels=[\"eggs\", \"ham\", \"spam\"]\n",
      "        ... )\n",
      "        0.146\n",
      "    \n",
      "    calinski_harabasz_score(X, labels)\n",
      "        Compute the Calinski and Harabasz score.\n",
      "        \n",
      "        It is also known as the Variance Ratio Criterion.\n",
      "        \n",
      "        The score is defined as ratio of the sum of between-cluster dispersion and\n",
      "        of within-cluster dispersion.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <calinski_harabasz_index>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            A list of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The resulting Calinski-Harabasz score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `T. Calinski and J. Harabasz, 1974. \"A dendrite method for cluster\n",
      "           analysis\". Communications in Statistics\n",
      "           <https://www.tandfonline.com/doi/abs/10.1080/03610927408827101>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets import make_blobs\n",
      "        >>> from sklearn.cluster import KMeans\n",
      "        >>> from sklearn.metrics import calinski_harabasz_score\n",
      "        >>> X, _ = make_blobs(random_state=0)\n",
      "        >>> kmeans = KMeans(n_clusters=3, random_state=0,).fit(X)\n",
      "        >>> calinski_harabasz_score(X, kmeans.labels_)\n",
      "        114.8...\n",
      "    \n",
      "    check_scoring(estimator=None, scoring=None, *, allow_none=False, raise_exc=True)\n",
      "        Determine scorer from user options.\n",
      "        \n",
      "        A TypeError will be thrown if the estimator cannot be scored.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit' or None, default=None\n",
      "            The object to use to fit the data. If `None`, then this function may error\n",
      "            depending on `allow_none`.\n",
      "        \n",
      "        scoring : str, callable, list, tuple, set, or dict, default=None\n",
      "            Scorer to use. If `scoring` represents a single score, one can use:\n",
      "        \n",
      "            - a single string (see :ref:`scoring_string_names`);\n",
      "            - a callable (see :ref:`scoring_callable`) that returns a single value;\n",
      "            - `None`, the `estimator`'s\n",
      "              :ref:`default evaluation criterion <scoring_api_overview>` is used.\n",
      "        \n",
      "            If `scoring` represents multiple scores, one can use:\n",
      "        \n",
      "            - a list, tuple or set of unique strings;\n",
      "            - a callable returning a dictionary where the keys are the metric names and the\n",
      "              values are the metric scorers;\n",
      "            - a dictionary with metric names as keys and callables a values. The callables\n",
      "              need to have the signature `callable(estimator, X, y)`.\n",
      "        \n",
      "        allow_none : bool, default=False\n",
      "            Whether to return None or raise an error if no `scoring` is specified and the\n",
      "            estimator has no `score` method.\n",
      "        \n",
      "        raise_exc : bool, default=True\n",
      "            Whether to raise an exception (if a subset of the scorers in multimetric scoring\n",
      "            fails) or to return an error code.\n",
      "        \n",
      "            - If set to `True`, raises the failing scorer's exception.\n",
      "            - If set to `False`, a formatted string of the exception details is passed as\n",
      "              result of the failing scorer(s).\n",
      "        \n",
      "            This applies if `scoring` is list, tuple, set, or dict. Ignored if `scoring` is\n",
      "            a str or a callable.\n",
      "        \n",
      "            .. versionadded:: 1.6\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scoring : callable\n",
      "            A scorer callable object / function with signature ``scorer(estimator, X, y)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets import load_iris\n",
      "        >>> from sklearn.metrics import check_scoring\n",
      "        >>> from sklearn.tree import DecisionTreeClassifier\n",
      "        >>> X, y = load_iris(return_X_y=True)\n",
      "        >>> classifier = DecisionTreeClassifier(max_depth=2).fit(X, y)\n",
      "        >>> scorer = check_scoring(classifier, scoring='accuracy')\n",
      "        >>> scorer(classifier, X, y)\n",
      "        0.96...\n",
      "        \n",
      "        >>> from sklearn.metrics import make_scorer, accuracy_score, mean_squared_log_error\n",
      "        >>> X, y = load_iris(return_X_y=True)\n",
      "        >>> y *= -1\n",
      "        >>> clf = DecisionTreeClassifier().fit(X, y)\n",
      "        >>> scoring = {\n",
      "        ...     \"accuracy\": make_scorer(accuracy_score),\n",
      "        ...     \"mean_squared_log_error\": make_scorer(mean_squared_log_error),\n",
      "        ... }\n",
      "        >>> scoring_call = check_scoring(estimator=clf, scoring=scoring, raise_exc=False)\n",
      "        >>> scores = scoring_call(clf, X, y)\n",
      "        >>> scores\n",
      "        {'accuracy': 1.0, 'mean_squared_log_error': 'Traceback ...'}\n",
      "    \n",
      "    class_likelihood_ratios(y_true, y_pred, *, labels=None, sample_weight=None, raise_warning='deprecated', replace_undefined_by=nan)\n",
      "        Compute binary classification positive and negative likelihood ratios.\n",
      "        \n",
      "        The positive likelihood ratio is `LR+ = sensitivity / (1 - specificity)`\n",
      "        where the sensitivity or recall is the ratio `tp / (tp + fn)` and the\n",
      "        specificity is `tn / (tn + fp)`. The negative likelihood ratio is `LR- = (1\n",
      "        - sensitivity) / specificity`. Here `tp` is the number of true positives,\n",
      "        `fp` the number of false positives, `tn` is the number of true negatives and\n",
      "        `fn` the number of false negatives. Both class likelihood ratios can be used\n",
      "        to obtain post-test probabilities given a pre-test probability.\n",
      "        \n",
      "        `LR+` ranges from 1.0 to infinity. A `LR+` of 1.0 indicates that the probability\n",
      "        of predicting the positive class is the same for samples belonging to either\n",
      "        class; therefore, the test is useless. The greater `LR+` is, the more a\n",
      "        positive prediction is likely to be a true positive when compared with the\n",
      "        pre-test probability. A value of `LR+` lower than 1.0 is invalid as it would\n",
      "        indicate that the odds of a sample being a true positive decrease with\n",
      "        respect to the pre-test odds.\n",
      "        \n",
      "        `LR-` ranges from 0.0 to 1.0. The closer it is to 0.0, the lower the probability\n",
      "        of a given sample to be a false negative. A `LR-` of 1.0 means the test is\n",
      "        useless because the odds of having the condition did not change after the\n",
      "        test. A value of `LR-` greater than 1.0 invalidates the classifier as it\n",
      "        indicates an increase in the odds of a sample belonging to the positive\n",
      "        class after being classified as negative. This is the case when the\n",
      "        classifier systematically predicts the opposite of the true label.\n",
      "        \n",
      "        A typical application in medicine is to identify the positive/negative class\n",
      "        to the presence/absence of a disease, respectively; the classifier being a\n",
      "        diagnostic test; the pre-test probability of an individual having the\n",
      "        disease can be the prevalence of such disease (proportion of a particular\n",
      "        population found to be affected by a medical condition); and the post-test\n",
      "        probabilities would be the probability that the condition is truly present\n",
      "        given a positive test result.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <class_likelihood_ratios>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            List of labels to index the matrix. This may be used to select the\n",
      "            positive and negative classes with the ordering `labels=[negative_class,\n",
      "            positive_class]`. If `None` is given, those that appear at least once in\n",
      "            `y_true` or `y_pred` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        raise_warning : bool, default=True\n",
      "            Whether or not a case-specific warning message is raised when there is division\n",
      "            by zero.\n",
      "        \n",
      "            .. deprecated:: 1.7\n",
      "                `raise_warning` was deprecated in version 1.7 and will be removed in 1.9,\n",
      "                when an :class:`~sklearn.exceptions.UndefinedMetricWarning` will always\n",
      "                raise in case of a division by zero.\n",
      "        \n",
      "        replace_undefined_by : np.nan, 1.0, or dict, default=np.nan\n",
      "            Sets the return values for LR+ and LR- when there is a division by zero. Can\n",
      "            take the following values:\n",
      "        \n",
      "            - `np.nan` to return `np.nan` for both `LR+` and `LR-`\n",
      "            - `1.0` to return the worst possible scores: `{\"LR+\": 1.0, \"LR-\": 1.0}`\n",
      "            - a dict in the format `{\"LR+\": value_1, \"LR-\": value_2}` where the values can\n",
      "              be non-negative floats, `np.inf` or `np.nan` in the range of the\n",
      "              likelihood ratios. For example, `{\"LR+\": 1.0, \"LR-\": 1.0}` can be used for\n",
      "              returning the worst scores, indicating a useless model, and `{\"LR+\": np.inf,\n",
      "              \"LR-\": 0.0}` can be used for returning the best scores, indicating a useful\n",
      "              model.\n",
      "        \n",
      "            If a division by zero occurs, only the affected metric is replaced with the set\n",
      "            value; the other metric is calculated as usual.\n",
      "        \n",
      "            .. versionadded:: 1.7\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        (positive_likelihood_ratio, negative_likelihood_ratio) : tuple\n",
      "            A tuple of two floats, the first containing the positive likelihood ratio (LR+)\n",
      "            and the second the negative likelihood ratio (LR-).\n",
      "        \n",
      "        Warns\n",
      "        -----\n",
      "        Raises :class:`~sklearn.exceptions.UndefinedMetricWarning` when `y_true` and\n",
      "        `y_pred` lead to the following conditions:\n",
      "        \n",
      "            - The number of false positives is 0 and `raise_warning` is set to `True`\n",
      "              (default): positive likelihood ratio is undefined.\n",
      "            - The number of true negatives is 0 and `raise_warning` is set to `True`\n",
      "              (default): negative likelihood ratio is undefined.\n",
      "            - The sum of true positives and false negatives is 0 (no samples of the positive\n",
      "              class are present in `y_true`): both likelihood ratios are undefined.\n",
      "        \n",
      "            For the first two cases, an undefined metric can be defined by setting the\n",
      "            `replace_undefined_by` param.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Likelihood ratios in diagnostic testing\n",
      "               <https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import class_likelihood_ratios\n",
      "        >>> class_likelihood_ratios([0, 1, 0, 1, 0], [1, 1, 0, 0, 0])\n",
      "        (1.5, 0.75)\n",
      "        >>> y_true = np.array([\"non-cat\", \"cat\", \"non-cat\", \"cat\", \"non-cat\"])\n",
      "        >>> y_pred = np.array([\"cat\", \"cat\", \"non-cat\", \"non-cat\", \"non-cat\"])\n",
      "        >>> class_likelihood_ratios(y_true, y_pred)\n",
      "        (1.33, 0.66)\n",
      "        >>> y_true = np.array([\"non-zebra\", \"zebra\", \"non-zebra\", \"zebra\", \"non-zebra\"])\n",
      "        >>> y_pred = np.array([\"zebra\", \"zebra\", \"non-zebra\", \"non-zebra\", \"non-zebra\"])\n",
      "        >>> class_likelihood_ratios(y_true, y_pred)\n",
      "        (1.5, 0.75)\n",
      "        \n",
      "        To avoid ambiguities, use the notation `labels=[negative_class,\n",
      "        positive_class]`\n",
      "        \n",
      "        >>> y_true = np.array([\"non-cat\", \"cat\", \"non-cat\", \"cat\", \"non-cat\"])\n",
      "        >>> y_pred = np.array([\"cat\", \"cat\", \"non-cat\", \"non-cat\", \"non-cat\"])\n",
      "        >>> class_likelihood_ratios(y_true, y_pred, labels=[\"non-cat\", \"cat\"])\n",
      "        (1.5, 0.75)\n",
      "    \n",
      "    classification_report(y_true, y_pred, *, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn')\n",
      "        Build a text report showing the main classification metrics.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <classification_report>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_labels,), default=None\n",
      "            Optional list of label indices to include in the report.\n",
      "        \n",
      "        target_names : array-like of shape (n_labels,), default=None\n",
      "            Optional display names matching the labels (same order).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        digits : int, default=2\n",
      "            Number of digits for formatting output floating point values.\n",
      "            When ``output_dict`` is ``True``, this will be ignored and the\n",
      "            returned values will not be rounded.\n",
      "        \n",
      "        output_dict : bool, default=False\n",
      "            If True, return output as dict.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division. If set to\n",
      "            \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "               `np.nan` option was added.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        report : str or dict\n",
      "            Text summary of the precision, recall, F1 score for each class.\n",
      "            Dictionary returned if output_dict is True. Dictionary has the\n",
      "            following structure::\n",
      "        \n",
      "                {'label 1': {'precision':0.5,\n",
      "                             'recall':1.0,\n",
      "                             'f1-score':0.67,\n",
      "                             'support':1},\n",
      "                 'label 2': { ... },\n",
      "                  ...\n",
      "                }\n",
      "        \n",
      "            The reported averages include macro average (averaging the unweighted\n",
      "            mean per label), weighted average (averaging the support-weighted mean\n",
      "            per label), and sample average (only for multilabel classification).\n",
      "            Micro average (averaging the total true positives, false negatives and\n",
      "            false positives) is only shown for multi-label or multi-class\n",
      "            with a subset of classes, because it corresponds to accuracy\n",
      "            otherwise and would be the same for all metrics.\n",
      "            See also :func:`precision_recall_fscore_support` for more details\n",
      "            on averages.\n",
      "        \n",
      "            Note that in binary classification, recall of the positive class\n",
      "            is also known as \"sensitivity\"; recall of the negative class is\n",
      "            \"specificity\".\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support: Compute precision, recall, F-measure and\n",
      "            support for each class.\n",
      "        confusion_matrix: Compute confusion matrix to evaluate the accuracy of a\n",
      "            classification.\n",
      "        multilabel_confusion_matrix: Compute a confusion matrix for each class or sample.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import classification_report\n",
      "        >>> y_true = [0, 1, 2, 2, 2]\n",
      "        >>> y_pred = [0, 0, 2, 2, 1]\n",
      "        >>> target_names = ['class 0', 'class 1', 'class 2']\n",
      "        >>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "             class 0       0.50      1.00      0.67         1\n",
      "             class 1       0.00      0.00      0.00         1\n",
      "             class 2       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "            accuracy                           0.60         5\n",
      "           macro avg       0.50      0.56      0.49         5\n",
      "        weighted avg       0.70      0.60      0.61         5\n",
      "        <BLANKLINE>\n",
      "        >>> y_pred = [1, 1, 0]\n",
      "        >>> y_true = [1, 1, 1]\n",
      "        >>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "                   1       1.00      0.67      0.80         3\n",
      "                   2       0.00      0.00      0.00         0\n",
      "                   3       0.00      0.00      0.00         0\n",
      "        <BLANKLINE>\n",
      "           micro avg       1.00      0.67      0.80         3\n",
      "           macro avg       0.33      0.22      0.27         3\n",
      "        weighted avg       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "    \n",
      "    cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None)\n",
      "        Compute Cohen's kappa: a statistic that measures inter-annotator agreement.\n",
      "        \n",
      "        This function computes Cohen's kappa [1]_, a score that expresses the level\n",
      "        of agreement between two annotators on a classification problem. It is\n",
      "        defined as\n",
      "        \n",
      "        .. math::\n",
      "            \\kappa = (p_o - p_e) / (1 - p_e)\n",
      "        \n",
      "        where :math:`p_o` is the empirical probability of agreement on the label\n",
      "        assigned to any sample (the observed agreement ratio), and :math:`p_e` is\n",
      "        the expected agreement when both annotators assign labels randomly.\n",
      "        :math:`p_e` is estimated using a per-annotator empirical prior over the\n",
      "        class labels [2]_.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cohen_kappa>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y1 : array-like of shape (n_samples,)\n",
      "            Labels assigned by the first annotator.\n",
      "        \n",
      "        y2 : array-like of shape (n_samples,)\n",
      "            Labels assigned by the second annotator. The kappa statistic is\n",
      "            symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            List of labels to index the matrix. This may be used to select a\n",
      "            subset of labels. If `None`, all labels that appear at least once in\n",
      "            ``y1`` or ``y2`` are used. Note that at least one label in `labels` must be\n",
      "            present in `y1`, even though this function is otherwise agnostic to the order\n",
      "            of `y1` and `y2`.\n",
      "        \n",
      "        weights : {'linear', 'quadratic'}, default=None\n",
      "            Weighting type to calculate the score. `None` means not weighted;\n",
      "            \"linear\" means linear weighting; \"quadratic\" means quadratic weighting.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kappa : float\n",
      "            The kappa statistic, which is a number between -1 and 1. The maximum\n",
      "            value means complete agreement; zero or lower means chance agreement.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] :doi:`J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n",
      "               Educational and Psychological Measurement 20(1):37-46.\n",
      "               <10.1177/001316446002000104>`\n",
      "        .. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n",
      "               computational linguistics\". Computational Linguistics 34(4):555-596\n",
      "               <https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2>`_.\n",
      "        .. [3] `Wikipedia entry for the Cohen's kappa\n",
      "                <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import cohen_kappa_score\n",
      "        >>> y1 = [\"negative\", \"positive\", \"negative\", \"neutral\", \"positive\"]\n",
      "        >>> y2 = [\"negative\", \"positive\", \"negative\", \"neutral\", \"negative\"]\n",
      "        >>> cohen_kappa_score(y1, y2)\n",
      "        0.6875\n",
      "    \n",
      "    completeness_score(labels_true, labels_pred)\n",
      "        Compute completeness metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`homogeneity_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,)\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        completeness : float\n",
      "           Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score : Homogeneity metric of cluster labeling.\n",
      "        v_measure_score : V-Measure (NMI with arithmetic mean option).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are complete::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import completeness_score\n",
      "          >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that assign all classes members to the same clusters\n",
      "        are still complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          1.0\n",
      "          >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          0.999\n",
      "        \n",
      "        If classes members are split across different clusters, the\n",
      "        assignment cannot be complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          0.0\n",
      "          >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          0.0\n",
      "    \n",
      "    confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n",
      "        Compute confusion matrix to evaluate the accuracy of a classification.\n",
      "        \n",
      "        By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n",
      "        is equal to the number of observations known to be in group :math:`i` and\n",
      "        predicted to be in group :math:`j`.\n",
      "        \n",
      "        Thus in binary classification, the count of true negatives is\n",
      "        :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n",
      "        :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_classes), default=None\n",
      "            List of labels to index the matrix. This may be used to reorder\n",
      "            or select a subset of labels.\n",
      "            If ``None`` is given, those that appear at least once\n",
      "            in ``y_true`` or ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        normalize : {'true', 'pred', 'all'}, default=None\n",
      "            Normalizes confusion matrix over the true (rows), predicted (columns)\n",
      "            conditions or all the population. If None, confusion matrix will not be\n",
      "            normalized.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : ndarray of shape (n_classes, n_classes)\n",
      "            Confusion matrix whose i-th row and j-th\n",
      "            column entry indicates the number of\n",
      "            samples with true label being i-th class\n",
      "            and predicted label being j-th class.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "            given an estimator, the data, and the label.\n",
      "        ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "            given the true and predicted labels.\n",
      "        ConfusionMatrixDisplay : Confusion Matrix visualization.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Confusion matrix\n",
      "               <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n",
      "               (Wikipedia and other references may use a different\n",
      "               convention for axes).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import confusion_matrix\n",
      "        >>> y_true = [2, 0, 2, 2, 0, 1]\n",
      "        >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
      "        >>> confusion_matrix(y_true, y_pred)\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        In the binary case, we can extract true positives, etc. as follows:\n",
      "        \n",
      "        >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel().tolist()\n",
      "        >>> (tn, fp, fn, tp)\n",
      "        (0, 2, 1, 1)\n",
      "    \n",
      "    consensus_score(a, b, *, similarity='jaccard')\n",
      "        The similarity of two sets of biclusters.\n",
      "        \n",
      "        Similarity between individual biclusters is computed. Then the best\n",
      "        matching between sets is found by solving a linear sum assignment problem,\n",
      "        using a modified Jonker-Volgenant algorithm.\n",
      "        The final score is the sum of similarities divided by the size of\n",
      "        the larger set.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <biclustering>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : tuple (rows, columns)\n",
      "            Tuple of row and column indicators for a set of biclusters.\n",
      "        \n",
      "        b : tuple (rows, columns)\n",
      "            Another set of biclusters like ``a``.\n",
      "        \n",
      "        similarity : 'jaccard' or callable, default='jaccard'\n",
      "            May be the string \"jaccard\" to use the Jaccard coefficient, or\n",
      "            any function that takes four arguments, each of which is a 1d\n",
      "            indicator vector: (a_rows, a_columns, b_rows, b_columns).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        consensus_score : float\n",
      "           Consensus score, a non-negative value, sum of similarities\n",
      "           divided by size of larger set.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.optimize.linear_sum_assignment : Solve the linear sum assignment problem.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis\n",
      "          for bicluster acquisition\n",
      "          <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import consensus_score\n",
      "        >>> a = ([[True, False], [False, True]], [[False, True], [True, False]])\n",
      "        >>> b = ([[False, True], [True, False]], [[True, False], [False, True]])\n",
      "        >>> consensus_score(a, b, similarity='jaccard')\n",
      "        1.0\n",
      "    \n",
      "    coverage_error(y_true, y_score, *, sample_weight=None)\n",
      "        Coverage error measure.\n",
      "        \n",
      "        Compute how far we need to go through the ranked scores to cover all\n",
      "        true labels. The best value is equal to the average number\n",
      "        of labels in ``y_true`` per sample.\n",
      "        \n",
      "        Ties in ``y_scores`` are broken by giving maximal rank that would have\n",
      "        been assigned to all tied values.\n",
      "        \n",
      "        Note: Our implementation's score is 1 greater than the one given in\n",
      "        Tsoumakas et al., 2010. This extends it to handle the degenerate case\n",
      "        in which an instance has 0 true labels.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <coverage_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "            For :term:`decision_function` scores, values greater than or equal to\n",
      "            zero should indicate the positive class.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coverage_error : float\n",
      "            The coverage error.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import coverage_error\n",
      "        >>> y_true = [[1, 0, 0], [0, 1, 1]]\n",
      "        >>> y_score = [[1, 0, 0], [0, 1, 1]]\n",
      "        >>> coverage_error(y_true, y_score)\n",
      "        1.5\n",
      "    \n",
      "    d2_absolute_error_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        :math:`D^2` regression score function, fraction of absolute error explained.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the model can be\n",
      "        arbitrarily worse). A model that always uses the empirical median of `y_true`\n",
      "        as constant prediction, disregarding the input features,\n",
      "        gets a :math:`D^2` score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <d2_score>`.\n",
      "        \n",
      "        .. versionadded:: 1.1\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average scores.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of floats\n",
      "            The :math:`D^2` score with an absolute error deviance\n",
      "            or ndarray of scores if 'multioutput' is 'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Like :math:`R^2`, :math:`D^2` score may be negative\n",
      "        (it need not actually be the square of a quantity D).\n",
      "        \n",
      "        This metric is not well-defined for single samples and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "         References\n",
      "        ----------\n",
      "        .. [1] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.\n",
      "               Wainwright. \"Statistical Learning with Sparsity: The Lasso and\n",
      "               Generalizations.\" (2015). https://hastie.su.domains/StatLearnSparsity/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import d2_absolute_error_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> d2_absolute_error_score(y_true, y_pred)\n",
      "        0.764...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> d2_absolute_error_score(y_true, y_pred, multioutput='uniform_average')\n",
      "        0.691...\n",
      "        >>> d2_absolute_error_score(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.8125    , 0.57142857])\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [1, 2, 3]\n",
      "        >>> d2_absolute_error_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [2, 2, 2]\n",
      "        >>> d2_absolute_error_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [3, 2, 1]\n",
      "        >>> d2_absolute_error_score(y_true, y_pred)\n",
      "        -1.0\n",
      "    \n",
      "    d2_log_loss_score(y_true, y_pred, *, sample_weight=None, labels=None)\n",
      "        :math:`D^2` score function, fraction of log loss explained.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the model can be\n",
      "        arbitrarily worse). A model that always predicts the per-class proportions\n",
      "        of `y_true`, disregarding the input features, gets a D^2 score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <d2_score_classification>`.\n",
      "        \n",
      "        .. versionadded:: 1.5\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like or label indicator matrix\n",
      "            The actuals labels for the n_samples samples.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples, n_classes) or (n_samples,)\n",
      "            Predicted probabilities, as returned by a classifier's\n",
      "            predict_proba method. If ``y_pred.shape = (n_samples,)``\n",
      "            the probabilities provided are assumed to be that of the\n",
      "            positive class. The labels in ``y_pred`` are assumed to be\n",
      "            ordered alphabetically, as done by\n",
      "            :class:`~sklearn.preprocessing.LabelBinarizer`.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            If not provided, labels will be inferred from y_true. If ``labels``\n",
      "            is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n",
      "            assumed to be binary and are inferred from ``y_true``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        d2 : float or ndarray of floats\n",
      "            The D^2 score.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Like R^2, D^2 score may be negative (it need not actually be the square of\n",
      "        a quantity D).\n",
      "        \n",
      "        This metric is not well-defined for a single sample and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "    \n",
      "    d2_pinball_score(y_true, y_pred, *, sample_weight=None, alpha=0.5, multioutput='uniform_average')\n",
      "        :math:`D^2` regression score function, fraction of pinball loss explained.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the model can be\n",
      "        arbitrarily worse). A model that always uses the empirical alpha-quantile of\n",
      "        `y_true` as constant prediction, disregarding the input features,\n",
      "        gets a :math:`D^2` score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <d2_score>`.\n",
      "        \n",
      "        .. versionadded:: 1.1\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        alpha : float, default=0.5\n",
      "            Slope of the pinball deviance. It determines the quantile level alpha\n",
      "            for which the pinball deviance and also D2 are optimal.\n",
      "            The default `alpha=0.5` is equivalent to `d2_absolute_error_score`.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average scores.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of floats\n",
      "            The :math:`D^2` score with a pinball deviance\n",
      "            or ndarray of scores if `multioutput='raw_values'`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Like :math:`R^2`, :math:`D^2` score may be negative\n",
      "        (it need not actually be the square of a quantity D).\n",
      "        \n",
      "        This metric is not well-defined for a single point and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "         References\n",
      "        ----------\n",
      "        .. [1] Eq. (7) of `Koenker, Roger; Machado, José A. F. (1999).\n",
      "               \"Goodness of Fit and Related Inference Processes for Quantile Regression\"\n",
      "               <https://doi.org/10.1080/01621459.1999.10473882>`_\n",
      "        .. [2] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.\n",
      "               Wainwright. \"Statistical Learning with Sparsity: The Lasso and\n",
      "               Generalizations.\" (2015). https://hastie.su.domains/StatLearnSparsity/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import d2_pinball_score\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [1, 3, 3]\n",
      "        >>> d2_pinball_score(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> d2_pinball_score(y_true, y_pred, alpha=0.9)\n",
      "        0.772...\n",
      "        >>> d2_pinball_score(y_true, y_pred, alpha=0.1)\n",
      "        -1.045...\n",
      "        >>> d2_pinball_score(y_true, y_true, alpha=0.1)\n",
      "        1.0\n",
      "    \n",
      "    d2_tweedie_score(y_true, y_pred, *, sample_weight=None, power=0)\n",
      "        :math:`D^2` regression score function, fraction of Tweedie deviance explained.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the model can be\n",
      "        arbitrarily worse). A model that always uses the empirical mean of `y_true` as\n",
      "        constant prediction, disregarding the input features, gets a D^2 score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <d2_score>`.\n",
      "        \n",
      "        .. versionadded:: 1.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        power : float, default=0\n",
      "            Tweedie power parameter. Either power <= 0 or power >= 1.\n",
      "        \n",
      "            The higher `p` the less weight is given to extreme\n",
      "            deviations between true and predicted targets.\n",
      "        \n",
      "            - power < 0: Extreme stable distribution. Requires: y_pred > 0.\n",
      "            - power = 0 : Normal distribution, output corresponds to r2_score.\n",
      "              y_true and y_pred can be any real numbers.\n",
      "            - power = 1 : Poisson distribution. Requires: y_true >= 0 and\n",
      "              y_pred > 0.\n",
      "            - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\n",
      "              and y_pred > 0.\n",
      "            - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\n",
      "            - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "            - otherwise : Positive stable distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : float\n",
      "            The D^2 score.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Like R^2, D^2 score may be negative (it need not actually be the square of\n",
      "        a quantity D).\n",
      "        \n",
      "        This metric is not well-defined for single samples and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Eq. (3.11) of Hastie, Trevor J., Robert Tibshirani and Martin J.\n",
      "               Wainwright. \"Statistical Learning with Sparsity: The Lasso and\n",
      "               Generalizations.\" (2015). https://hastie.su.domains/StatLearnSparsity/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import d2_tweedie_score\n",
      "        >>> y_true = [0.5, 1, 2.5, 7]\n",
      "        >>> y_pred = [1, 1, 5, 3.5]\n",
      "        >>> d2_tweedie_score(y_true, y_pred)\n",
      "        0.285...\n",
      "        >>> d2_tweedie_score(y_true, y_pred, power=1)\n",
      "        0.487...\n",
      "        >>> d2_tweedie_score(y_true, y_pred, power=2)\n",
      "        0.630...\n",
      "        >>> d2_tweedie_score(y_true, y_true, power=2)\n",
      "        1.0\n",
      "    \n",
      "    davies_bouldin_score(X, labels)\n",
      "        Compute the Davies-Bouldin score.\n",
      "        \n",
      "        The score is defined as the average similarity measure of each cluster with\n",
      "        its most similar cluster, where similarity is the ratio of within-cluster\n",
      "        distances to between-cluster distances. Thus, clusters which are farther\n",
      "        apart and less dispersed will result in a better score.\n",
      "        \n",
      "        The minimum score is zero, with lower values indicating better clustering.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <davies-bouldin_index>`.\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            A list of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score: float\n",
      "            The resulting Davies-Bouldin score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Davies, David L.; Bouldin, Donald W. (1979).\n",
      "           `\"A Cluster Separation Measure\"\n",
      "           <https://ieeexplore.ieee.org/document/4766909>`__.\n",
      "           IEEE Transactions on Pattern Analysis and Machine Intelligence.\n",
      "           PAMI-1 (2): 224-227\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import davies_bouldin_score\n",
      "        >>> X = [[0, 1], [1, 1], [3, 4]]\n",
      "        >>> labels = [0, 0, 1]\n",
      "        >>> davies_bouldin_score(X, labels)\n",
      "        0.12...\n",
      "    \n",
      "    dcg_score(y_true, y_score, *, k=None, log_base=2, sample_weight=None, ignore_ties=False)\n",
      "        Compute Discounted Cumulative Gain.\n",
      "        \n",
      "        Sum the true scores ranked in the order induced by the predicted scores,\n",
      "        after applying a logarithmic discount.\n",
      "        \n",
      "        This ranking metric yields a high value if true labels are ranked high by\n",
      "        ``y_score``.\n",
      "        \n",
      "        Usually the Normalized Discounted Cumulative Gain (NDCG, computed by\n",
      "        ndcg_score) is preferred.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples, n_labels)\n",
      "            True targets of multilabel classification, or true scores of entities\n",
      "            to be ranked.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates, confidence values,\n",
      "            or non-thresholded measure of decisions (as returned by\n",
      "            \"decision_function\" on some classifiers).\n",
      "        \n",
      "        k : int, default=None\n",
      "            Only consider the highest k scores in the ranking. If None, use all\n",
      "            outputs.\n",
      "        \n",
      "        log_base : float, default=2\n",
      "            Base of the logarithm used for the discount. A low value means a\n",
      "            sharper discount (top results are more important).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights. If `None`, all samples are given the same weight.\n",
      "        \n",
      "        ignore_ties : bool, default=False\n",
      "            Assume that there are no ties in y_score (which is likely to be the\n",
      "            case if y_score is continuous) for efficiency gains.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        discounted_cumulative_gain : float\n",
      "            The averaged sample DCG scores.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ndcg_score : The Discounted Cumulative Gain divided by the Ideal Discounted\n",
      "            Cumulative Gain (the DCG obtained for a perfect ranking), in order to\n",
      "            have a score between 0 and 1.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        `Wikipedia entry for Discounted Cumulative Gain\n",
      "        <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_.\n",
      "        \n",
      "        Jarvelin, K., & Kekalainen, J. (2002).\n",
      "        Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n",
      "        Information Systems (TOIS), 20(4), 422-446.\n",
      "        \n",
      "        Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n",
      "        A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n",
      "        Annual Conference on Learning Theory (COLT 2013).\n",
      "        \n",
      "        McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n",
      "        performance measures efficiently in the presence of tied scores. In\n",
      "        European conference on information retrieval (pp. 414-421). Springer,\n",
      "        Berlin, Heidelberg.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import dcg_score\n",
      "        >>> # we have ground-truth relevance of some answers to a query:\n",
      "        >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n",
      "        >>> # we predict scores for the answers\n",
      "        >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n",
      "        >>> dcg_score(true_relevance, scores)\n",
      "        9.49\n",
      "        >>> # we can set k to truncate the sum; only top k answers contribute\n",
      "        >>> dcg_score(true_relevance, scores, k=2)\n",
      "        5.63\n",
      "        >>> # now we have some ties in our prediction\n",
      "        >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n",
      "        >>> # by default ties are averaged, so here we get the average true\n",
      "        >>> # relevance of our top predictions: (10 + 5) / 2 = 7.5\n",
      "        >>> dcg_score(true_relevance, scores, k=1)\n",
      "        7.5\n",
      "        >>> # we can choose to ignore ties for faster results, but only\n",
      "        >>> # if we know there aren't ties in our scores, otherwise we get\n",
      "        >>> # wrong results:\n",
      "        >>> dcg_score(true_relevance,\n",
      "        ...           scores, k=1, ignore_ties=True)\n",
      "        5.0\n",
      "    \n",
      "    det_curve(y_true, y_score, pos_label=None, sample_weight=None, drop_intermediate=False)\n",
      "        Compute Detection Error Tradeoff (DET) for different probability thresholds.\n",
      "        \n",
      "        .. note::\n",
      "           This metric is used for evaluation of ranking and error tradeoffs of\n",
      "           a binary classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <det_curve>`.\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        .. versionchanged:: 1.7\n",
      "           An arbitrary threshold at infinity is added to represent a classifier\n",
      "           that always predicts the negative class, i.e. `fpr=0` and `fnr=1`, unless\n",
      "           `fpr=0` is already reached at a finite threshold.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : ndarray of shape of (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "            For :term:`decision_function` scores, values greater than or equal to\n",
      "            zero should indicate the positive class.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        drop_intermediate : bool, default=False\n",
      "            Whether to drop thresholds where true positives (tp) do not change from\n",
      "            the previous or subsequent threshold. All points with the same tp value\n",
      "            have the same `fnr` and thus same y coordinate.\n",
      "        \n",
      "            .. versionadded:: 1.7\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fpr : ndarray of shape (n_thresholds,)\n",
      "            False positive rate (FPR) such that element i is the false positive\n",
      "            rate of predictions with score >= thresholds[i]. This is occasionally\n",
      "            referred to as false acceptance probability or fall-out.\n",
      "        \n",
      "        fnr : ndarray of shape (n_thresholds,)\n",
      "            False negative rate (FNR) such that element i is the false negative\n",
      "            rate of predictions with score >= thresholds[i]. This is occasionally\n",
      "            referred to as false rejection or miss rate.\n",
      "        \n",
      "        thresholds : ndarray of shape (n_thresholds,)\n",
      "            Decreasing thresholds on the decision function (either `predict_proba`\n",
      "            or `decision_function`) used to compute FPR and FNR.\n",
      "        \n",
      "            .. versionchanged:: 1.7\n",
      "               An arbitrary threshold at infinity is added for the case `fpr=0`\n",
      "               and `fnr=1`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        DetCurveDisplay.from_estimator : Plot DET curve given an estimator and\n",
      "            some data.\n",
      "        DetCurveDisplay.from_predictions : Plot DET curve given the true and\n",
      "            predicted labels.\n",
      "        DetCurveDisplay : DET curve visualization.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        precision_recall_curve : Compute precision-recall curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import det_curve\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, fnr, thresholds = det_curve(y_true, y_scores)\n",
      "        >>> fpr\n",
      "        array([0.5, 0.5, 0. ])\n",
      "        >>> fnr\n",
      "        array([0. , 0.5, 0.5])\n",
      "        >>> thresholds\n",
      "        array([0.35, 0.4 , 0.8 ])\n",
      "    \n",
      "    euclidean_distances(X, Y=None, *, Y_norm_squared=None, squared=False, X_norm_squared=None)\n",
      "        Compute the distance matrix between each pair from a feature array X and Y.\n",
      "        \n",
      "        For efficiency reasons, the euclidean distance between a pair of row\n",
      "        vector x and y is computed as::\n",
      "        \n",
      "            dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n",
      "        \n",
      "        This formulation has two advantages over other ways of computing distances.\n",
      "        First, it is computationally efficient when dealing with sparse data.\n",
      "        Second, if one argument varies but the other remains unchanged, then\n",
      "        `dot(x, x)` and/or `dot(y, y)` can be pre-computed.\n",
      "        \n",
      "        However, this is not the most precise way of doing this computation,\n",
      "        because this equation potentially suffers from \"catastrophic cancellation\".\n",
      "        Also, the distance matrix returned by this function may not be exactly\n",
      "        symmetric as required by, e.g., :mod:`scipy.spatial.distance` functions.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "            If `None`, method uses `Y=X`.\n",
      "        \n",
      "        Y_norm_squared : array-like of shape (n_samples_Y,) or (n_samples_Y, 1)             or (1, n_samples_Y), default=None\n",
      "            Pre-computed dot-products of vectors in Y (e.g.,\n",
      "            ``(Y**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        squared : bool, default=False\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        X_norm_squared : array-like of shape (n_samples_X,) or (n_samples_X, 1)             or (1, n_samples_X), default=None\n",
      "            Pre-computed dot-products of vectors in X (e.g.,\n",
      "            ``(X**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "            Returns the distances between the row vectors of `X`\n",
      "            and the row vectors of `Y`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        paired_distances : Distances between pairs of elements of X and Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        To achieve a better accuracy, `X_norm_squared` and `Y_norm_squared` may be\n",
      "        unused if they are passed as `np.float32`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import euclidean_distances\n",
      "        >>> X = [[0, 1], [1, 1]]\n",
      "        >>> # distance between rows of X\n",
      "        >>> euclidean_distances(X, X)\n",
      "        array([[0., 1.],\n",
      "               [1., 0.]])\n",
      "        >>> # get distance to origin\n",
      "        >>> euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "    \n",
      "    explained_variance_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', force_finite=True)\n",
      "        Explained variance regression score function.\n",
      "        \n",
      "        Best possible score is 1.0, lower values are worse.\n",
      "        \n",
      "        In the particular case when ``y_true`` is constant, the explained variance\n",
      "        score is not finite: it is either ``NaN`` (perfect predictions) or\n",
      "        ``-Inf`` (imperfect predictions). To prevent such non-finite numbers to\n",
      "        pollute higher-level experiments such as a grid search cross-validation,\n",
      "        by default these cases are replaced with 1.0 (perfect predictions) or 0.0\n",
      "        (imperfect predictions) respectively. If ``force_finite``\n",
      "        is set to ``False``, this score falls back on the original :math:`R^2`\n",
      "        definition.\n",
      "        \n",
      "        .. note::\n",
      "           The Explained Variance score is similar to the\n",
      "           :func:`R^2 score <r2_score>`, with the notable difference that it\n",
      "           does not account for systematic offsets in the prediction. Most often\n",
      "           the :func:`R^2 score <r2_score>` should be preferred.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <explained_variance_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average', 'variance_weighted'} or             array-like of shape (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "        force_finite : bool, default=True\n",
      "            Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant\n",
      "            data should be replaced with real numbers (``1.0`` if prediction is\n",
      "            perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting\n",
      "            for hyperparameters' search procedures (e.g. grid search\n",
      "            cross-validation).\n",
      "        \n",
      "            .. versionadded:: 1.1\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of floats\n",
      "            The explained variance or ndarray if 'multioutput' is 'raw_values'.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        r2_score :\n",
      "            Similar metric, but accounting for systematic offsets in\n",
      "            prediction.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import explained_variance_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> explained_variance_score(y_true, y_pred)\n",
      "        0.957...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n",
      "        0.983...\n",
      "        >>> y_true = [-2, -2, -2]\n",
      "        >>> y_pred = [-2, -2, -2]\n",
      "        >>> explained_variance_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> explained_variance_score(y_true, y_pred, force_finite=False)\n",
      "        nan\n",
      "        >>> y_true = [-2, -2, -2]\n",
      "        >>> y_pred = [-2, -2, -2 + 1e-8]\n",
      "        >>> explained_variance_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> explained_variance_score(y_true, y_pred, force_finite=False)\n",
      "        -inf\n",
      "    \n",
      "    f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the F1 score, also known as balanced F-score or F-measure.\n",
      "        \n",
      "        The F1 score can be interpreted as a harmonic mean of the precision and\n",
      "        recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
      "        The relative contribution of precision and recall to the F1 score are\n",
      "        equal. The formula for the F1 score is:\n",
      "        \n",
      "        .. math::\n",
      "            \\text{F1} = \\frac{2 * \\text{TP}}{2 * \\text{TP} + \\text{FP} + \\text{FN}}\n",
      "        \n",
      "        Where :math:`\\text{TP}` is the number of true positives, :math:`\\text{FN}` is the\n",
      "        number of false negatives, and :math:`\\text{FP}` is the number of false positives.\n",
      "        F1 is by default\n",
      "        calculated as 0.0 when there are no true positives, false negatives, or\n",
      "        false positives.\n",
      "        \n",
      "        Support beyond :term:`binary` targets is achieved by treating :term:`multiclass`\n",
      "        and :term:`multilabel` data as a collection of binary problems, one for each\n",
      "        label. For the :term:`binary` case, setting `average='binary'` will return\n",
      "        F1 score for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored\n",
      "        and F1 score for both classes are computed, then averaged or both returned (when\n",
      "        `average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,\n",
      "        F1 score for all `labels` are either returned or averaged depending on the\n",
      "        `average` parameter. Use `labels` specify the set of labels to calculate F1 score\n",
      "        for.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when `average != 'binary'`, and their\n",
      "            order if `average is None`. Labels present in the data can be\n",
      "            excluded, for example in multiclass classification to exclude a \"negative\n",
      "            class\". Labels not present in the data can be included and will be\n",
      "            \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n",
      "            By default, all labels in `y_true` and `y_pred` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=1\n",
      "            The class to report if `average='binary'` and the data is binary,\n",
      "            otherwise this parameter is ignored.\n",
      "            For multiclass or multilabel targets, set `labels=[pos_label]` and\n",
      "            `average != 'binary'` to report metrics for one label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the metrics for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when all\n",
      "            predictions and labels are negative.\n",
      "        \n",
      "            Notes:\n",
      "            - If set to \"warn\", this acts like 0, but a warning is also raised.\n",
      "            - If set to `np.nan`, such values will be excluded from the average.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "               `np.nan` option was added.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f1_score : float or array of float, shape = [n_unique_labels]\n",
      "            F1 score of the positive class in binary classification or weighted\n",
      "            average of the F1 scores of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fbeta_score : Compute the F-beta score.\n",
      "        precision_recall_fscore_support : Compute the precision, recall, F-score,\n",
      "            and support.\n",
      "        jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "        multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "            sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive + false negative == 0`` (i.e. a class\n",
      "        is completely absent from both ``y_true`` or ``y_pred``), f-score is\n",
      "        undefined. In such cases, by default f-score will be set to 0.0, and\n",
      "        ``UndefinedMetricWarning`` will be raised. This behavior can be modified by\n",
      "        setting the ``zero_division`` parameter.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import f1_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> f1_score(y_true, y_pred, average='macro')\n",
      "        0.267\n",
      "        >>> f1_score(y_true, y_pred, average='micro')\n",
      "        0.33\n",
      "        >>> f1_score(y_true, y_pred, average='weighted')\n",
      "        0.267\n",
      "        >>> f1_score(y_true, y_pred, average=None)\n",
      "        array([0.8, 0. , 0. ])\n",
      "        \n",
      "        >>> # binary classification\n",
      "        >>> y_true_empty = [0, 0, 0, 0, 0, 0]\n",
      "        >>> y_pred_empty = [0, 0, 0, 0, 0, 0]\n",
      "        >>> f1_score(y_true_empty, y_pred_empty)\n",
      "        0.0...\n",
      "        >>> f1_score(y_true_empty, y_pred_empty, zero_division=1.0)\n",
      "        1.0...\n",
      "        >>> f1_score(y_true_empty, y_pred_empty, zero_division=np.nan)\n",
      "        nan...\n",
      "        \n",
      "        >>> # multilabel classification\n",
      "        >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "        >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "        >>> f1_score(y_true, y_pred, average=None)\n",
      "        array([0.66666667, 1.        , 0.66666667])\n",
      "    \n",
      "    fbeta_score(y_true, y_pred, *, beta, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the F-beta score.\n",
      "        \n",
      "        The F-beta score is the weighted harmonic mean of precision and recall,\n",
      "        reaching its optimal value at 1 and its worst value at 0.\n",
      "        \n",
      "        The `beta` parameter represents the ratio of recall importance to\n",
      "        precision importance. `beta > 1` gives more weight to recall, while\n",
      "        `beta < 1` favors precision. For example, `beta = 2` makes recall twice\n",
      "        as important as precision, while `beta = 0.5` does the opposite.\n",
      "        Asymptotically, `beta -> +inf` considers only recall, and `beta -> 0`\n",
      "        only precision.\n",
      "        \n",
      "        The formula for F-beta score is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           F_\\beta = \\frac{(1 + \\beta^2) \\text{tp}}\n",
      "                            {(1 + \\beta^2) \\text{tp} + \\text{fp} + \\beta^2 \\text{fn}}\n",
      "        \n",
      "        Where :math:`\\text{tp}` is the number of true positives, :math:`\\text{fp}` is the\n",
      "        number of false positives, and :math:`\\text{fn}` is the number of false negatives.\n",
      "        \n",
      "        Support beyond term:`binary` targets is achieved by treating :term:`multiclass`\n",
      "        and :term:`multilabel` data as a collection of binary problems, one for each\n",
      "        label. For the :term:`binary` case, setting `average='binary'` will return\n",
      "        F-beta score for `pos_label`. If `average` is not `'binary'`, `pos_label` is\n",
      "        ignored and F-beta score for both classes are computed, then averaged or both\n",
      "        returned (when `average=None`). Similarly, for :term:`multiclass` and\n",
      "        :term:`multilabel` targets, F-beta score for all `labels` are either returned or\n",
      "        averaged depending on the `average` parameter. Use `labels` specify the set of\n",
      "        labels to calculate F-beta score for.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float\n",
      "            Determines the weight of recall in the combined score.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when `average != 'binary'`, and their\n",
      "            order if `average is None`. Labels present in the data can be\n",
      "            excluded, for example in multiclass classification to exclude a \"negative\n",
      "            class\". Labels not present in the data can be included and will be\n",
      "            \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n",
      "            By default, all labels in `y_true` and `y_pred` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=1\n",
      "            The class to report if `average='binary'` and the data is binary,\n",
      "            otherwise this parameter is ignored.\n",
      "            For multiclass or multilabel targets, set `labels=[pos_label]` and\n",
      "            `average != 'binary'` to report metrics for one label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the metrics for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when all\n",
      "            predictions and labels are negative.\n",
      "        \n",
      "            Notes:\n",
      "        \n",
      "            - If set to \"warn\", this acts like 0, but a warning is also raised.\n",
      "            - If set to `np.nan`, such values will be excluded from the average.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "               `np.nan` option was added.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            F-beta score of the positive class in binary classification or weighted\n",
      "            average of the F-beta score of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support : Compute the precision, recall, F-score,\n",
      "            and support.\n",
      "        multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "            sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive + false negative == 0``, f-score\n",
      "        returns 0.0 and raises ``UndefinedMetricWarning``. This behavior can be\n",
      "        modified by setting ``zero_division``.\n",
      "        \n",
      "        F-beta score is not implemented as a named scorer that can be passed to\n",
      "        the `scoring` parameter of cross-validation tools directly: it requires to be\n",
      "        wrapped with :func:`make_scorer` so as to specify the value of `beta`. See\n",
      "        examples for details.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).\n",
      "               Modern Information Retrieval. Addison Wesley, pp. 327-328.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import fbeta_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
      "        0.238\n",
      "        >>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n",
      "        0.33\n",
      "        >>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\n",
      "        0.238\n",
      "        >>> fbeta_score(y_true, y_pred, average=None, beta=0.5)\n",
      "        array([0.71, 0.        , 0.        ])\n",
      "        >>> y_pred_empty = [0, 0, 0, 0, 0, 0]\n",
      "        >>> fbeta_score(\n",
      "        ...     y_true,\n",
      "        ...     y_pred_empty,\n",
      "        ...     average=\"macro\",\n",
      "        ...     zero_division=np.nan,\n",
      "        ...     beta=0.5,\n",
      "        ... )\n",
      "        0.128\n",
      "        \n",
      "        In order to use :func:`fbeta_scorer` as a scorer, a callable\n",
      "        scorer objects needs to be created first with :func:`make_scorer`,\n",
      "        passing the value for the `beta` parameter.\n",
      "        \n",
      "        >>> from sklearn.metrics import fbeta_score, make_scorer\n",
      "        >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
      "        >>> from sklearn.model_selection import GridSearchCV\n",
      "        >>> from sklearn.svm import LinearSVC\n",
      "        >>> grid = GridSearchCV(\n",
      "        ...     LinearSVC(dual=\"auto\"),\n",
      "        ...     param_grid={'C': [1, 10]},\n",
      "        ...     scoring=ftwo_scorer,\n",
      "        ...     cv=5\n",
      "        ... )\n",
      "    \n",
      "    fowlkes_mallows_score(labels_true, labels_pred, *, sparse='deprecated')\n",
      "        Measure the similarity of two clusterings of a set of points.\n",
      "        \n",
      "        .. versionadded:: 0.18\n",
      "        \n",
      "        The Fowlkes-Mallows index (FMI) is defined as the geometric mean of\n",
      "        the precision and recall::\n",
      "        \n",
      "            FMI = TP / sqrt((TP + FP) * (TP + FN))\n",
      "        \n",
      "        Where ``TP`` is the number of **True Positive** (i.e. the number of pairs of\n",
      "        points that belong to the same cluster in both ``labels_true`` and\n",
      "        ``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the\n",
      "        number of pairs of points that belong to the same cluster in\n",
      "        ``labels_pred`` but not in ``labels_true``) and ``FN`` is the number of\n",
      "        **False Negative** (i.e. the number of pairs of points that belong to the\n",
      "        same cluster in ``labels_true`` but not in ``labels_pred``).\n",
      "        \n",
      "        The score ranges from 0 to 1. A high value indicates a good similarity\n",
      "        between two clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <fowlkes_mallows_scores>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=int\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=int\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        sparse : bool, default=False\n",
      "            Compute contingency matrix internally with sparse matrix.\n",
      "        \n",
      "            .. deprecated:: 1.7\n",
      "                The ``sparse`` parameter is deprecated and will be removed in 1.9. It has\n",
      "                no effect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "           The resulting Fowlkes-Mallows score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two\n",
      "           hierarchical clusterings\". Journal of the American Statistical\n",
      "           Association\n",
      "           <https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Fowlkes-Mallows Index\n",
      "               <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import fowlkes_mallows_score\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally random, hence the FMI is null::\n",
      "        \n",
      "          >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "    \n",
      "    get_scorer(scoring)\n",
      "        Get a scorer from string.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <scoring_parameter>`.\n",
      "        :func:`~sklearn.metrics.get_scorer_names` can be used to retrieve the names\n",
      "        of all available scorers.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scoring : str, callable or None\n",
      "            Scoring method as string. If callable it is returned as is.\n",
      "            If None, returns None.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            The scorer.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When passed a string, this function always returns a copy of the scorer\n",
      "        object. Calling `get_scorer` twice for the same scorer results in two\n",
      "        separate scorer objects.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.dummy import DummyClassifier\n",
      "        >>> from sklearn.metrics import get_scorer\n",
      "        >>> X = np.reshape([0, 1, -1, -0.5, 2], (-1, 1))\n",
      "        >>> y = np.array([0, 1, 1, 0, 1])\n",
      "        >>> classifier = DummyClassifier(strategy=\"constant\", constant=0).fit(X, y)\n",
      "        >>> accuracy = get_scorer(\"accuracy\")\n",
      "        >>> accuracy(classifier, X, y)\n",
      "        0.4\n",
      "    \n",
      "    get_scorer_names()\n",
      "        Get the names of all available scorers.\n",
      "        \n",
      "        These names can be passed to :func:`~sklearn.metrics.get_scorer` to\n",
      "        retrieve the scorer object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        list of str\n",
      "            Names of all available scorers.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import get_scorer_names\n",
      "        >>> all_scorers = get_scorer_names()\n",
      "        >>> type(all_scorers)\n",
      "        <class 'list'>\n",
      "        >>> all_scorers[:3]\n",
      "        ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score']\n",
      "        >>> \"roc_auc\" in all_scorers\n",
      "        True\n",
      "    \n",
      "    hamming_loss(y_true, y_pred, *, sample_weight=None)\n",
      "        Compute the average Hamming loss.\n",
      "        \n",
      "        The Hamming loss is the fraction of labels that are incorrectly predicted.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hamming_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int\n",
      "            Return the average Hamming loss between element of ``y_true`` and\n",
      "            ``y_pred``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score : Compute the accuracy score. By default, the function will\n",
      "            return the fraction of correct predictions divided by the total number\n",
      "            of predictions.\n",
      "        jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "        zero_one_loss : Compute the Zero-one classification loss. By default, the\n",
      "            function will return the percentage of imperfectly predicted subsets.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multiclass classification, the Hamming loss corresponds to the Hamming\n",
      "        distance between ``y_true`` and ``y_pred`` which is equivalent to the\n",
      "        subset ``zero_one_loss`` function, when `normalize` parameter is set to\n",
      "        True.\n",
      "        \n",
      "        In multilabel classification, the Hamming loss is different from the\n",
      "        subset zero-one loss. The zero-one loss considers the entire set of labels\n",
      "        for a given sample incorrect if it does not entirely match the true set of\n",
      "        labels. Hamming loss is more forgiving in that it penalizes only the\n",
      "        individual labels.\n",
      "        \n",
      "        The Hamming loss is upperbounded by the subset zero-one loss, when\n",
      "        `normalize` parameter is set to True. It is always between 0 and 1,\n",
      "        lower being better.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:\n",
      "               An Overview. International Journal of Data Warehousing & Mining,\n",
      "               3(3), 1-13, July-September 2007.\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Hamming distance\n",
      "               <https://en.wikipedia.org/wiki/Hamming_distance>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import hamming_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> hamming_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n",
      "        0.75\n",
      "    \n",
      "    hinge_loss(y_true, pred_decision, *, labels=None, sample_weight=None)\n",
      "        Average hinge loss (non-regularized).\n",
      "        \n",
      "        In binary class case, assuming labels in y_true are encoded with +1 and -1,\n",
      "        when a prediction mistake is made, ``margin = y_true * pred_decision`` is\n",
      "        always negative (since the signs disagree), implying ``1 - margin`` is\n",
      "        always greater than 1.  The cumulated hinge loss is therefore an upper\n",
      "        bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        In multiclass case, the function expects that either all the labels are\n",
      "        included in y_true or an optional labels argument is provided which\n",
      "        contains all the labels. The multilabel margin is calculated according\n",
      "        to Crammer-Singer's method. As in the binary case, the cumulated hinge loss\n",
      "        is an upper bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hinge_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            True target, consisting of integers of two values. The positive label\n",
      "            must be greater than the negative label.\n",
      "        \n",
      "        pred_decision : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Predicted decisions, as output by decision_function (floats).\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            Contains all the labels for the problem. Used in multiclass hinge loss.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            Average hinge loss.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Hinge loss\n",
      "               <https://en.wikipedia.org/wiki/Hinge_loss>`_.\n",
      "        \n",
      "        .. [2] Koby Crammer, Yoram Singer. On the Algorithmic\n",
      "               Implementation of Multiclass Kernel-based Vector\n",
      "               Machines. Journal of Machine Learning Research 2,\n",
      "               (2001), 265-292.\n",
      "        \n",
      "        .. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models\n",
      "               by Robert C. Moore, John DeNero\n",
      "               <https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37362.pdf>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import svm\n",
      "        >>> from sklearn.metrics import hinge_loss\n",
      "        >>> X = [[0], [1]]\n",
      "        >>> y = [-1, 1]\n",
      "        >>> est = svm.LinearSVC(random_state=0)\n",
      "        >>> est.fit(X, y)\n",
      "        LinearSVC(random_state=0)\n",
      "        >>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
      "        >>> pred_decision\n",
      "        array([-2.18,  2.36,  0.09])\n",
      "        >>> hinge_loss([-1, 1, 1], pred_decision)\n",
      "        0.30\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> X = np.array([[0], [1], [2], [3]])\n",
      "        >>> Y = np.array([0, 1, 2, 3])\n",
      "        >>> labels = np.array([0, 1, 2, 3])\n",
      "        >>> est = svm.LinearSVC()\n",
      "        >>> est.fit(X, Y)\n",
      "        LinearSVC()\n",
      "        >>> pred_decision = est.decision_function([[-1], [2], [3]])\n",
      "        >>> y_true = [0, 2, 3]\n",
      "        >>> hinge_loss(y_true, pred_decision, labels=labels)\n",
      "        0.56\n",
      "    \n",
      "    homogeneity_completeness_v_measure(labels_true, labels_pred, *, beta=1.0)\n",
      "        Compute the homogeneity and completeness and V-Measure scores at once.\n",
      "        \n",
      "        Those metrics are based on normalized conditional entropy measures of\n",
      "        the clustering labeling to evaluate given the knowledge of a Ground\n",
      "        Truth class labels of the same samples.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        Both scores have positive values between 0.0 and 1.0, larger values\n",
      "        being desirable.\n",
      "        \n",
      "        Those 3 metrics are independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score values in any way.\n",
      "        \n",
      "        V-Measure is furthermore symmetric: swapping ``labels_true`` and\n",
      "        ``label_pred`` will give the same score. This does not hold for\n",
      "        homogeneity and completeness. V-Measure is identical to\n",
      "        :func:`normalized_mutual_info_score` with the arithmetic averaging\n",
      "        method.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,)\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "            Score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling.\n",
      "        \n",
      "        completeness : float\n",
      "            Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.\n",
      "        \n",
      "        v_measure : float\n",
      "            Harmonic mean of the first two.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score : Homogeneity metric of cluster labeling.\n",
      "        completeness_score : Completeness metric of cluster labeling.\n",
      "        v_measure_score : V-Measure (NMI with arithmetic mean option).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import homogeneity_completeness_v_measure\n",
      "        >>> y_true, y_pred = [0, 0, 1, 1, 2, 2], [0, 0, 1, 2, 2, 2]\n",
      "        >>> homogeneity_completeness_v_measure(y_true, y_pred)\n",
      "        (0.71, 0.771, 0.74)\n",
      "    \n",
      "    homogeneity_score(labels_true, labels_pred)\n",
      "        Homogeneity metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`completeness_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,)\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "           Score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        completeness_score : Completeness metric of cluster labeling.\n",
      "        v_measure_score : V-Measure (NMI with arithmetic mean option).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are homogeneous::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import homogeneity_score\n",
      "          >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that further split classes into more clusters can be\n",
      "        perfectly homogeneous::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          1.000000\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          1.000000\n",
      "        \n",
      "        Clusters that include samples from different classes do not make for an\n",
      "        homogeneous labeling::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          0.0...\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          0.0...\n",
      "    \n",
      "    jaccard_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Jaccard similarity coefficient score.\n",
      "        \n",
      "        The Jaccard index [1], or Jaccard similarity coefficient, defined as\n",
      "        the size of the intersection divided by the size of the union of two label\n",
      "        sets, is used to compare set of predicted labels for a sample to the\n",
      "        corresponding set of labels in ``y_true``.\n",
      "        \n",
      "        Support beyond term:`binary` targets is achieved by treating :term:`multiclass`\n",
      "        and :term:`multilabel` data as a collection of binary problems, one for each\n",
      "        label. For the :term:`binary` case, setting `average='binary'` will return the\n",
      "        Jaccard similarity coefficient for `pos_label`. If `average` is not `'binary'`,\n",
      "        `pos_label` is ignored and scores for both classes are computed, then averaged or\n",
      "        both returned (when `average=None`). Similarly, for :term:`multiclass` and\n",
      "        :term:`multilabel` targets, scores for all `labels` are either returned or\n",
      "        averaged depending on the `average` parameter. Use `labels` specify the set of\n",
      "        labels to calculate the score for.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <jaccard_similarity_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            The set of labels to include when `average != 'binary'`, and their\n",
      "            order if `average is None`. Labels present in the data can be\n",
      "            excluded, for example in multiclass classification to exclude a \"negative\n",
      "            class\". Labels not present in the data can be included and will be\n",
      "            \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n",
      "            By default, all labels in `y_true` and `y_pred` are used in sorted order.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=1\n",
      "            The class to report if `average='binary'` and the data is binary,\n",
      "            otherwise this parameter is ignored.\n",
      "            For multiclass or multilabel targets, set `labels=[pos_label]` and\n",
      "            `average != 'binary'` to report metrics for one label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted',             'binary'} or None, default='binary'\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", {0.0, 1.0}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when there\n",
      "            there are no negative values in predictions and labels. If set to\n",
      "            \"warn\", this acts like 0, but a warning is also raised.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of shape (n_unique_labels,), dtype=np.float64\n",
      "            The Jaccard score. When `average` is not `None`, a single scalar is\n",
      "            returned.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score : Function for calculating the accuracy score.\n",
      "        f1_score : Function for calculating the F1 score.\n",
      "        multilabel_confusion_matrix : Function for computing a confusion matrix                                  for each class or sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        :func:`jaccard_score` may be a poor metric if there are no\n",
      "        positives for some samples or classes. Jaccard is undefined if there are\n",
      "        no true or predicted labels, and our implementation will return a score\n",
      "        of 0 with a warning.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Jaccard index\n",
      "               <https://en.wikipedia.org/wiki/Jaccard_index>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import jaccard_score\n",
      "        >>> y_true = np.array([[0, 1, 1],\n",
      "        ...                    [1, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 1, 1],\n",
      "        ...                    [1, 0, 0]])\n",
      "        \n",
      "        In the binary case:\n",
      "        \n",
      "        >>> jaccard_score(y_true[0], y_pred[0])\n",
      "        0.6666\n",
      "        \n",
      "        In the 2D comparison case (e.g. image similarity):\n",
      "        \n",
      "        >>> jaccard_score(y_true, y_pred, average=\"micro\")\n",
      "        0.6\n",
      "        \n",
      "        In the multilabel case:\n",
      "        \n",
      "        >>> jaccard_score(y_true, y_pred, average='samples')\n",
      "        0.5833\n",
      "        >>> jaccard_score(y_true, y_pred, average='macro')\n",
      "        0.6666\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 0.5, 1. ])\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> y_pred = [0, 2, 1, 2]\n",
      "        >>> y_true = [0, 1, 2, 2]\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        array([1. , 0. , 0.33])\n",
      "    \n",
      "    label_ranking_average_precision_score(y_true, y_score, *, sample_weight=None)\n",
      "        Compute ranking-based average precision.\n",
      "        \n",
      "        Label ranking average precision (LRAP) is the average over each ground\n",
      "        truth label assigned to each sample, of the ratio of true vs. total\n",
      "        labels with lower score.\n",
      "        \n",
      "        This metric is used in multilabel ranking problem, where the goal\n",
      "        is to give better rank to the labels associated to each sample.\n",
      "        \n",
      "        The obtained score is always strictly greater than 0 and\n",
      "        the best value is 1.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_average_precision>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {array-like, sparse matrix} of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "            For :term:`decision_function` scores, values greater than or equal to\n",
      "            zero should indicate the positive class.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            Ranking-based average precision score.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import label_ranking_average_precision_score\n",
      "        >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
      "        >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
      "        >>> label_ranking_average_precision_score(y_true, y_score)\n",
      "        0.416\n",
      "    \n",
      "    label_ranking_loss(y_true, y_score, *, sample_weight=None)\n",
      "        Compute Ranking loss measure.\n",
      "        \n",
      "        Compute the average number of label pairs that are incorrectly ordered\n",
      "        given y_score weighted by the size of the label set and the number of\n",
      "        labels not in the label set.\n",
      "        \n",
      "        This is similar to the error set size, but weighted by the number of\n",
      "        relevant and irrelevant labels. The best performance is achieved with\n",
      "        a ranking loss of zero.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_loss>`.\n",
      "        \n",
      "        .. versionadded:: 0.17\n",
      "           A function *label_ranking_loss*\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {array-like, sparse matrix} of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "            For :term:`decision_function` scores, values greater than or equal to\n",
      "            zero should indicate the positive class.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            Average number of label pairs that are incorrectly ordered given\n",
      "            y_score weighted by the size of the label set and the number of labels not\n",
      "            in the label set.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import label_ranking_loss\n",
      "        >>> y_true = [[1, 0, 0], [0, 0, 1]]\n",
      "        >>> y_score = [[0.75, 0.5, 1], [1, 0.2, 0.1]]\n",
      "        >>> label_ranking_loss(y_true, y_score)\n",
      "        0.75\n",
      "    \n",
      "    log_loss(y_true, y_pred, *, normalize=True, sample_weight=None, labels=None)\n",
      "        Log loss, aka logistic loss or cross-entropy loss.\n",
      "        \n",
      "        This is the loss function used in (multinomial) logistic regression\n",
      "        and extensions of it such as neural networks, defined as the negative\n",
      "        log-likelihood of a logistic model that returns ``y_pred`` probabilities\n",
      "        for its training data ``y_true``.\n",
      "        The log loss is only defined for two or more labels.\n",
      "        For a single sample with true label :math:`y \\in \\{0,1\\}` and\n",
      "        a probability estimate :math:`p = \\operatorname{Pr}(y = 1)`, the log\n",
      "        loss is:\n",
      "        \n",
      "        .. math::\n",
      "            L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))\n",
      "        \n",
      "        Read more in the :ref:`User Guide <log_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like or label indicator matrix\n",
      "            Ground truth (correct) labels for n_samples samples.\n",
      "        \n",
      "        y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n",
      "            Predicted probabilities, as returned by a classifier's\n",
      "            predict_proba method. If ``y_pred.shape = (n_samples,)``\n",
      "            the probabilities provided are assumed to be that of the\n",
      "            positive class. The labels in ``y_pred`` are assumed to be\n",
      "            ordered alphabetically, as done by\n",
      "            :class:`~sklearn.preprocessing.LabelBinarizer`.\n",
      "        \n",
      "            `y_pred` values are clipped to `[eps, 1-eps]` where `eps` is the machine\n",
      "            precision for `y_pred`'s dtype.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If true, return the mean loss per sample.\n",
      "            Otherwise, return the sum of the per-sample losses.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            If not provided, labels will be inferred from y_true. If ``labels``\n",
      "            is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n",
      "            assumed to be binary and are inferred from ``y_true``.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            Log loss, aka logistic loss or cross-entropy loss.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\n",
      "        p. 209.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import log_loss\n",
      "        >>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],\n",
      "        ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n",
      "        0.21616\n",
      "    \n",
      "    make_scorer(score_func, *, response_method='default', greater_is_better=True, **kwargs)\n",
      "        Make a scorer from a performance metric or loss function.\n",
      "        \n",
      "        A scorer is a wrapper around an arbitrary metric or loss function that is called\n",
      "        with the signature `scorer(estimator, X, y_true, **kwargs)`.\n",
      "        \n",
      "        It is accepted in all scikit-learn estimators or functions allowing a `scoring`\n",
      "        parameter.\n",
      "        \n",
      "        The parameter `response_method` allows to specify which method of the estimator\n",
      "        should be used to feed the scoring/loss function.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <scoring_callable>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        score_func : callable\n",
      "            Score function (or loss function) with signature\n",
      "            ``score_func(y, y_pred, **kwargs)``.\n",
      "        \n",
      "        response_method : {\"predict_proba\", \"decision_function\", \"predict\"} or             list/tuple of such str, default=None\n",
      "        \n",
      "            Specifies the response method to use get prediction from an estimator\n",
      "            (i.e. :term:`predict_proba`, :term:`decision_function` or\n",
      "            :term:`predict`). Possible choices are:\n",
      "        \n",
      "            - if `str`, it corresponds to the name to the method to return;\n",
      "            - if a list or tuple of `str`, it provides the method names in order of\n",
      "              preference. The method returned corresponds to the first method in\n",
      "              the list and which is implemented by `estimator`.\n",
      "            - if `None`, it is equivalent to `\"predict\"`.\n",
      "        \n",
      "            .. versionadded:: 1.4\n",
      "        \n",
      "            .. deprecated:: 1.6\n",
      "                None is equivalent to 'predict' and is deprecated. It will be removed in\n",
      "                version 1.8.\n",
      "        \n",
      "        greater_is_better : bool, default=True\n",
      "            Whether `score_func` is a score function (default), meaning high is\n",
      "            good, or a loss function, meaning low is good. In the latter case, the\n",
      "            scorer object will sign-flip the outcome of the `score_func`.\n",
      "        \n",
      "        **kwargs : additional arguments\n",
      "            Additional parameters to be passed to `score_func`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            Callable object that returns a scalar score; greater is better.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score, make_scorer\n",
      "        >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
      "        >>> ftwo_scorer\n",
      "        make_scorer(fbeta_score, response_method='predict', beta=2)\n",
      "        >>> from sklearn.model_selection import GridSearchCV\n",
      "        >>> from sklearn.svm import LinearSVC\n",
      "        >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n",
      "        ...                     scoring=ftwo_scorer)\n",
      "    \n",
      "    matthews_corrcoef(y_true, y_pred, *, sample_weight=None)\n",
      "        Compute the Matthews correlation coefficient (MCC).\n",
      "        \n",
      "        The Matthews correlation coefficient is used in machine learning as a\n",
      "        measure of the quality of binary and multiclass classifications. It takes\n",
      "        into account true and false positives and negatives and is generally\n",
      "        regarded as a balanced measure which can be used even if the classes are of\n",
      "        very different sizes. The MCC is in essence a correlation coefficient value\n",
      "        between -1 and +1. A coefficient of +1 represents a perfect prediction, 0\n",
      "        an average random prediction and -1 an inverse prediction.  The statistic\n",
      "        is also known as the phi coefficient. [source: Wikipedia]\n",
      "        \n",
      "        Binary and multiclass labels are supported.  Only in the binary case does\n",
      "        this relate to information about true and false positives and negatives.\n",
      "        See references below.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <matthews_corrcoef>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mcc : float\n",
      "            The Matthews correlation coefficient (+1 represents a perfect\n",
      "            prediction, 0 an average random prediction and -1 and inverse\n",
      "            prediction).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] :doi:`Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the\n",
      "           accuracy of prediction algorithms for classification: an overview.\n",
      "           <10.1093/bioinformatics/16.5.412>`\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Matthews Correlation Coefficient (phi coefficient)\n",
      "           <https://en.wikipedia.org/wiki/Phi_coefficient>`_.\n",
      "        \n",
      "        .. [3] `Gorodkin, (2004). Comparing two K-category assignments by a\n",
      "            K-category correlation coefficient\n",
      "            <https://www.sciencedirect.com/science/article/pii/S1476927104000799>`_.\n",
      "        \n",
      "        .. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN\n",
      "            Error Measures in MultiClass Prediction\n",
      "            <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import matthews_corrcoef\n",
      "        >>> y_true = [+1, +1, +1, -1]\n",
      "        >>> y_pred = [+1, -1, +1, +1]\n",
      "        >>> matthews_corrcoef(y_true, y_pred)\n",
      "        -0.33\n",
      "    \n",
      "    max_error(y_true, y_pred)\n",
      "        The max_error metric calculates the maximum residual error.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <max_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        max_error : float\n",
      "            A positive floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import max_error\n",
      "        >>> y_true = [3, 2, 7, 1]\n",
      "        >>> y_pred = [4, 2, 7, 1]\n",
      "        >>> max_error(y_true, y_pred)\n",
      "        1.0\n",
      "    \n",
      "    mean_absolute_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean absolute error regression loss.\n",
      "        \n",
      "        The mean absolute error is a non-negative floating point value, where best value\n",
      "        is 0.0. Read more in the :ref:`User Guide <mean_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or array of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            MAE output is non-negative floating point. The best value is 0.0.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.75\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.5, 1. ])\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.85...\n",
      "    \n",
      "    mean_absolute_percentage_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean absolute percentage error (MAPE) regression loss.\n",
      "        \n",
      "        Note that we are not using the common \"percentage\" definition: the percentage\n",
      "        in the range [0, 100] is converted to a relative value in the range [0, 1]\n",
      "        by dividing by 100. Thus, an error of 200% corresponds to a relative error of 2.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_absolute_percentage_error>`.\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "            If input is list then the shape must be (n_outputs,).\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute percentage error\n",
      "            is returned for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            MAPE output is non-negative floating point. The best value is 0.0.\n",
      "            But note that bad predictions can lead to arbitrarily large\n",
      "            MAPE values, especially if some `y_true` values are very close to zero.\n",
      "            Note that we return a large value instead of `inf` when `y_true` is zero.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_absolute_percentage_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        0.3273...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        0.5515...\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.6198...\n",
      "        >>> # the value when some element of the y_true is zero is arbitrarily high because\n",
      "        >>> # of the division by epsilon\n",
      "        >>> y_true = [1., 0., 2.4, 7.]\n",
      "        >>> y_pred = [1.2, 0.1, 2.4, 8.]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        112589990684262.48\n",
      "    \n",
      "    mean_gamma_deviance(y_true, y_pred, *, sample_weight=None)\n",
      "        Mean Gamma deviance regression loss.\n",
      "        \n",
      "        Gamma deviance is equivalent to the Tweedie deviance with\n",
      "        the power parameter `power=2`. It is invariant to scaling of\n",
      "        the target variable, and measures relative errors.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values. Requires y_true > 0.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values. Requires y_pred > 0.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_gamma_deviance\n",
      "        >>> y_true = [2, 0.5, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_gamma_deviance(y_true, y_pred)\n",
      "        1.0568...\n",
      "    \n",
      "    mean_pinball_loss(y_true, y_pred, *, sample_weight=None, alpha=0.5, multioutput='uniform_average')\n",
      "        Pinball loss for quantile regression.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <pinball_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        alpha : float, slope of the pinball loss, default=0.5,\n",
      "            This loss is equivalent to :ref:`mean_absolute_error` when `alpha=0.5`,\n",
      "            `alpha=0.95` is minimized by estimators of the 95th percentile.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            The pinball loss output is a non-negative floating point. The best\n",
      "            value is 0.0.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_pinball_loss\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)\n",
      "        0.03...\n",
      "        >>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)\n",
      "        0.3...\n",
      "        >>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)\n",
      "        0.3...\n",
      "        >>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)\n",
      "        0.03...\n",
      "        >>> mean_pinball_loss(y_true, y_true, alpha=0.1)\n",
      "        0.0\n",
      "        >>> mean_pinball_loss(y_true, y_true, alpha=0.9)\n",
      "        0.0\n",
      "    \n",
      "    mean_poisson_deviance(y_true, y_pred, *, sample_weight=None)\n",
      "        Mean Poisson deviance regression loss.\n",
      "        \n",
      "        Poisson deviance is equivalent to the Tweedie deviance with\n",
      "        the power parameter `power=1`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values. Requires y_true >= 0.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values. Requires y_pred > 0.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_poisson_deviance\n",
      "        >>> y_true = [2, 0, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_poisson_deviance(y_true, y_pred)\n",
      "        1.4260...\n",
      "    \n",
      "    mean_squared_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean squared error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or array of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_squared_error(y_true, y_pred)\n",
      "        0.375\n",
      "        >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
      "        >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
      "        >>> mean_squared_error(y_true, y_pred)\n",
      "        0.708...\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.41666667, 1.        ])\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.825...\n",
      "    \n",
      "    mean_squared_log_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean squared logarithmic error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_log_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "        \n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors when the input is of multioutput\n",
      "                format.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_log_error\n",
      "        >>> y_true = [3, 5, 2.5, 7]\n",
      "        >>> y_pred = [2.5, 5, 4, 8]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)\n",
      "        0.039...\n",
      "        >>> y_true = [[0.5, 1], [1, 2], [7, 6]]\n",
      "        >>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)\n",
      "        0.044...\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.00462428, 0.08377444])\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.060...\n",
      "    \n",
      "    mean_tweedie_deviance(y_true, y_pred, *, sample_weight=None, power=0)\n",
      "        Mean Tweedie deviance regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        power : float, default=0\n",
      "            Tweedie power parameter. Either power <= 0 or power >= 1.\n",
      "        \n",
      "            The higher `p` the less weight is given to extreme\n",
      "            deviations between true and predicted targets.\n",
      "        \n",
      "            - power < 0: Extreme stable distribution. Requires: y_pred > 0.\n",
      "            - power = 0 : Normal distribution, output corresponds to\n",
      "              mean_squared_error. y_true and y_pred can be any real numbers.\n",
      "            - power = 1 : Poisson distribution. Requires: y_true >= 0 and\n",
      "              y_pred > 0.\n",
      "            - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\n",
      "              and y_pred > 0.\n",
      "            - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\n",
      "            - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "            - otherwise : Positive stable distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_tweedie_deviance\n",
      "        >>> y_true = [2, 0, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_tweedie_deviance(y_true, y_pred, power=1)\n",
      "        1.4260...\n",
      "    \n",
      "    median_absolute_error(y_true, y_pred, *, multioutput='uniform_average', sample_weight=None)\n",
      "        Median absolute error regression loss.\n",
      "        \n",
      "        Median absolute error output is non-negative floating point. The best value\n",
      "        is 0.0. Read more in the :ref:`User Guide <median_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values. Array-like value defines\n",
      "            weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import median_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> median_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> median_absolute_error(y_true, y_pred)\n",
      "        0.75\n",
      "        >>> median_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.5, 1. ])\n",
      "        >>> median_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.85\n",
      "    \n",
      "    multilabel_confusion_matrix(y_true, y_pred, *, sample_weight=None, labels=None, samplewise=False)\n",
      "        Compute a confusion matrix for each class or sample.\n",
      "        \n",
      "        .. versionadded:: 0.21\n",
      "        \n",
      "        Compute class-wise (default) or sample-wise (samplewise=True) multilabel\n",
      "        confusion matrix to evaluate the accuracy of a classification, and output\n",
      "        confusion matrices for each class or sample.\n",
      "        \n",
      "        In multilabel confusion matrix :math:`MCM`, the count of true negatives\n",
      "        is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,\n",
      "        true positives is :math:`MCM_{:,1,1}` and false positives is\n",
      "        :math:`MCM_{:,0,1}`.\n",
      "        \n",
      "        Multiclass data will be treated as if binarized under a one-vs-rest\n",
      "        transformation. Returned confusion matrices will be in the order of\n",
      "        sorted unique labels in the union of (y_true, y_pred).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <multilabel_confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            A list of classes or column indices to select some (or to force\n",
      "            inclusion of classes absent from the data).\n",
      "        \n",
      "        samplewise : bool, default=False\n",
      "            In the multilabel case, this calculates a confusion matrix per sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        multi_confusion : ndarray of shape (n_outputs, 2, 2)\n",
      "            A 2x2 confusion matrix corresponding to each output in the input.\n",
      "            When calculating class-wise multi_confusion (default), then\n",
      "            n_outputs = n_labels; when calculating sample-wise multi_confusion\n",
      "            (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,\n",
      "            the results will be returned in the order specified in ``labels``,\n",
      "            otherwise the results will be returned in sorted order by default.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        confusion_matrix : Compute confusion matrix to evaluate the accuracy of a\n",
      "            classifier.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The `multilabel_confusion_matrix` calculates class-wise or sample-wise\n",
      "        multilabel confusion matrices, and in multiclass tasks, labels are\n",
      "        binarized under a one-vs-rest way; while\n",
      "        :func:`~sklearn.metrics.confusion_matrix` calculates one confusion matrix\n",
      "        for confusion between every two classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Multilabel-indicator case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import multilabel_confusion_matrix\n",
      "        >>> y_true = np.array([[1, 0, 1],\n",
      "        ...                    [0, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 0, 0],\n",
      "        ...                    [0, 1, 1]])\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred)\n",
      "        array([[[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[0, 1],\n",
      "                [1, 0]]])\n",
      "        \n",
      "        Multiclass case:\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred,\n",
      "        ...                             labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[[3, 1],\n",
      "                [0, 2]],\n",
      "        <BLANKLINE>\n",
      "               [[5, 0],\n",
      "                [1, 0]],\n",
      "        <BLANKLINE>\n",
      "               [[2, 1],\n",
      "                [1, 2]]])\n",
      "    \n",
      "    mutual_info_score(labels_true, labels_pred, *, contingency=None)\n",
      "        Mutual Information between two clusterings.\n",
      "        \n",
      "        The Mutual Information is a measure of the similarity between two labels\n",
      "        of the same data. Where :math:`|U_i|` is the number of the samples\n",
      "        in cluster :math:`U_i` and :math:`|V_j|` is the number of the\n",
      "        samples in cluster :math:`V_j`, the Mutual Information\n",
      "        between clusterings :math:`U` and :math:`V` is given as:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}\n",
      "            \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching :math:`U` (i.e\n",
      "        ``label_true``) with :math:`V` (i.e. ``label_pred``) will return the\n",
      "        same score value. This can be useful to measure the agreement of two\n",
      "        independent label assignments strategies on the same dataset when the\n",
      "        real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=integral\n",
      "            A clustering of the data into disjoint subsets, called :math:`U` in\n",
      "            the above formula.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=integral\n",
      "            A clustering of the data into disjoint subsets, called :math:`V` in\n",
      "            the above formula.\n",
      "        \n",
      "        contingency : {array-like, sparse matrix} of shape             (n_classes_true, n_classes_pred), default=None\n",
      "            A contingency matrix given by the\n",
      "            :func:`~sklearn.metrics.cluster.contingency_matrix` function. If value\n",
      "            is ``None``, it will be computed, otherwise the given value is used,\n",
      "            with ``labels_true`` and ``labels_pred`` ignored.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mi : float\n",
      "           Mutual information, a non-negative value, measured in nats using the\n",
      "           natural logarithm.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_mutual_info_score : Adjusted against chance Mutual Information.\n",
      "        normalized_mutual_info_score : Normalized Mutual Information.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mutual_info_score\n",
      "        >>> labels_true = [0, 1, 1, 0, 1, 0]\n",
      "        >>> labels_pred = [0, 1, 0, 0, 1, 1]\n",
      "        >>> mutual_info_score(labels_true, labels_pred)\n",
      "        0.0566\n",
      "    \n",
      "    nan_euclidean_distances(X, Y=None, *, squared=False, missing_values=nan, copy=True)\n",
      "        Calculate the euclidean distances in the presence of missing values.\n",
      "        \n",
      "        Compute the euclidean distance between each pair of samples in X and Y,\n",
      "        where Y=X is assumed if Y=None. When calculating the distance between a\n",
      "        pair of samples, this formulation ignores feature coordinates with a\n",
      "        missing value in either sample and scales up the weight of the remaining\n",
      "        coordinates:\n",
      "        \n",
      "        .. code-block:: text\n",
      "        \n",
      "            dist(x,y) = sqrt(weight * sq. distance from present coordinates)\n",
      "        \n",
      "        where:\n",
      "        \n",
      "        .. code-block:: text\n",
      "        \n",
      "            weight = Total # of coordinates / # of present coordinates\n",
      "        \n",
      "        For example, the distance between ``[3, na, na, 6]`` and ``[1, na, 4, 5]`` is:\n",
      "        \n",
      "        .. math::\n",
      "            \\sqrt{\\frac{4}{2}((3-1)^2 + (6-5)^2)}\n",
      "        \n",
      "        If all the coordinates are missing or if there are no common present\n",
      "        coordinates then NaN is returned for that pair.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        .. versionadded:: 0.22\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_X, n_features)\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "        \n",
      "        Y : array-like of shape (n_samples_Y, n_features), default=None\n",
      "            An array where each row is a sample and each column is a feature.\n",
      "            If `None`, method uses `Y=X`.\n",
      "        \n",
      "        squared : bool, default=False\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        missing_values : np.nan, float or int, default=np.nan\n",
      "            Representation of missing value.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            Make and use a deep copy of X and Y (if Y exists).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "            Returns the distances between the row vectors of `X`\n",
      "            and the row vectors of `Y`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        paired_distances : Distances between pairs of elements of X and Y.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * John K. Dixon, \"Pattern Recognition with Partly Missing Data\",\n",
      "          IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue:\n",
      "          10, pp. 617 - 621, Oct. 1979.\n",
      "          http://ieeexplore.ieee.org/abstract/document/4310090/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import nan_euclidean_distances\n",
      "        >>> nan = float(\"NaN\")\n",
      "        >>> X = [[0, 1], [1, nan]]\n",
      "        >>> nan_euclidean_distances(X, X) # distance between rows of X\n",
      "        array([[0.        , 1.41421356],\n",
      "               [1.41421356, 0.        ]])\n",
      "        \n",
      "        >>> # get distance to origin\n",
      "        >>> nan_euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "    \n",
      "    ndcg_score(y_true, y_score, *, k=None, sample_weight=None, ignore_ties=False)\n",
      "        Compute Normalized Discounted Cumulative Gain.\n",
      "        \n",
      "        Sum the true scores ranked in the order induced by the predicted scores,\n",
      "        after applying a logarithmic discount. Then divide by the best possible\n",
      "        score (Ideal DCG, obtained for a perfect ranking) to obtain a score between\n",
      "        0 and 1.\n",
      "        \n",
      "        This ranking metric returns a high value if true labels are ranked high by\n",
      "        ``y_score``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples, n_labels)\n",
      "            True targets of multilabel classification, or true scores of entities\n",
      "            to be ranked. Negative values in `y_true` may result in an output\n",
      "            that is not between 0 and 1.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates, confidence values,\n",
      "            or non-thresholded measure of decisions (as returned by\n",
      "            \"decision_function\" on some classifiers).\n",
      "        \n",
      "        k : int, default=None\n",
      "            Only consider the highest k scores in the ranking. If `None`, use all\n",
      "            outputs.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights. If `None`, all samples are given the same weight.\n",
      "        \n",
      "        ignore_ties : bool, default=False\n",
      "            Assume that there are no ties in y_score (which is likely to be the\n",
      "            case if y_score is continuous) for efficiency gains.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        normalized_discounted_cumulative_gain : float in [0., 1.]\n",
      "            The averaged NDCG scores for all samples.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        dcg_score : Discounted Cumulative Gain (not normalized).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        `Wikipedia entry for Discounted Cumulative Gain\n",
      "        <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_\n",
      "        \n",
      "        Jarvelin, K., & Kekalainen, J. (2002).\n",
      "        Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n",
      "        Information Systems (TOIS), 20(4), 422-446.\n",
      "        \n",
      "        Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n",
      "        A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n",
      "        Annual Conference on Learning Theory (COLT 2013)\n",
      "        \n",
      "        McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n",
      "        performance measures efficiently in the presence of tied scores. In\n",
      "        European conference on information retrieval (pp. 414-421). Springer,\n",
      "        Berlin, Heidelberg.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import ndcg_score\n",
      "        >>> # we have ground-truth relevance of some answers to a query:\n",
      "        >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n",
      "        >>> # we predict some scores (relevance) for the answers\n",
      "        >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n",
      "        >>> ndcg_score(true_relevance, scores)\n",
      "        0.69\n",
      "        >>> scores = np.asarray([[.05, 1.1, 1., .5, .0]])\n",
      "        >>> ndcg_score(true_relevance, scores)\n",
      "        0.49\n",
      "        >>> # we can set k to truncate the sum; only top k answers contribute.\n",
      "        >>> ndcg_score(true_relevance, scores, k=4)\n",
      "        0.35\n",
      "        >>> # the normalization takes k into account so a perfect answer\n",
      "        >>> # would still get 1.0\n",
      "        >>> ndcg_score(true_relevance, true_relevance, k=4)\n",
      "        1.0...\n",
      "        >>> # now we have some ties in our prediction\n",
      "        >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n",
      "        >>> # by default ties are averaged, so here we get the average (normalized)\n",
      "        >>> # true relevance of our top predictions: (10 / 10 + 5 / 10) / 2 = .75\n",
      "        >>> ndcg_score(true_relevance, scores, k=1)\n",
      "        0.75\n",
      "        >>> # we can choose to ignore ties for faster results, but only\n",
      "        >>> # if we know there aren't ties in our scores, otherwise we get\n",
      "        >>> # wrong results:\n",
      "        >>> ndcg_score(true_relevance,\n",
      "        ...           scores, k=1, ignore_ties=True)\n",
      "        0.5...\n",
      "    \n",
      "    normalized_mutual_info_score(labels_true, labels_pred, *, average_method='arithmetic')\n",
      "        Normalized Mutual Information between two clusterings.\n",
      "        \n",
      "        Normalized Mutual Information (NMI) is a normalization of the Mutual\n",
      "        Information (MI) score to scale the results between 0 (no mutual\n",
      "        information) and 1 (perfect correlation). In this function, mutual\n",
      "        information is normalized by some generalized mean of ``H(labels_true)``\n",
      "        and ``H(labels_pred))``, defined by the `average_method`.\n",
      "        \n",
      "        This measure is not adjusted for chance. Therefore\n",
      "        :func:`adjusted_mutual_info_score` might be preferred.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        average_method : {'min', 'geometric', 'arithmetic', 'max'}, default='arithmetic'\n",
      "            How to compute the normalizer in the denominator.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "               The default value of ``average_method`` changed from 'geometric' to\n",
      "               'arithmetic'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        nmi : float\n",
      "           Score between 0.0 and 1.0 in normalized nats (based on the natural\n",
      "           logarithm). 1.0 stands for perfectly complete labeling.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        v_measure_score : V-Measure (NMI with arithmetic mean option).\n",
      "        adjusted_rand_score : Adjusted Rand Index.\n",
      "        adjusted_mutual_info_score : Adjusted Mutual Information (adjusted\n",
      "            against chance).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import normalized_mutual_info_score\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the NMI is null::\n",
      "        \n",
      "          >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "    \n",
      "    pair_confusion_matrix(labels_true, labels_pred)\n",
      "        Pair confusion matrix arising from two clusterings.\n",
      "        \n",
      "        The pair confusion matrix :math:`C` computes a 2 by 2 similarity matrix\n",
      "        between two clusterings by considering all pairs of samples and counting\n",
      "        pairs that are assigned into the same or into different clusters under\n",
      "        the true and predicted clusterings [1]_.\n",
      "        \n",
      "        Considering a pair of samples that is clustered together a positive pair,\n",
      "        then as in binary classification the count of true negatives is\n",
      "        :math:`C_{00}`, false negatives is :math:`C_{10}`, true positives is\n",
      "        :math:`C_{11}` and false positives is :math:`C_{01}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <pair_confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=integral\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=integral\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : ndarray of shape (2, 2), dtype=np.int64\n",
      "            The contingency matrix.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        sklearn.metrics.rand_score : Rand Score.\n",
      "        sklearn.metrics.adjusted_rand_score : Adjusted Rand Score.\n",
      "        sklearn.metrics.adjusted_mutual_info_score : Adjusted Mutual Information.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] :doi:`Hubert, L., Arabie, P. \"Comparing partitions.\"\n",
      "               Journal of Classification 2, 193–218 (1985).\n",
      "               <10.1007/BF01908075>`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have all non-zero entries on the\n",
      "        diagonal regardless of actual label values:\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import pair_confusion_matrix\n",
      "          >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          array([[8, 0],\n",
      "                 [0, 4]]...\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may be not always pure, hence penalized, and\n",
      "        have some off-diagonal non-zero entries:\n",
      "        \n",
      "          >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          array([[8, 2],\n",
      "                 [0, 2]]...\n",
      "        \n",
      "        Note that the matrix is not symmetric.\n",
      "    \n",
      "    pairwise_distances(X, Y=None, metric='euclidean', *, n_jobs=None, force_all_finite='deprecated', ensure_all_finite=None, **kwds)\n",
      "        Compute the distance matrix from a feature array X and optional Y.\n",
      "        \n",
      "        This function takes one or two feature arrays or a distance matrix, and returns\n",
      "        a distance matrix.\n",
      "        \n",
      "        - If `X` is a feature array, of shape (n_samples_X, n_features), and:\n",
      "        \n",
      "          - `Y` is `None` and `metric` is not 'precomputed', the pairwise distances\n",
      "            between `X` and itself are returned.\n",
      "          - `Y` is a feature array of shape (n_samples_Y, n_features), the pairwise\n",
      "            distances between `X` and `Y` is returned.\n",
      "        \n",
      "        - If `X` is a distance matrix, of shape (n_samples_X, n_samples_X), `metric`\n",
      "          should be 'precomputed'. `Y` is thus ignored and `X` is returned as is.\n",
      "        \n",
      "        If the input is a collection of non-numeric data (e.g. a list of strings or a\n",
      "        boolean array), a custom metric must be passed.\n",
      "        \n",
      "        This method provides a safe way to take a distance matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "        \n",
      "        - From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "          'manhattan', 'nan_euclidean']. All metrics support sparse matrix\n",
      "          inputs except 'nan_euclidean'.\n",
      "        \n",
      "        - From :mod:`scipy.spatial.distance`: ['braycurtis', 'canberra', 'chebyshev',\n",
      "          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n",
      "          'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
      "          'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'].\n",
      "          These metrics do not support sparse matrix inputs.\n",
      "        \n",
      "        .. note::\n",
      "            `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\n",
      "        \n",
      "        .. note::\n",
      "            `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).\n",
      "        \n",
      "        Note that in the case of 'cityblock', 'cosine' and 'euclidean' (which are\n",
      "        valid :mod:`scipy.spatial.distance` metrics), the scikit-learn implementation\n",
      "        will be used, which is faster and has support for sparse matrices (except\n",
      "        for 'cityblock'). For a verbose description of the metrics from\n",
      "        scikit-learn, see :func:`sklearn.metrics.pairwise.distance_metrics`\n",
      "        function.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "            The shape of the array should be (n_samples_X, n_samples_X) if\n",
      "            metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`scipy.spatial.distance.pdist` for its metric parameter, or\n",
      "            a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them\n",
      "            using multithreading.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "            The \"euclidean\" and \"cosine\" metrics rely heavily on BLAS which is already\n",
      "            multithreaded. So, increasing `n_jobs` would likely cause oversubscription\n",
      "            and quickly degrade performance.\n",
      "        \n",
      "        force_all_finite : bool or 'allow-nan', default=True\n",
      "            Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored\n",
      "            for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The\n",
      "            possibilities are:\n",
      "        \n",
      "            - True: Force all values of array to be finite.\n",
      "            - False: accepts np.inf, np.nan, pd.NA in array.\n",
      "            - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n",
      "              cannot be infinite.\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "               ``force_all_finite`` accepts the string ``'allow-nan'``.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Accepts `pd.NA` and converts it into `np.nan`.\n",
      "        \n",
      "            .. deprecated:: 1.6\n",
      "               `force_all_finite` was renamed to `ensure_all_finite` and will be removed\n",
      "               in 1.8.\n",
      "        \n",
      "        ensure_all_finite : bool or 'allow-nan', default=True\n",
      "            Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored\n",
      "            for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The\n",
      "            possibilities are:\n",
      "        \n",
      "            - True: Force all values of array to be finite.\n",
      "            - False: accepts np.inf, np.nan, pd.NA in array.\n",
      "            - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n",
      "              cannot be infinite.\n",
      "        \n",
      "            .. versionadded:: 1.6\n",
      "               `force_all_finite` was renamed to `ensure_all_finite`.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        D : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)\n",
      "            A distance matrix D such that D_{i, j} is the distance between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then D_{i, j} is the distance between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pairwise_distances_chunked : Performs the same calculation as this\n",
      "            function, but returns a generator of chunks of the distance matrix, in\n",
      "            order to limit memory usage.\n",
      "        sklearn.metrics.pairwise.paired_distances : Computes the distances between\n",
      "            corresponding elements of two arrays.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If metric is a callable, no restrictions are placed on `X` and `Y` dimensions.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import pairwise_distances\n",
      "        >>> X = [[0, 0, 0], [1, 1, 1]]\n",
      "        >>> Y = [[1, 0, 0], [1, 1, 0]]\n",
      "        >>> pairwise_distances(X, Y, metric='sqeuclidean')\n",
      "        array([[1., 2.],\n",
      "               [2., 1.]])\n",
      "    \n",
      "    pairwise_distances_argmin(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance).\n",
      "        \n",
      "        This is mostly equivalent to calling::\n",
      "        \n",
      "            pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        This function works with dense 2D arrays only.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n",
      "            Arrays containing points.\n",
      "        \n",
      "        axis : int, default=1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : str or callable, default=\"euclidean\"\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or :mod:`scipy.spatial.distance` can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan', 'nan_euclidean']\n",
      "        \n",
      "            - from :mod:`scipy.spatial.distance`: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for :mod:`scipy.spatial.distance` for details on these\n",
      "            metrics.\n",
      "        \n",
      "            .. note::\n",
      "               `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\n",
      "        \n",
      "            .. note::\n",
      "               `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).\n",
      "        \n",
      "        metric_kwargs : dict, default=None\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : numpy.ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pairwise_distances : Distances between every pair of samples of X and Y.\n",
      "        pairwise_distances_argmin_min : Same as `pairwise_distances_argmin` but also\n",
      "            returns the distances.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
      "        >>> X = [[0, 0, 0], [1, 1, 1]]\n",
      "        >>> Y = [[1, 0, 0], [1, 1, 0]]\n",
      "        >>> pairwise_distances_argmin(X, Y)\n",
      "        array([0, 1])\n",
      "    \n",
      "    pairwise_distances_argmin_min(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance). The minimal distances are\n",
      "        also returned.\n",
      "        \n",
      "        This is mostly equivalent to calling::\n",
      "        \n",
      "            (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),\n",
      "             pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        axis : int, default=1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or :mod:`scipy.spatial.distance` can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan', 'nan_euclidean']\n",
      "        \n",
      "            - from :mod:`scipy.spatial.distance`: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for :mod:`scipy.spatial.distance` for details on these\n",
      "            metrics.\n",
      "        \n",
      "            .. note::\n",
      "               `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.\n",
      "        \n",
      "            .. note::\n",
      "               `'matching'` has been removed in SciPy 1.9 (use `'hamming'` instead).\n",
      "        \n",
      "        metric_kwargs : dict, default=None\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        distances : ndarray\n",
      "            The array of minimum distances. `distances[i]` is the distance between\n",
      "            the i-th row in X and the argmin[i]-th row in Y.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pairwise_distances : Distances between every pair of samples of X and Y.\n",
      "        pairwise_distances_argmin : Same as `pairwise_distances_argmin_min` but only\n",
      "            returns the argmins.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import pairwise_distances_argmin_min\n",
      "        >>> X = [[0, 0, 0], [1, 1, 1]]\n",
      "        >>> Y = [[1, 0, 0], [1, 1, 0]]\n",
      "        >>> argmin, distances = pairwise_distances_argmin_min(X, Y)\n",
      "        >>> argmin\n",
      "        array([0, 1])\n",
      "        >>> distances\n",
      "        array([1., 1.])\n",
      "    \n",
      "    pairwise_distances_chunked(X, Y=None, *, reduce_func=None, metric='euclidean', n_jobs=None, working_memory=None, **kwds)\n",
      "        Generate a distance matrix chunk by chunk with optional reduction.\n",
      "        \n",
      "        In cases where not all of a pairwise distance matrix needs to be\n",
      "        stored at once, this is used to calculate pairwise distances in\n",
      "        ``working_memory``-sized chunks.  If ``reduce_func`` is given, it is\n",
      "        run on each chunk and its return values are concatenated into lists,\n",
      "        arrays or sparse matrices.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "            The shape the array should be (n_samples_X, n_samples_X) if\n",
      "            metric='precomputed' and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        reduce_func : callable, default=None\n",
      "            The function which is applied on each chunk of the distance matrix,\n",
      "            reducing it to needed values.  ``reduce_func(D_chunk, start)``\n",
      "            is called repeatedly, where ``D_chunk`` is a contiguous vertical\n",
      "            slice of the pairwise distance matrix, starting at row ``start``.\n",
      "            It should return one of: None; an array, a list, or a sparse matrix\n",
      "            of length ``D_chunk.shape[0]``; or a tuple of such objects.\n",
      "            Returning None is useful for in-place operations, rather than\n",
      "            reductions.\n",
      "        \n",
      "            If None, pairwise_distances_chunked returns a generator of vertical\n",
      "            chunks of the distance matrix.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`scipy.spatial.distance.pdist` for its metric parameter,\n",
      "            or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on\n",
      "            each pair of instances (rows) and the resulting value recorded.\n",
      "            The callable should take two arrays from X as input and return a\n",
      "            value indicating the distance between them.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by\n",
      "            breaking down the pairwise matrix into n_jobs even slices and\n",
      "            computing them in parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        working_memory : float, default=None\n",
      "            The sought maximum memory for temporary distance matrix chunks.\n",
      "            When None (default), the value of\n",
      "            ``sklearn.get_config()['working_memory']`` is used.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a :mod:`scipy.spatial.distance` metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        D_chunk : {ndarray, sparse matrix}\n",
      "            A contiguous slice of distance matrix, optionally processed by\n",
      "            ``reduce_func``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Without reduce_func:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import pairwise_distances_chunked\n",
      "        >>> X = np.random.RandomState(0).rand(5, 3)\n",
      "        >>> D_chunk = next(pairwise_distances_chunked(X))\n",
      "        >>> D_chunk\n",
      "        array([[0.   , 0.295, 0.417, 0.197, 0.572],\n",
      "               [0.295, 0.   , 0.576, 0.419, 0.764],\n",
      "               [0.417, 0.576, 0.   , 0.449, 0.903],\n",
      "               [0.197, 0.419, 0.449, 0.   , 0.512],\n",
      "               [0.572, 0.764, 0.903, 0.512, 0.   ]])\n",
      "        \n",
      "        Retrieve all neighbors and average distance within radius r:\n",
      "        \n",
      "        >>> r = .2\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r) for d in D_chunk]\n",
      "        ...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n",
      "        ...     return neigh, avg_dist\n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n",
      "        >>> neigh, avg_dist = next(gen)\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]\n",
      "        >>> avg_dist\n",
      "        array([0.039, 0.        , 0.        , 0.039, 0.        ])\n",
      "        \n",
      "        Where r is defined per sample, we need to make use of ``start``:\n",
      "        \n",
      "        >>> r = [.2, .4, .4, .3, .1]\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r[i])\n",
      "        ...              for i, d in enumerate(D_chunk, start)]\n",
      "        ...     return neigh\n",
      "        >>> neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]\n",
      "        \n",
      "        Force row-by-row generation by reducing ``working_memory``:\n",
      "        \n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func,\n",
      "        ...                                  working_memory=0)\n",
      "        >>> next(gen)\n",
      "        [array([0, 3])]\n",
      "        >>> next(gen)\n",
      "        [array([0, 1])]\n",
      "    \n",
      "    pairwise_kernels(X, Y=None, metric='linear', *, filter_params=False, n_jobs=None, **kwds)\n",
      "        Compute the kernel between arrays X and optional array Y.\n",
      "        \n",
      "        This function takes one or two feature arrays or a kernel matrix, and returns\n",
      "        a kernel matrix.\n",
      "        \n",
      "        - If `X` is a feature array, of shape (n_samples_X, n_features), and:\n",
      "        \n",
      "          - `Y` is `None` and `metric` is not 'precomputed', the pairwise kernels\n",
      "            between `X` and itself are returned.\n",
      "          - `Y` is a feature array of shape (n_samples_Y, n_features), the pairwise\n",
      "            kernels between `X` and `Y` is returned.\n",
      "        \n",
      "        - If `X` is a kernel matrix, of shape (n_samples_X, n_samples_X), `metric`\n",
      "          should be 'precomputed'. `Y` is thus ignored and `X` is returned as is.\n",
      "        \n",
      "        This method provides a safe way to take a kernel matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "            ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',\n",
      "            'laplacian', 'sigmoid', 'cosine']\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}  of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise kernels between samples, or a feature array.\n",
      "            The shape of the array should be (n_samples_X, n_samples_X) if\n",
      "            metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), default=None\n",
      "            A second feature array only if X has shape (n_samples_X, n_features).\n",
      "        \n",
      "        metric : str or callable, default=\"linear\"\n",
      "            The metric to use when calculating kernel between instances in a\n",
      "            feature array. If metric is a string, it must be one of the metrics\n",
      "            in ``pairwise.PAIRWISE_KERNEL_FUNCTIONS``.\n",
      "            If metric is \"precomputed\", X is assumed to be a kernel matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two rows from X as input and return the corresponding\n",
      "            kernel value as a single number. This means that callables from\n",
      "            :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on\n",
      "            matrices, not single samples. Use the string identifying the kernel\n",
      "            instead.\n",
      "        \n",
      "        filter_params : bool, default=False\n",
      "            Whether to filter invalid parameters or not.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them\n",
      "            using multithreading.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the kernel function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : ndarray of shape (n_samples_X, n_samples_X) or (n_samples_X, n_samples_Y)\n",
      "            A kernel matrix K such that K_{i, j} is the kernel between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then K_{i, j} is the kernel between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If metric is a callable, no restrictions are placed on `X` and `Y` dimensions.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import pairwise_kernels\n",
      "        >>> X = [[0, 0, 0], [1, 1, 1]]\n",
      "        >>> Y = [[1, 0, 0], [1, 1, 0]]\n",
      "        >>> pairwise_kernels(X, Y, metric='linear')\n",
      "        array([[0., 0.],\n",
      "               [1., 2.]])\n",
      "    \n",
      "    precision_recall_curve(y_true, y_score, *, pos_label=None, sample_weight=None, drop_intermediate=False)\n",
      "        Compute precision-recall pairs for different probability thresholds.\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The last precision and recall values are 1. and 0. respectively and do not\n",
      "        have a corresponding threshold. This ensures that the graph starts on the\n",
      "        y axis.\n",
      "        \n",
      "        The first precision and recall values are precision=class balance and recall=1.0\n",
      "        which corresponds to a classifier that always predicts the positive class.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, or non-thresholded measure of decisions (as returned by\n",
      "            `decision_function` on some classifiers).\n",
      "            For :term:`decision_function` scores, values greater than or equal to\n",
      "            zero should indicate the positive class.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        drop_intermediate : bool, default=False\n",
      "            Whether to drop some suboptimal thresholds which would not appear\n",
      "            on a plotted precision-recall curve. This is useful in order to create\n",
      "            lighter precision-recall curves.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : ndarray of shape (n_thresholds + 1,)\n",
      "            Precision values such that element i is the precision of\n",
      "            predictions with score >= thresholds[i] and the last element is 1.\n",
      "        \n",
      "        recall : ndarray of shape (n_thresholds + 1,)\n",
      "            Decreasing recall values such that element i is the recall of\n",
      "            predictions with score >= thresholds[i] and the last element is 0.\n",
      "        \n",
      "        thresholds : ndarray of shape (n_thresholds,)\n",
      "            Increasing thresholds on the decision function used to compute\n",
      "            precision and recall where `n_thresholds = len(np.unique(y_score))`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        PrecisionRecallDisplay.from_estimator : Plot Precision Recall Curve given\n",
      "            a binary classifier.\n",
      "        PrecisionRecallDisplay.from_predictions : Plot Precision Recall Curve\n",
      "            using predictions from a binary classifier.\n",
      "        average_precision_score : Compute average precision from prediction scores.\n",
      "        det_curve: Compute error rates for different probability thresholds.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_curve\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> precision, recall, thresholds = precision_recall_curve(\n",
      "        ...     y_true, y_scores)\n",
      "        >>> precision\n",
      "        array([0.5       , 0.66666667, 0.5       , 1.        , 1.        ])\n",
      "        >>> recall\n",
      "        array([1. , 1. , 0.5, 0.5, 0. ])\n",
      "        >>> thresholds\n",
      "        array([0.1 , 0.35, 0.4 , 0.8 ])\n",
      "    \n",
      "    precision_recall_fscore_support(y_true, y_pred, *, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')\n",
      "        Compute precision, recall, F-measure and support for each class.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label a negative sample as\n",
      "        positive.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The F-beta score can be interpreted as a weighted harmonic mean of\n",
      "        the precision and recall, where an F-beta score reaches its best\n",
      "        value at 1 and worst score at 0.\n",
      "        \n",
      "        The F-beta score weights recall more than precision by a factor of\n",
      "        ``beta``. ``beta == 1.0`` means recall and precision are equally important.\n",
      "        \n",
      "        The support is the number of occurrences of each class in ``y_true``.\n",
      "        \n",
      "        Support beyond term:`binary` targets is achieved by treating :term:`multiclass`\n",
      "        and :term:`multilabel` data as a collection of binary problems, one for each\n",
      "        label. For the :term:`binary` case, setting `average='binary'` will return\n",
      "        metrics for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored\n",
      "        and metrics for both classes are computed, then averaged or both returned (when\n",
      "        `average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,\n",
      "        metrics for all `labels` are either returned or averaged depending on the `average`\n",
      "        parameter. Use `labels` specify the set of labels to calculate metrics for.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            The strength of recall versus precision in the F-score.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when `average != 'binary'`, and their\n",
      "            order if `average is None`. Labels present in the data can be\n",
      "            excluded, for example in multiclass classification to exclude a \"negative\n",
      "            class\". Labels not present in the data can be included and will be\n",
      "            \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n",
      "            By default, all labels in `y_true` and `y_pred` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=1\n",
      "            The class to report if `average='binary'` and the data is binary,\n",
      "            otherwise this parameter is ignored.\n",
      "            For multiclass or multilabel targets, set `labels=[pos_label]` and\n",
      "            `average != 'binary'` to report metrics for one label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the metrics for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        warn_for : list, tuple or set, for internal use\n",
      "            This determines which warnings will be made in the case that this\n",
      "            function is being used to return only one of its metrics.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division:\n",
      "        \n",
      "            - recall: when there are no positive labels\n",
      "            - precision: when there are no positive predictions\n",
      "            - f-score: both\n",
      "        \n",
      "            Notes:\n",
      "        \n",
      "            - If set to \"warn\", this acts like 0, but a warning is also raised.\n",
      "            - If set to `np.nan`, such values will be excluded from the average.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "               `np.nan` option was added.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            Precision score.\n",
      "        \n",
      "        recall : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            Recall score.\n",
      "        \n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            F-beta score.\n",
      "        \n",
      "        support : None (if average is not None) or array of int, shape =        [n_unique_labels]\n",
      "            The number of occurrences of each label in ``y_true``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision is undefined.\n",
      "        When ``true positive + false negative == 0``, recall is undefined. When\n",
      "        ``true positive + false negative + false positive == 0``, f-score is\n",
      "        undefined. In such cases, by default the metric will be set to 0, and\n",
      "        ``UndefinedMetricWarning`` will be raised. This behavior can be modified\n",
      "        with ``zero_division``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Precision and recall\n",
      "               <https://en.wikipedia.org/wiki/Precision_and_recall>`_.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        .. [3] `Discriminative Methods for Multi-labeled Classification Advances\n",
      "               in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n",
      "               Godbole, Sunita Sarawagi\n",
      "               <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_fscore_support\n",
      "        >>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
      "        >>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
      "        (0.222, 0.333, 0.267, None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
      "        (0.33, 0.33, 0.33, None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
      "        (0.222, 0.333, 0.267, None)\n",
      "        \n",
      "        It is possible to compute per-label precisions, recalls, F1-scores and\n",
      "        supports instead of averaging:\n",
      "        \n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average=None,\n",
      "        ... labels=['pig', 'dog', 'cat'])\n",
      "        (array([0.        , 0.        , 0.66]),\n",
      "         array([0., 0., 1.]), array([0. , 0. , 0.8]),\n",
      "         array([2, 2, 2]))\n",
      "    \n",
      "    precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the precision.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Support beyond term:`binary` targets is achieved by treating :term:`multiclass`\n",
      "        and :term:`multilabel` data as a collection of binary problems, one for each\n",
      "        label. For the :term:`binary` case, setting `average='binary'` will return\n",
      "        precision for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored\n",
      "        and precision for both classes are computed, then averaged or both returned (when\n",
      "        `average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,\n",
      "        precision for all `labels` are either returned or averaged depending on the\n",
      "        `average` parameter. Use `labels` specify the set of labels to calculate precision\n",
      "        for.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when `average != 'binary'`, and their\n",
      "            order if `average is None`. Labels present in the data can be\n",
      "            excluded, for example in multiclass classification to exclude a \"negative\n",
      "            class\". Labels not present in the data can be included and will be\n",
      "            \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n",
      "            By default, all labels in `y_true` and `y_pred` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=1\n",
      "            The class to report if `average='binary'` and the data is binary,\n",
      "            otherwise this parameter is ignored.\n",
      "            For multiclass or multilabel targets, set `labels=[pos_label]` and\n",
      "            `average != 'binary'` to report metrics for one label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the metrics for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division.\n",
      "        \n",
      "            Notes:\n",
      "        \n",
      "            - If set to \"warn\", this acts like 0, but a warning is also raised.\n",
      "            - If set to `np.nan`, such values will be excluded from the average.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "               `np.nan` option was added.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float of shape                 (n_unique_labels,)\n",
      "            Precision of the positive class in binary classification or weighted\n",
      "            average of the precision of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support : Compute precision, recall, F-measure and\n",
      "            support for each class.\n",
      "        recall_score :  Compute the ratio ``tp / (tp + fn)`` where ``tp`` is the\n",
      "            number of true positives and ``fn`` the number of false negatives.\n",
      "        PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n",
      "            an estimator and some data.\n",
      "        PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n",
      "            binary class predictions.\n",
      "        multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "            sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision returns 0 and\n",
      "        raises ``UndefinedMetricWarning``. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> precision_score(y_true, y_pred, average='macro')\n",
      "        0.22\n",
      "        >>> precision_score(y_true, y_pred, average='micro')\n",
      "        0.33\n",
      "        >>> precision_score(y_true, y_pred, average='weighted')\n",
      "        0.22\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.66, 0.        , 0.        ])\n",
      "        >>> y_pred = [0, 0, 0, 0, 0, 0]\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.33, 0.        , 0.        ])\n",
      "        >>> precision_score(y_true, y_pred, average=None, zero_division=1)\n",
      "        array([0.33, 1.        , 1.        ])\n",
      "        >>> precision_score(y_true, y_pred, average=None, zero_division=np.nan)\n",
      "        array([0.33,        nan,        nan])\n",
      "        \n",
      "        >>> # multilabel classification\n",
      "        >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "        >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 1. , 1. ])\n",
      "    \n",
      "    r2_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', force_finite=True)\n",
      "        :math:`R^2` (coefficient of determination) regression score function.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the\n",
      "        model can be arbitrarily worse). In the general case when the true y is\n",
      "        non-constant, a constant model that always predicts the average y\n",
      "        disregarding the input features would get a :math:`R^2` score of 0.0.\n",
      "        \n",
      "        In the particular case when ``y_true`` is constant, the :math:`R^2` score\n",
      "        is not finite: it is either ``NaN`` (perfect predictions) or ``-Inf``\n",
      "        (imperfect predictions). To prevent such non-finite numbers to pollute\n",
      "        higher-level experiments such as a grid search cross-validation, by default\n",
      "        these cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect\n",
      "        predictions) respectively. You can set ``force_finite`` to ``False`` to\n",
      "        prevent this fix from happening.\n",
      "        \n",
      "        Note: when the prediction residuals have zero mean, the :math:`R^2` score\n",
      "        is identical to the\n",
      "        :func:`Explained Variance score <explained_variance_score>`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <r2_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'\n",
      "        \n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "            Default is \"uniform_average\".\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "            .. versionchanged:: 0.19\n",
      "                Default value of multioutput is 'uniform_average'.\n",
      "        \n",
      "        force_finite : bool, default=True\n",
      "            Flag indicating if ``NaN`` and ``-Inf`` scores resulting from constant\n",
      "            data should be replaced with real numbers (``1.0`` if prediction is\n",
      "            perfect, ``0.0`` otherwise). Default is ``True``, a convenient setting\n",
      "            for hyperparameters' search procedures (e.g. grid search\n",
      "            cross-validation).\n",
      "        \n",
      "            .. versionadded:: 1.1\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : float or ndarray of floats\n",
      "            The :math:`R^2` score or ndarray of scores if 'multioutput' is\n",
      "            'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Unlike most other scores, :math:`R^2` score may be negative (it need not\n",
      "        actually be the square of a quantity R).\n",
      "        \n",
      "        This metric is not well-defined for single samples and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Coefficient of determination\n",
      "                <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import r2_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.948...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> r2_score(y_true, y_pred,\n",
      "        ...          multioutput='variance_weighted')\n",
      "        0.938...\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [1, 2, 3]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [2, 2, 2]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [3, 2, 1]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        -3.0\n",
      "        >>> y_true = [-2, -2, -2]\n",
      "        >>> y_pred = [-2, -2, -2]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> r2_score(y_true, y_pred, force_finite=False)\n",
      "        nan\n",
      "        >>> y_true = [-2, -2, -2]\n",
      "        >>> y_pred = [-2, -2, -2 + 1e-8]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> r2_score(y_true, y_pred, force_finite=False)\n",
      "        -inf\n",
      "    \n",
      "    rand_score(labels_true, labels_pred)\n",
      "        Rand index.\n",
      "        \n",
      "        The Rand Index computes a similarity measure between two clusterings\n",
      "        by considering all pairs of samples and counting pairs that are\n",
      "        assigned in the same or different clusters in the predicted and\n",
      "        true clusterings [1]_ [2]_.\n",
      "        \n",
      "        The raw RI score [3]_ is:\n",
      "        \n",
      "        .. code-block:: text\n",
      "        \n",
      "            RI = (number of agreeing pairs) / (number of pairs)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <rand_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=integral\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=integral\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        RI : float\n",
      "           Similarity score between 0.0 and 1.0, inclusive, 1.0 stands for\n",
      "           perfect match.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_rand_score: Adjusted Rand Score.\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] :doi:`Hubert, L., Arabie, P. \"Comparing partitions.\"\n",
      "           Journal of Classification 2, 193–218 (1985).\n",
      "           <10.1007/BF01908075>`.\n",
      "        \n",
      "        .. [2] `Wikipedia: Simple Matching Coefficient\n",
      "            <https://en.wikipedia.org/wiki/Simple_matching_coefficient>`_\n",
      "        \n",
      "        .. [3] `Wikipedia: Rand Index <https://en.wikipedia.org/wiki/Rand_index>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have a score of 1 even\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import rand_score\n",
      "          >>> rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may not always be pure, hence penalized:\n",
      "        \n",
      "          >>> rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          0.83\n",
      "    \n",
      "    recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the recall.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Support beyond term:`binary` targets is achieved by treating :term:`multiclass`\n",
      "        and :term:`multilabel` data as a collection of binary problems, one for each\n",
      "        label. For the :term:`binary` case, setting `average='binary'` will return\n",
      "        recall for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored\n",
      "        and recall for both classes are computed then averaged or both returned (when\n",
      "        `average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,\n",
      "        recall for all `labels` are either returned or averaged depending on the `average`\n",
      "        parameter. Use `labels` specify the set of labels to calculate recall for.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when `average != 'binary'`, and their\n",
      "            order if `average is None`. Labels present in the data can be\n",
      "            excluded, for example in multiclass classification to exclude a \"negative\n",
      "            class\". Labels not present in the data can be included and will be\n",
      "            \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n",
      "            By default, all labels in `y_true` and `y_pred` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=1\n",
      "            The class to report if `average='binary'` and the data is binary,\n",
      "            otherwise this parameter is ignored.\n",
      "            For multiclass or multilabel targets, set `labels=[pos_label]` and\n",
      "            `average != 'binary'` to report metrics for one label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the metrics for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall. Weighted recall\n",
      "                is equal to accuracy.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division.\n",
      "        \n",
      "            Notes:\n",
      "        \n",
      "            - If set to \"warn\", this acts like 0, but a warning is also raised.\n",
      "            - If set to `np.nan`, such values will be excluded from the average.\n",
      "        \n",
      "            .. versionadded:: 1.3\n",
      "               `np.nan` option was added.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        recall : float (if average is not None) or array of float of shape              (n_unique_labels,)\n",
      "            Recall of the positive class in binary classification or weighted\n",
      "            average of the recall of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support : Compute precision, recall, F-measure and\n",
      "            support for each class.\n",
      "        precision_score : Compute the ratio ``tp / (tp + fp)`` where ``tp`` is the\n",
      "            number of true positives and ``fp`` the number of false positives.\n",
      "        balanced_accuracy_score : Compute balanced accuracy to deal with imbalanced\n",
      "            datasets.\n",
      "        multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
      "            sample.\n",
      "        PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n",
      "            an estimator and some data.\n",
      "        PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n",
      "            binary class predictions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false negative == 0``, recall returns 0 and raises\n",
      "        ``UndefinedMetricWarning``. This behavior can be modified with\n",
      "        ``zero_division``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import recall_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> recall_score(y_true, y_pred, average='macro')\n",
      "        0.33\n",
      "        >>> recall_score(y_true, y_pred, average='micro')\n",
      "        0.33\n",
      "        >>> recall_score(y_true, y_pred, average='weighted')\n",
      "        0.33\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([1., 0., 0.])\n",
      "        >>> y_true = [0, 0, 0, 0, 0, 0]\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 0. , 0. ])\n",
      "        >>> recall_score(y_true, y_pred, average=None, zero_division=1)\n",
      "        array([0.5, 1. , 1. ])\n",
      "        >>> recall_score(y_true, y_pred, average=None, zero_division=np.nan)\n",
      "        array([0.5, nan, nan])\n",
      "        \n",
      "        >>> # multilabel classification\n",
      "        >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
      "        >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([1. , 1. , 0.5])\n",
      "    \n",
      "    roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)\n",
      "        Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\n",
      "        \n",
      "        Note: this implementation can be used with binary, multiclass and\n",
      "        multilabel classification, but some restrictions apply (see Parameters).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            True labels or binary label indicators. The binary and multiclass cases\n",
      "            expect labels with shape (n_samples,) while the multilabel case expects\n",
      "            binary label indicators with shape (n_samples, n_classes).\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores.\n",
      "        \n",
      "            * In the binary case, it corresponds to an array of shape\n",
      "              `(n_samples,)`. Both probability estimates and non-thresholded\n",
      "              decision values can be provided. The probability estimates correspond\n",
      "              to the **probability of the class with the greater label**,\n",
      "              i.e. `estimator.classes_[1]` and thus\n",
      "              `estimator.predict_proba(X, y)[:, 1]`. The decision values\n",
      "              corresponds to the output of `estimator.decision_function(X, y)`.\n",
      "              See more information in the :ref:`User guide <roc_auc_binary>`;\n",
      "            * In the multiclass case, it corresponds to an array of shape\n",
      "              `(n_samples, n_classes)` of probability estimates provided by the\n",
      "              `predict_proba` method. The probability estimates **must**\n",
      "              sum to 1 across the possible classes. In addition, the order of the\n",
      "              class scores must correspond to the order of ``labels``,\n",
      "              if provided, or else to the numerical or lexicographical order of\n",
      "              the labels in ``y_true``. See more information in the\n",
      "              :ref:`User guide <roc_auc_multiclass>`;\n",
      "            * In the multilabel case, it corresponds to an array of shape\n",
      "              `(n_samples, n_classes)`. Probability estimates are provided by the\n",
      "              `predict_proba` method and the non-thresholded decision values by\n",
      "              the `decision_function` method. The probability estimates correspond\n",
      "              to the **probability of the class with the greater label for each\n",
      "              output** of the classifier. See more information in the\n",
      "              :ref:`User guide <roc_auc_multilabel>`.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted'} or None,             default='macro'\n",
      "            If ``None``, the scores for each class are returned.\n",
      "            Otherwise, this determines the type of averaging performed on the data.\n",
      "            Note: multiclass ROC AUC currently only handles the 'macro' and\n",
      "            'weighted' averages. For multiclass targets, `average=None` is only\n",
      "            implemented for `multi_class='ovr'` and `average='micro'` is only\n",
      "            implemented for `multi_class='ovr'`.\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        max_fpr : float > 0 and <= 1, default=None\n",
      "            If not ``None``, the standardized partial AUC [2]_ over the range\n",
      "            [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,\n",
      "            should be either equal to ``None`` or ``1.0`` as AUC ROC partial\n",
      "            computation currently is not supported for multiclass.\n",
      "        \n",
      "        multi_class : {'raise', 'ovr', 'ovo'}, default='raise'\n",
      "            Only used for multiclass targets. Determines the type of configuration\n",
      "            to use. The default value raises an error, so either\n",
      "            ``'ovr'`` or ``'ovo'`` must be passed explicitly.\n",
      "        \n",
      "            ``'ovr'``:\n",
      "                Stands for One-vs-rest. Computes the AUC of each class\n",
      "                against the rest [3]_ [4]_. This\n",
      "                treats the multiclass case in the same way as the multilabel case.\n",
      "                Sensitive to class imbalance even when ``average == 'macro'``,\n",
      "                because class imbalance affects the composition of each of the\n",
      "                'rest' groupings.\n",
      "            ``'ovo'``:\n",
      "                Stands for One-vs-one. Computes the average AUC of all\n",
      "                possible pairwise combinations of classes [5]_.\n",
      "                Insensitive to class imbalance when\n",
      "                ``average == 'macro'``.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            Only used for multiclass targets. List of labels that index the\n",
      "            classes in ``y_score``. If ``None``, the numerical or lexicographical\n",
      "            order of the labels in ``y_true`` is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "            Area Under the Curve score.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        average_precision_score : Area under the precision-recall curve.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given an estimator and some data.\n",
      "        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given the true and predicted values.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The Gini Coefficient is a summary measure of the ranking ability of binary\n",
      "        classifiers. It is expressed using the area under of the ROC as follows:\n",
      "        \n",
      "        G = 2 * AUC - 1\n",
      "        \n",
      "        Where G is the Gini coefficient and AUC is the ROC-AUC score. This normalisation\n",
      "        will ensure that random guessing will yield a score of 0 in expectation, and it is\n",
      "        upper bounded by 1.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] `Analyzing a portion of the ROC curve. McClish, 1989\n",
      "                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_\n",
      "        \n",
      "        .. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving\n",
      "               probability estimation trees (Section 6.2), CeDER Working Paper\n",
      "               #IS-00-04, Stern School of Business, New York University.\n",
      "        \n",
      "        .. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern\n",
      "                Recognition Letters, 27(8), 861-874.\n",
      "                <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_\n",
      "        \n",
      "        .. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area\n",
      "                Under the ROC Curve for Multiple Class Classification Problems.\n",
      "                Machine Learning, 45(2), 171-186.\n",
      "                <http://link.springer.com/article/10.1023/A:1010920819831>`_\n",
      "        .. [6] `Wikipedia entry for the Gini coefficient\n",
      "                <https://en.wikipedia.org/wiki/Gini_coefficient>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Binary case:\n",
      "        \n",
      "        >>> from sklearn.datasets import load_breast_cancer\n",
      "        >>> from sklearn.linear_model import LogisticRegression\n",
      "        >>> from sklearn.metrics import roc_auc_score\n",
      "        >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "        >>> clf = LogisticRegression(solver=\"newton-cholesky\", random_state=0).fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])\n",
      "        0.99\n",
      "        >>> roc_auc_score(y, clf.decision_function(X))\n",
      "        0.99\n",
      "        \n",
      "        Multiclass case:\n",
      "        \n",
      "        >>> from sklearn.datasets import load_iris\n",
      "        >>> X, y = load_iris(return_X_y=True)\n",
      "        >>> clf = LogisticRegression(solver=\"newton-cholesky\").fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')\n",
      "        0.99\n",
      "        \n",
      "        Multilabel case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.datasets import make_multilabel_classification\n",
      "        >>> from sklearn.multioutput import MultiOutputClassifier\n",
      "        >>> X, y = make_multilabel_classification(random_state=0)\n",
      "        >>> clf = MultiOutputClassifier(clf).fit(X, y)\n",
      "        >>> # get a list of n_output containing probability arrays of shape\n",
      "        >>> # (n_samples, n_classes)\n",
      "        >>> y_score = clf.predict_proba(X)\n",
      "        >>> # extract the positive columns for each output\n",
      "        >>> y_score = np.transpose([score[:, 1] for score in y_score])\n",
      "        >>> roc_auc_score(y, y_score, average=None)\n",
      "        array([0.828, 0.852, 0.94, 0.869, 0.95])\n",
      "        >>> from sklearn.linear_model import RidgeClassifierCV\n",
      "        >>> clf = RidgeClassifierCV().fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.decision_function(X), average=None)\n",
      "        array([0.82, 0.847, 0.93, 0.872, 0.944])\n",
      "    \n",
      "    roc_curve(y_true, y_score, *, pos_label=None, sample_weight=None, drop_intermediate=True)\n",
      "        Compute Receiver operating characteristic (ROC).\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "            For :term:`decision_function` scores, values greater than or equal to\n",
      "            zero should indicate the positive class.\n",
      "        \n",
      "        pos_label : int, float, bool or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        drop_intermediate : bool, default=True\n",
      "            Whether to drop thresholds where the resulting point is collinear with\n",
      "            its neighbors in ROC space. This has no effect on the ROC AUC or visual\n",
      "            shape of the curve, but reduces the number of plotted points.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               parameter *drop_intermediate*.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fpr : ndarray of shape (>2,)\n",
      "            Increasing false positive rates such that element i is the false\n",
      "            positive rate of predictions with score >= `thresholds[i]`.\n",
      "        \n",
      "        tpr : ndarray of shape (>2,)\n",
      "            Increasing true positive rates such that element `i` is the true\n",
      "            positive rate of predictions with score >= `thresholds[i]`.\n",
      "        \n",
      "        thresholds : ndarray of shape (n_thresholds,)\n",
      "            Decreasing thresholds on the decision function used to compute\n",
      "            fpr and tpr. The first threshold is set to `np.inf`.\n",
      "        \n",
      "            .. versionchanged:: 1.3\n",
      "               An arbitrary threshold at infinity (stored in `thresholds[0]`) is\n",
      "               added to represent a classifier that always predicts the negative\n",
      "               class, i.e. `fpr=0` and `tpr=0`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        RocCurveDisplay.from_estimator : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given an estimator and some data.\n",
      "        RocCurveDisplay.from_predictions : Plot Receiver Operating Characteristic\n",
      "            (ROC) curve given the true and predicted values.\n",
      "        det_curve: Compute error rates for different probability thresholds.\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Since the thresholds are sorted from low to high values, they\n",
      "        are reversed upon returning them to ensure they correspond to both ``fpr``\n",
      "        and ``tpr``, which are sorted in reversed order during their calculation.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n",
      "               Letters, 2006, 27(8):861-874.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n",
      "        >>> fpr\n",
      "        array([0. , 0. , 0.5, 0.5, 1. ])\n",
      "        >>> tpr\n",
      "        array([0. , 0.5, 0.5, 1. , 1. ])\n",
      "        >>> thresholds\n",
      "        array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\n",
      "    \n",
      "    root_mean_squared_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Root mean squared error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_error>`.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import root_mean_squared_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> root_mean_squared_error(y_true, y_pred)\n",
      "        0.612...\n",
      "        >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
      "        >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
      "        >>> root_mean_squared_error(y_true, y_pred)\n",
      "        0.822...\n",
      "    \n",
      "    root_mean_squared_log_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Root mean squared logarithmic error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_log_error>`.\n",
      "        \n",
      "        .. versionadded:: 1.4\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "        \n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors when the input is of multioutput\n",
      "                format.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import root_mean_squared_log_error\n",
      "        >>> y_true = [3, 5, 2.5, 7]\n",
      "        >>> y_pred = [2.5, 5, 4, 8]\n",
      "        >>> root_mean_squared_log_error(y_true, y_pred)\n",
      "        0.199...\n",
      "    \n",
      "    silhouette_samples(X, labels, *, metric='euclidean', **kwds)\n",
      "        Compute the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The Silhouette Coefficient is a measure of how well samples are clustered\n",
      "        with samples that are similar to themselves. Clustering models with a high\n",
      "        Silhouette Coefficient are said to be dense, where samples in the same\n",
      "        cluster are similar to each other, and well separated, where samples in\n",
      "        different clusters are not very similar to each other.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is 2 ``<= n_labels <= n_samples - 1``.\n",
      "        \n",
      "        This function returns the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n",
      "            An array of pairwise distances between samples, or a feature array. If\n",
      "            a sparse matrix is provided, CSR format should be favoured avoiding\n",
      "            an additional copy.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Label values for each sample.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`~sklearn.metrics.pairwise_distances`.\n",
      "            If ``X`` is the distance array itself, use \"precomputed\" as the metric.\n",
      "            Precomputed distance matrices must have 0 along the diagonal.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a ``scipy.spatial.distance`` metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : array-like of shape (n_samples,)\n",
      "            Silhouette Coefficients for each sample.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import silhouette_samples\n",
      "        >>> from sklearn.datasets import make_blobs\n",
      "        >>> from sklearn.cluster import KMeans\n",
      "        >>> X, y = make_blobs(n_samples=50, random_state=42)\n",
      "        >>> kmeans = KMeans(n_clusters=3, random_state=42)\n",
      "        >>> labels = kmeans.fit_predict(X)\n",
      "        >>> silhouette_samples(X, labels)\n",
      "        array([...])\n",
      "    \n",
      "    silhouette_score(X, labels, *, metric='euclidean', sample_size=None, random_state=None, **kwds)\n",
      "        Compute the mean Silhouette Coefficient of all samples.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.  To clarify, ``b`` is the distance between a sample and the nearest\n",
      "        cluster that the sample is not a part of.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is ``2 <= n_labels <= n_samples - 1``.\n",
      "        \n",
      "        This function returns the mean Silhouette Coefficient over all samples.\n",
      "        To obtain the values for each sample, use :func:`silhouette_samples`.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters. Negative values generally indicate that a sample has\n",
      "        been assigned to the wrong cluster, as a different cluster is more similar.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n",
      "            An array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`~sklearn.metrics.pairwise_distances`. If ``X`` is\n",
      "            the distance array itself, use ``metric=\"precomputed\"``.\n",
      "        \n",
      "        sample_size : int, default=None\n",
      "            The size of the sample to use when computing the Silhouette Coefficient\n",
      "            on a random subset of the data.\n",
      "            If ``sample_size is None``, no sampling is used.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for selecting a subset of samples.\n",
      "            Used when ``sample_size is not None``.\n",
      "            Pass an int for reproducible results across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : float\n",
      "            Mean Silhouette Coefficient for all samples.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "               <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets import make_blobs\n",
      "        >>> from sklearn.cluster import KMeans\n",
      "        >>> from sklearn.metrics import silhouette_score\n",
      "        >>> X, y = make_blobs(random_state=42)\n",
      "        >>> kmeans = KMeans(n_clusters=2, random_state=42)\n",
      "        >>> silhouette_score(X, kmeans.fit_predict(X))\n",
      "        0.49...\n",
      "    \n",
      "    top_k_accuracy_score(y_true, y_score, *, k=2, normalize=True, sample_weight=None, labels=None)\n",
      "        Top-k Accuracy classification score.\n",
      "        \n",
      "        This metric computes the number of times where the correct label is among\n",
      "        the top `k` labels predicted (ranked by predicted scores). Note that the\n",
      "        multilabel case isn't covered here.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <top_k_accuracy_score>`\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            True labels.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores. These can be either probability estimates or\n",
      "            non-thresholded decision values (as returned by\n",
      "            :term:`decision_function` on some classifiers).\n",
      "            The binary case expects scores with shape (n_samples,) while the\n",
      "            multiclass case expects scores with shape (n_samples, n_classes).\n",
      "            In the multiclass case, the order of the class scores must\n",
      "            correspond to the order of ``labels``, if provided, or else to\n",
      "            the numerical or lexicographical order of the labels in ``y_true``.\n",
      "            If ``y_true`` does not contain all the labels, ``labels`` must be\n",
      "            provided.\n",
      "        \n",
      "        k : int, default=2\n",
      "            Number of most likely outcomes considered to find the correct label.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If `True`, return the fraction of correctly classified samples.\n",
      "            Otherwise, return the number of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights. If `None`, all samples are given the same weight.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            Multiclass only. List of labels that index the classes in ``y_score``.\n",
      "            If ``None``, the numerical or lexicographical order of the labels in\n",
      "            ``y_true`` is used. If ``y_true`` does not contain all the labels,\n",
      "            ``labels`` must be provided.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The top-k accuracy score. The best performance is 1 with\n",
      "            `normalize == True` and the number of samples with\n",
      "            `normalize == False`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score : Compute the accuracy score. By default, the function will\n",
      "            return the fraction of correct predictions divided by the total number\n",
      "            of predictions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In cases where two or more labels are assigned equal predicted scores,\n",
      "        the labels with the highest indices will be chosen first. This might\n",
      "        impact the result if the correct label falls after the threshold because\n",
      "        of that.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import top_k_accuracy_score\n",
      "        >>> y_true = np.array([0, 1, 2, 2])\n",
      "        >>> y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n",
      "        ...                     [0.3, 0.4, 0.2],  # 1 is in top 2\n",
      "        ...                     [0.2, 0.4, 0.3],  # 2 is in top 2\n",
      "        ...                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n",
      "        >>> top_k_accuracy_score(y_true, y_score, k=2)\n",
      "        0.75\n",
      "        >>> # Not normalizing gives the number of \"correctly\" classified samples\n",
      "        >>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\n",
      "        3.0\n",
      "    \n",
      "    v_measure_score(labels_true, labels_pred, *, beta=1.0)\n",
      "        V-measure cluster labeling given a ground truth.\n",
      "        \n",
      "        This score is identical to :func:`normalized_mutual_info_score` with\n",
      "        the ``'arithmetic'`` option for averaging.\n",
      "        \n",
      "        The V-measure is the harmonic mean between homogeneity and completeness::\n",
      "        \n",
      "            v = (1 + beta) * homogeneity * completeness\n",
      "                 / (beta * homogeneity + completeness)\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,)\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        v_measure : float\n",
      "           Score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score : Homogeneity metric of cluster labeling.\n",
      "        completeness_score : Completeness metric of cluster labeling.\n",
      "        normalized_mutual_info_score : Normalized Mutual Information.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfect labelings are both homogeneous and complete, hence have score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import v_measure_score\n",
      "          >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but not homogeneous, hence penalized::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\n",
      "          0.8\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          0.67\n",
      "        \n",
      "        Labelings that have pure clusters with members coming from the same\n",
      "        classes are homogeneous but un-necessary splits harm completeness\n",
      "        and thus penalize V-measure as well::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          0.8\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          0.67\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally incomplete, hence the V-Measure is null::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          0.0\n",
      "        \n",
      "        Clusters that include samples from totally different classes totally\n",
      "        destroy the homogeneity of the labeling, hence::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          0.0\n",
      "    \n",
      "    zero_one_loss(y_true, y_pred, *, normalize=True, sample_weight=None)\n",
      "        Zero-one classification loss.\n",
      "        \n",
      "        If normalize is ``True``, return the fraction of misclassifications\n",
      "        (float), else it returns the number of misclassifications (int). The best\n",
      "        performance is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <zero_one_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If ``False``, return the number of misclassifications.\n",
      "            Otherwise, return the fraction of misclassifications.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int,\n",
      "            If ``normalize == True``, return the fraction of misclassifications\n",
      "            (float), else it returns the number of misclassifications (int).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score : Compute the accuracy score. By default, the function will\n",
      "            return the fraction of correct predictions divided by the total number\n",
      "            of predictions.\n",
      "        hamming_loss : Compute the average Hamming loss or Hamming distance between\n",
      "            two sets of samples.\n",
      "        jaccard_score : Compute the Jaccard similarity coefficient score.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multilabel classification, the zero_one_loss function corresponds to\n",
      "        the subset zero-one loss: for each sample, the entire set of labels must be\n",
      "        correctly predicted, otherwise the loss for that sample is equal to one.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import zero_one_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> zero_one_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        >>> zero_one_loss(y_true, y_pred, normalize=False)\n",
      "        1.0\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "\n",
      "DATA\n",
      "    __all__ = ['ConfusionMatrixDisplay', 'DetCurveDisplay', 'DistanceMetri...\n",
      "\n",
      "FILE\n",
      "    c:\\project\\bigdata_cert\\.venv\\lib\\site-packages\\sklearn\\metrics\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "help(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fcced46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.linear_model in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.linear_model - A variety of linear models.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _base\n",
      "    _bayes\n",
      "    _cd_fast\n",
      "    _coordinate_descent\n",
      "    _glm (package)\n",
      "    _huber\n",
      "    _least_angle\n",
      "    _linear_loss\n",
      "    _logistic\n",
      "    _omp\n",
      "    _passive_aggressive\n",
      "    _perceptron\n",
      "    _quantile\n",
      "    _ransac\n",
      "    _ridge\n",
      "    _sag\n",
      "    _sag_fast\n",
      "    _sgd_fast\n",
      "    _stochastic_gradient\n",
      "    _theil_sen\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(sklearn.utils._repr_html.base.ReprHTMLMixin, sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin, sklearn.utils._metadata_requests._MetadataRequester)\n",
      "        sklearn.linear_model._huber.HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._logistic.LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.linear_model._logistic.LogisticRegressionCV(sklearn.linear_model._logistic.LogisticRegression, sklearn.linear_model._base.LinearClassifierMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._quantile.QuantileRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.MetaEstimatorMixin(builtins.object)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.MultiOutputMixin(builtins.object)\n",
      "        sklearn.linear_model._base.LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._coordinate_descent.Lasso\n",
      "                sklearn.linear_model._coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model._coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model._least_angle.Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._least_angle.LarsCV\n",
      "                sklearn.linear_model._least_angle.LassoLarsCV\n",
      "            sklearn.linear_model._least_angle.LassoLars\n",
      "                sklearn.linear_model._least_angle.LassoLarsIC\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._ridge.Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "    sklearn.base.OutlierMixin(builtins.object)\n",
      "        sklearn.linear_model._stochastic_gradient.SGDOneClassSVM(sklearn.base.OutlierMixin, sklearn.linear_model._stochastic_gradient.BaseSGD)\n",
      "    sklearn.base.RegressorMixin(builtins.object)\n",
      "        sklearn.linear_model._base.LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.ARDRegression(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.BayesianRidge(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._coordinate_descent.Lasso\n",
      "                sklearn.linear_model._coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model._coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.LassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskLassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._huber.HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._least_angle.Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._least_angle.LarsCV\n",
      "                sklearn.linear_model._least_angle.LassoLarsCV\n",
      "            sklearn.linear_model._least_angle.LassoLars\n",
      "                sklearn.linear_model._least_angle.LassoLarsIC\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuitCV(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._quantile.QuantileRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._ransac.RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._ridge.Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "        sklearn.linear_model._theil_sen.TheilSenRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "    sklearn.linear_model._base.LinearClassifierMixin(sklearn.base.ClassifierMixin)\n",
      "        sklearn.linear_model._logistic.LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.linear_model._logistic.LogisticRegressionCV(sklearn.linear_model._logistic.LogisticRegression, sklearn.linear_model._base.LinearClassifierMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.linear_model._base.LinearModel(sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._base.LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.ARDRegression(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._bayes.BayesianRidge(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._coordinate_descent.Lasso\n",
      "                sklearn.linear_model._coordinate_descent.MultiTaskElasticNet\n",
      "                    sklearn.linear_model._coordinate_descent.MultiTaskLasso\n",
      "        sklearn.linear_model._huber.HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._least_angle.Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "            sklearn.linear_model._least_angle.LarsCV\n",
      "                sklearn.linear_model._least_angle.LassoLarsCV\n",
      "            sklearn.linear_model._least_angle.LassoLars\n",
      "                sklearn.linear_model._least_angle.LassoLarsIC\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._omp.OrthogonalMatchingPursuitCV(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._quantile.QuantileRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._theil_sen.TheilSenRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "    sklearn.linear_model._base.SparseCoefMixin(builtins.object)\n",
      "        sklearn.linear_model._logistic.LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.linear_model._logistic.LogisticRegressionCV(sklearn.linear_model._logistic.LogisticRegression, sklearn.linear_model._base.LinearClassifierMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.linear_model._coordinate_descent.LinearModelCV(sklearn.base.MultiOutputMixin, sklearn.linear_model._base.LinearModel, abc.ABC)\n",
      "        sklearn.linear_model._coordinate_descent.ElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.LassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "        sklearn.linear_model._coordinate_descent.MultiTaskLassoCV(sklearn.base.RegressorMixin, sklearn.linear_model._coordinate_descent.LinearModelCV)\n",
      "    sklearn.linear_model._glm.glm._GeneralizedLinearRegressor(sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._glm.glm.GammaRegressor\n",
      "        sklearn.linear_model._glm.glm.PoissonRegressor\n",
      "        sklearn.linear_model._glm.glm.TweedieRegressor\n",
      "    sklearn.linear_model._ridge._BaseRidge(sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._ridge.Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeClassifier(sklearn.linear_model._ridge._RidgeClassifierMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "    sklearn.linear_model._ridge._BaseRidgeCV(sklearn.linear_model._base.LinearModel)\n",
      "        sklearn.linear_model._ridge.RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "        sklearn.linear_model._ridge.RidgeClassifierCV(sklearn.linear_model._ridge._RidgeClassifierMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "    sklearn.linear_model._ridge._RidgeClassifierMixin(sklearn.linear_model._base.LinearClassifierMixin)\n",
      "        sklearn.linear_model._ridge.RidgeClassifier(sklearn.linear_model._ridge._RidgeClassifierMixin, sklearn.linear_model._ridge._BaseRidge)\n",
      "        sklearn.linear_model._ridge.RidgeClassifierCV(sklearn.linear_model._ridge._RidgeClassifierMixin, sklearn.linear_model._ridge._BaseRidgeCV)\n",
      "    sklearn.linear_model._stochastic_gradient.BaseSGD(sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.linear_model._stochastic_gradient.SGDOneClassSVM(sklearn.base.OutlierMixin, sklearn.linear_model._stochastic_gradient.BaseSGD)\n",
      "    sklearn.linear_model._stochastic_gradient.BaseSGDClassifier(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._stochastic_gradient.BaseSGD)\n",
      "        sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier\n",
      "        sklearn.linear_model._perceptron.Perceptron\n",
      "        sklearn.linear_model._stochastic_gradient.SGDClassifier\n",
      "    sklearn.linear_model._stochastic_gradient.BaseSGDRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._stochastic_gradient.BaseSGD)\n",
      "        sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor\n",
      "        sklearn.linear_model._stochastic_gradient.SGDRegressor\n",
      "    \n",
      "    class ARDRegression(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  ARDRegression(*, max_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, threshold_lambda=10000.0, fit_intercept=True, copy_X=True, verbose=False)\n",
      "     |  \n",
      "     |  Bayesian ARD regression.\n",
      "     |  \n",
      "     |  Fit the weights of a regression model, using an ARD prior. The weights of\n",
      "     |  the regression model are assumed to be in Gaussian distributions.\n",
      "     |  Also estimate the parameters lambda (precisions of the distributions of the\n",
      "     |  weights) and alpha (precision of the distribution of the noise).\n",
      "     |  The estimation is done by an iterative procedures (Evidence Maximization)\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bayesian_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_iter : int, default=300\n",
      "     |      Maximum number of iterations.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.3\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Stop the algorithm if w has converged.\n",
      "     |  \n",
      "     |  alpha_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the alpha parameter.\n",
      "     |  \n",
      "     |  alpha_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the alpha parameter.\n",
      "     |  \n",
      "     |  lambda_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the lambda parameter.\n",
      "     |  \n",
      "     |  lambda_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the lambda parameter.\n",
      "     |  \n",
      "     |  compute_score : bool, default=False\n",
      "     |      If True, compute the objective function at each step of the model.\n",
      "     |  \n",
      "     |  threshold_lambda : float, default=10 000\n",
      "     |      Threshold for removing (pruning) weights with high precision from\n",
      "     |      the computation.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      Coefficients of the regression model (mean of distribution)\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |     estimated precision of the noise.\n",
      "     |  \n",
      "     |  lambda_ : array-like of shape (n_features,)\n",
      "     |     estimated precisions of the weights.\n",
      "     |  \n",
      "     |  sigma_ : array-like of shape (n_features, n_features)\n",
      "     |      estimated variance-covariance matrix of the weights\n",
      "     |  \n",
      "     |  scores_ : float\n",
      "     |      if computed, value of the objective function (to be maximized)\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  X_offset_ : float\n",
      "     |      If `fit_intercept=True`, offset subtracted for centering data to a\n",
      "     |      zero mean. Set to np.zeros(n_features) otherwise.\n",
      "     |  \n",
      "     |  X_scale_ : float\n",
      "     |      Set to np.ones(n_features).\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  BayesianRidge : Bayesian ridge regression.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  D. J. C. MacKay, Bayesian nonlinear modeling for the prediction\n",
      "     |  competition, ASHRAE Transactions, 1994.\n",
      "     |  \n",
      "     |  R. Salakhutdinov, Lecture notes on Statistical Machine Learning,\n",
      "     |  http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15\n",
      "     |  Their beta is our ``self.alpha_``\n",
      "     |  Their alpha is our ``self.lambda_``\n",
      "     |  ARD is a little different than the slide: only dimensions/features for\n",
      "     |  which ``self.lambda_ < self.threshold_lambda`` are kept and the rest are\n",
      "     |  discarded.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.ARDRegression()\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  ARDRegression()\n",
      "     |  >>> clf.predict([[1, 1]])\n",
      "     |  array([1.])\n",
      "     |  \n",
      "     |  -   :ref:`sphx_glr_auto_examples_linear_model_plot_ard.py` demonstrates ARD\n",
      "     |      Regression.\n",
      "     |  -   :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_and_elasticnet.py`\n",
      "     |      showcases ARD Regression alongside Lasso and Elastic-Net for sparse,\n",
      "     |      correlated signals, in the presence of noise.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ARDRegression\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, max_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, threshold_lambda=10000.0, fit_intercept=True, copy_X=True, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model according to the given training data and parameters.\n",
      "     |      \n",
      "     |      Iterative procedure to maximize the evidence\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training vector, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values (integers). Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  predict(self, X, return_std=False)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      In addition to the mean of the predictive distribution, also its\n",
      "     |      standard deviation can be returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      return_std : bool, default=False\n",
      "     |          Whether to return the standard deviation of posterior prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_mean : array-like of shape (n_samples,)\n",
      "     |          Mean of predictive distribution of query points.\n",
      "     |      \n",
      "     |      y_std : array-like of shape (n_samples,)\n",
      "     |          Standard deviation of predictive distribution of query points.\n",
      "     |  \n",
      "     |  set_predict_request(self: sklearn.linear_model._bayes.ARDRegression, *, return_std: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._bayes.ARDRegression from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``predict`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      return_std : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``return_std`` parameter in ``predict``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._bayes.ARDRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._bayes.ARDRegression from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class BayesianRidge(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  BayesianRidge(*, max_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, alpha_init=None, lambda_init=None, compute_score=False, fit_intercept=True, copy_X=True, verbose=False)\n",
      "     |  \n",
      "     |  Bayesian ridge regression.\n",
      "     |  \n",
      "     |  Fit a Bayesian ridge model. See the Notes section for details on this\n",
      "     |  implementation and the optimization of the regularization parameters\n",
      "     |  lambda (precision of the weights) and alpha (precision of the noise).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bayesian_regression>`.\n",
      "     |  For an intuitive visualization of how the sinusoid is approximated by\n",
      "     |  a polynomial using different pairs of initial values, see\n",
      "     |  :ref:`sphx_glr_auto_examples_linear_model_plot_bayesian_ridge_curvefit.py`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_iter : int, default=300\n",
      "     |      Maximum number of iterations over the complete dataset before\n",
      "     |      stopping independently of any early stopping criterion.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.3\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Stop the algorithm if w has converged.\n",
      "     |  \n",
      "     |  alpha_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the alpha parameter.\n",
      "     |  \n",
      "     |  alpha_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the alpha parameter.\n",
      "     |  \n",
      "     |  lambda_1 : float, default=1e-6\n",
      "     |      Hyper-parameter : shape parameter for the Gamma distribution prior\n",
      "     |      over the lambda parameter.\n",
      "     |  \n",
      "     |  lambda_2 : float, default=1e-6\n",
      "     |      Hyper-parameter : inverse scale parameter (rate parameter) for the\n",
      "     |      Gamma distribution prior over the lambda parameter.\n",
      "     |  \n",
      "     |  alpha_init : float, default=None\n",
      "     |      Initial value for alpha (precision of the noise).\n",
      "     |      If not set, alpha_init is 1/Var(y).\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  lambda_init : float, default=None\n",
      "     |      Initial value for lambda (precision of the weights).\n",
      "     |      If not set, lambda_init is 1.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  compute_score : bool, default=False\n",
      "     |      If True, compute the log marginal likelihood at each iteration of the\n",
      "     |      optimization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model.\n",
      "     |      The intercept is not treated as a probabilistic parameter\n",
      "     |      and thus has no associated variance. If set\n",
      "     |      to False, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      Coefficients of the regression model (mean of distribution)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      `fit_intercept = False`.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |     Estimated precision of the noise.\n",
      "     |  \n",
      "     |  lambda_ : float\n",
      "     |     Estimated precision of the weights.\n",
      "     |  \n",
      "     |  sigma_ : array-like of shape (n_features, n_features)\n",
      "     |      Estimated variance-covariance matrix of the weights\n",
      "     |  \n",
      "     |  scores_ : array-like of shape (n_iter_+1,)\n",
      "     |      If computed_score is True, value of the log marginal likelihood (to be\n",
      "     |      maximized) at each iteration of the optimization. The array starts\n",
      "     |      with the value of the log marginal likelihood obtained for the initial\n",
      "     |      values of alpha and lambda and ends with the value obtained for the\n",
      "     |      estimated alpha and lambda.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |  \n",
      "     |  X_offset_ : ndarray of shape (n_features,)\n",
      "     |      If `fit_intercept=True`, offset subtracted for centering data to a\n",
      "     |      zero mean. Set to np.zeros(n_features) otherwise.\n",
      "     |  \n",
      "     |  X_scale_ : ndarray of shape (n_features,)\n",
      "     |      Set to np.ones(n_features).\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  ARDRegression : Bayesian ARD regression.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  There exist several strategies to perform Bayesian ridge regression. This\n",
      "     |  implementation is based on the algorithm described in Appendix A of\n",
      "     |  (Tipping, 2001) where updates of the regularization parameters are done as\n",
      "     |  suggested in (MacKay, 1992). Note that according to A New\n",
      "     |  View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these\n",
      "     |  update rules do not guarantee that the marginal likelihood is increasing\n",
      "     |  between two consecutive iterations of the optimization.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,\n",
      "     |  Vol. 4, No. 3, 1992.\n",
      "     |  \n",
      "     |  M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,\n",
      "     |  Journal of Machine Learning Research, Vol. 1, 2001.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.BayesianRidge()\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  BayesianRidge()\n",
      "     |  >>> clf.predict([[1, 1]])\n",
      "     |  array([1.])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BayesianRidge\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, max_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, alpha_init=None, lambda_init=None, compute_score=False, fit_intercept=True, copy_X=True, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.20\n",
      "     |             parameter *sample_weight* support to BayesianRidge.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  predict(self, X, return_std=False)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      In addition to the mean of the predictive distribution, also its\n",
      "     |      standard deviation can be returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      return_std : bool, default=False\n",
      "     |          Whether to return the standard deviation of posterior prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_mean : array-like of shape (n_samples,)\n",
      "     |          Mean of predictive distribution of query points.\n",
      "     |      \n",
      "     |      y_std : array-like of shape (n_samples,)\n",
      "     |          Standard deviation of predictive distribution of query points.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._bayes.BayesianRidge, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._bayes.BayesianRidge from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_predict_request(self: sklearn.linear_model._bayes.BayesianRidge, *, return_std: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._bayes.BayesianRidge from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``predict`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      return_std : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``return_std`` parameter in ``predict``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._bayes.BayesianRidge, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._bayes.BayesianRidge from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  ElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |  \n",
      "     |  Minimizes the objective function::\n",
      "     |  \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |  \n",
      "     |  If you are interested in controlling the L1 and L2 penalty\n",
      "     |  separately, keep in mind that this is equivalent to::\n",
      "     |  \n",
      "     |          a * ||w||_1 + 0.5 * b * ||w||_2^2\n",
      "     |  \n",
      "     |  where::\n",
      "     |  \n",
      "     |          alpha = a + b and l1_ratio = a / (a + b)\n",
      "     |  \n",
      "     |  The parameter l1_ratio corresponds to alpha in the glmnet R package while\n",
      "     |  alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n",
      "     |  = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n",
      "     |  unless you supply your own sequence of alpha.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the penalty terms. Defaults to 1.0.\n",
      "     |      See the notes for the exact mathematical meaning of this\n",
      "     |      parameter. ``alpha = 0`` is equivalent to an ordinary least square,\n",
      "     |      solved by the :class:`LinearRegression` object. For numerical\n",
      "     |      reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
      "     |      Given this, you should use the :class:`LinearRegression` object.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.5\n",
      "     |      The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n",
      "     |      ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n",
      "     |      is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n",
      "     |      combination of L1 and L2.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If ``False``, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  precompute : bool or array-like of shape (n_features, n_features),                 default=False\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. The Gram matrix can also be passed as argument.\n",
      "     |      For sparse input this option is always ``False`` to preserve sparsity.\n",
      "     |      Check :ref:`an example on how to use a precomputed Gram Matrix in ElasticNet\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.py>`\n",
      "     |      for details.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``, see Notes below.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the cost function formula).\n",
      "     |  \n",
      "     |  sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n",
      "     |      Sparse representation of the `coef_`.\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : list of int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  dual_gap_ : float or ndarray of shape (n_targets,)\n",
      "     |      Given param alpha, the dual gaps at the end of the optimization,\n",
      "     |      same shape as each observation of y.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  ElasticNetCV : Elastic net model with best model selection by\n",
      "     |      cross-validation.\n",
      "     |  SGDRegressor : Implements elastic net regression with incremental training.\n",
      "     |  SGDClassifier : Implements logistic regression with elastic net penalty\n",
      "     |      (``SGDClassifier(loss=\"log_loss\", penalty=\"elasticnet\")``).\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  The precise stopping criteria based on `tol` are the following: First, check that\n",
      "     |  that maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|`\n",
      "     |  is smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`.\n",
      "     |  If so, then additionally check whether the dual gap is smaller than `tol` times\n",
      "     |  :math:`||y||_2^2 / n_{      ext{samples}}`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import ElasticNet\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  \n",
      "     |  >>> X, y = make_regression(n_features=2, random_state=0)\n",
      "     |  >>> regr = ElasticNet(random_state=0)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  ElasticNet(random_state=0)\n",
      "     |  >>> print(regr.coef_)\n",
      "     |  [18.83816048 64.55968825]\n",
      "     |  >>> print(regr.intercept_)\n",
      "     |  1.451\n",
      "     |  >>> print(regr.predict([[0, 0]]))\n",
      "     |  [1.451]\n",
      "     |  \n",
      "     |  -   :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_and_elasticnet.py`\n",
      "     |      showcases ElasticNet alongside Lasso and ARD Regression for sparse\n",
      "     |      signal recovery in the presence of noise and feature correlation.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True)\n",
      "     |      Fit model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix, sparse array} of (n_samples, n_features)\n",
      "     |          Data.\n",
      "     |      \n",
      "     |          Note that large sparse matrices and arrays requiring `int64`\n",
      "     |          indices are not accepted.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : float or array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. Internally, the `sample_weight` vector will be\n",
      "     |          rescaled to sum to `n_samples`.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.23\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._coordinate_descent.ElasticNet, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.ElasticNet from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._coordinate_descent.ElasticNet, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.ElasticNet from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params) from sklearn.linear_model._coordinate_descent\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : array-like, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array-like of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      "     |      MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |      ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |      ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from sklearn.linear_model import enet_path\n",
      "     |      >>> from sklearn.datasets import make_regression\n",
      "     |      >>> X, y, true_coef = make_regression(\n",
      "     |      ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n",
      "     |      ... )\n",
      "     |      >>> true_coef\n",
      "     |      array([ 0.        ,  0.        ,  0.        , 97.9, 45.7])\n",
      "     |      >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n",
      "     |      >>> alphas.shape\n",
      "     |      (3,)\n",
      "     |      >>> estimated_coef\n",
      "     |       array([[ 0.,  0.787,  0.568],\n",
      "     |              [ 0.,  1.120,  0.620],\n",
      "     |              [-0., -2.129, -1.128],\n",
      "     |              [ 0., 23.046, 88.939],\n",
      "     |              [ 0., 10.637, 41.566]])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      Sparse representation of the fitted `coef_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class ElasticNetCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  ElasticNetCV(*, l1_ratio=0.5, eps=0.001, n_alphas='deprecated', alphas='warn', fit_intercept=True, precompute='auto', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Elastic Net model with iterative fitting along a regularization path.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  l1_ratio : float or list of float, default=0.5\n",
      "     |      Float between 0 and 1 passed to ElasticNet (scaling between\n",
      "     |      l1 and l2 penalties). For ``l1_ratio = 0``\n",
      "     |      the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2\n",
      "     |      This parameter can be a list, in which case the different\n",
      "     |      values are tested by cross-validation and the one giving the best\n",
      "     |      prediction score is used. Note that a good choice of list of\n",
      "     |      values for l1_ratio is often to put more values close to 1\n",
      "     |      (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n",
      "     |      .9, .95, .99, 1]``.\n",
      "     |  \n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path, used for each l1_ratio.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.7\n",
      "     |          `n_alphas` was deprecated in 1.7 and will be removed in 1.9. Use `alphas`\n",
      "     |          instead.\n",
      "     |  \n",
      "     |  alphas : array-like or int, default=None\n",
      "     |      Values of alphas to test along the regularization path, used for each l1_ratio.\n",
      "     |      If int, `alphas` values are generated automatically.\n",
      "     |      If array-like, list of alpha values to use.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.7\n",
      "     |          `alphas` accepts an integer value which removes the need to pass\n",
      "     |          `n_alphas`.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.7\n",
      "     |          `alphas=None` was deprecated in 1.7 and will be removed in 1.9, at which\n",
      "     |          point the default value will be set to 100.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=0\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation.\n",
      "     |  \n",
      "     |  l1_ratio_ : float\n",
      "     |      The compromise between l1 and l2 penalization chosen by\n",
      "     |      cross validation.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the cost function formula).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets, n_features)\n",
      "     |      Independent term in the decision function.\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_l1_ratio, n_alpha, n_folds)\n",
      "     |      Mean square error for the test set on each fold, varying l1_ratio and\n",
      "     |      alpha.\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n",
      "     |      The grid of alphas used for fitting, for each l1_ratio.\n",
      "     |  \n",
      "     |  dual_gap_ : float\n",
      "     |      The dual gaps at the end of the optimization for the optimal alpha.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  enet_path : Compute elastic net path with coordinate descent.\n",
      "     |  ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  In `fit`, once the best parameters `l1_ratio` and `alpha` are found through\n",
      "     |  cross-validation, the model is fit again using the entire training set.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the `X` argument of the `fit`\n",
      "     |  method should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  The parameter `l1_ratio` corresponds to alpha in the glmnet R package\n",
      "     |  while alpha corresponds to the lambda parameter in glmnet.\n",
      "     |  More specifically, the optimization objective is::\n",
      "     |  \n",
      "     |      1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |      + alpha * l1_ratio * ||w||_1\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |  \n",
      "     |  If you are interested in controlling the L1 and L2 penalty\n",
      "     |  separately, keep in mind that this is equivalent to::\n",
      "     |  \n",
      "     |      a * L1 + b * L2\n",
      "     |  \n",
      "     |  for::\n",
      "     |  \n",
      "     |      alpha = a + b and l1_ratio = a / (a + b).\n",
      "     |  \n",
      "     |  For an example, see\n",
      "     |  :ref:`examples/linear_model/plot_lasso_model_selection.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import ElasticNetCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  \n",
      "     |  >>> X, y = make_regression(n_features=2, random_state=0)\n",
      "     |  >>> regr = ElasticNetCV(cv=5, random_state=0)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  ElasticNetCV(cv=5, random_state=0)\n",
      "     |  >>> print(regr.alpha_)\n",
      "     |  0.199\n",
      "     |  >>> print(regr.intercept_)\n",
      "     |  0.398\n",
      "     |  >>> print(regr.predict([[0, 0]]))\n",
      "     |  [0.398]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ElasticNetCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, l1_ratio=0.5, eps=0.001, n_alphas='deprecated', alphas='warn', fit_intercept=True, precompute='auto', max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, **params)\n",
      "     |      Fit ElasticNet model with coordinate descent.\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse. Note that large sparse matrices and arrays\n",
      "     |          requiring `int64` indices are not accepted.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : float or array-like of shape (n_samples,),                 default=None\n",
      "     |          Sample weights used for fitting and evaluation of the weighted\n",
      "     |          mean squared error of each cv-fold. Note that the cross validated\n",
      "     |          MSE that is finally used to find the best model is the unweighted\n",
      "     |          mean over the (weighted) MSEs of each test fold.\n",
      "     |      \n",
      "     |      **params : dict, default=None\n",
      "     |          Parameters to be passed to the CV splitter.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |              Only available if `enable_metadata_routing=True`,\n",
      "     |              which can be set by using\n",
      "     |              ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of fitted model.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._coordinate_descent.ElasticNetCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.ElasticNetCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._coordinate_descent.ElasticNetCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.ElasticNetCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params) from sklearn.linear_model._coordinate_descent\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : array-like, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array-like of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      "     |      MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |      ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |      ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from sklearn.linear_model import enet_path\n",
      "     |      >>> from sklearn.datasets import make_regression\n",
      "     |      >>> X, y, true_coef = make_regression(\n",
      "     |      ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n",
      "     |      ... )\n",
      "     |      >>> true_coef\n",
      "     |      array([ 0.        ,  0.        ,  0.        , 97.9, 45.7])\n",
      "     |      >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n",
      "     |      >>> alphas.shape\n",
      "     |      (3,)\n",
      "     |      >>> estimated_coef\n",
      "     |       array([[ 0.,  0.787,  0.568],\n",
      "     |              [ 0.,  1.120,  0.620],\n",
      "     |              [-0., -2.129, -1.128],\n",
      "     |              [ 0., 23.046, 88.939],\n",
      "     |              [ 0., 10.637, 41.566]])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class GammaRegressor(_GeneralizedLinearRegressor)\n",
      "     |  GammaRegressor(*, alpha=1.0, fit_intercept=True, solver='lbfgs', max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |  \n",
      "     |  Generalized Linear Model with a Gamma distribution.\n",
      "     |  \n",
      "     |  This regressor uses the 'log' link function.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <Generalized_linear_models>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1\n",
      "     |      Constant that multiplies the L2 penalty term and determines the\n",
      "     |      regularization strength. ``alpha = 0`` is equivalent to unpenalized\n",
      "     |      GLMs. In this case, the design matrix `X` must have full column rank\n",
      "     |      (no collinearities).\n",
      "     |      Values of `alpha` must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the linear predictor `X @ coef_ + intercept_`.\n",
      "     |  \n",
      "     |  solver : {'lbfgs', 'newton-cholesky'}, default='lbfgs'\n",
      "     |      Algorithm to use in the optimization problem:\n",
      "     |  \n",
      "     |      'lbfgs'\n",
      "     |          Calls scipy's L-BFGS-B optimizer.\n",
      "     |  \n",
      "     |      'newton-cholesky'\n",
      "     |          Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to\n",
      "     |          iterated reweighted least squares) with an inner Cholesky based solver.\n",
      "     |          This solver is a good choice for `n_samples` >> `n_features`, especially\n",
      "     |          with one-hot encoded categorical features with rare categories. Be aware\n",
      "     |          that the memory usage of this solver has a quadratic dependency on\n",
      "     |          `n_features` because it explicitly computes the Hessian matrix.\n",
      "     |  \n",
      "     |          .. versionadded:: 1.2\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      The maximal number of iterations for the solver.\n",
      "     |      Values must be in the range `[1, inf)`.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Stopping criterion. For the lbfgs solver,\n",
      "     |      the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n",
      "     |      where ``g_j`` is the j-th component of the gradient (derivative) of\n",
      "     |      the objective function.\n",
      "     |      Values must be in the range `(0.0, inf)`.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      If set to ``True``, reuse the solution of the previous call to ``fit``\n",
      "     |      as initialization for `coef_` and `intercept_`.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the lbfgs solver set verbose to any positive number for verbosity.\n",
      "     |      Values must be in the range `[0, inf)`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features,)\n",
      "     |      Estimated coefficients for the linear predictor (`X @ coef_ +\n",
      "     |      intercept_`) in the GLM.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Intercept (a.k.a. bias) added to linear predictor.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Actual number of iterations used in the solver.\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  PoissonRegressor : Generalized Linear Model with a Poisson distribution.\n",
      "     |  TweedieRegressor : Generalized Linear Model with a Tweedie distribution.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.GammaRegressor()\n",
      "     |  >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n",
      "     |  >>> y = [19, 26, 33, 30]\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  GammaRegressor()\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  np.float64(0.773)\n",
      "     |  >>> clf.coef_\n",
      "     |  array([0.073, 0.067])\n",
      "     |  >>> clf.intercept_\n",
      "     |  np.float64(2.896)\n",
      "     |  >>> clf.predict([[1, 0], [2, 8]])\n",
      "     |  array([19.483, 35.795])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GammaRegressor\n",
      "     |      _GeneralizedLinearRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, fit_intercept=True, solver='lbfgs', max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._glm.glm.GammaRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._glm.glm.GammaRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._glm.glm.GammaRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._glm.glm.GammaRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _GeneralizedLinearRegressor:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit a Generalized Linear Model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted model.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using GLM with feature matrix X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array of shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Compute D^2, the percentage of deviance explained.\n",
      "     |      \n",
      "     |      D^2 is a generalization of the coefficient of determination R^2.\n",
      "     |      R^2 uses squared error and D^2 uses the deviance of this GLM, see the\n",
      "     |      :ref:`User Guide <regression_metrics>`.\n",
      "     |      \n",
      "     |      D^2 is defined as\n",
      "     |      :math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n",
      "     |      :math:`D_{null}` is the null deviance, i.e. the deviance of a model\n",
      "     |      with intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\n",
      "     |      The mean :math:`\\bar{y}` is averaged by sample_weight.\n",
      "     |      Best possible score is 1.0 and it can be negative (because the model\n",
      "     |      can be arbitrarily worse).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True values of target.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          D^2 of self.predict(X) w.r.t. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class HuberRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "     |  HuberRegressor(*, epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)\n",
      "     |  \n",
      "     |  L2-regularized linear regression model that is robust to outliers.\n",
      "     |  \n",
      "     |  The Huber Regressor optimizes the squared loss for the samples where\n",
      "     |  ``|(y - Xw - c) / sigma| < epsilon`` and the absolute loss for the samples\n",
      "     |  where ``|(y - Xw - c) / sigma| > epsilon``, where the model coefficients\n",
      "     |  ``w``, the intercept ``c`` and the scale ``sigma`` are parameters\n",
      "     |  to be optimized. The parameter `sigma` makes sure that if `y` is scaled up\n",
      "     |  or down by a certain factor, one does not need to rescale `epsilon` to\n",
      "     |  achieve the same robustness. Note that this does not take into account\n",
      "     |  the fact that the different features of `X` may be of different scales.\n",
      "     |  \n",
      "     |  The Huber loss function has the advantage of not being heavily influenced\n",
      "     |  by the outliers while not completely ignoring their effect.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <huber_regression>`\n",
      "     |  \n",
      "     |  .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  epsilon : float, default=1.35\n",
      "     |      The parameter epsilon controls the number of samples that should be\n",
      "     |      classified as outliers. The smaller the epsilon, the more robust it is\n",
      "     |      to outliers. Epsilon must be in the range `[1, inf)`.\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      Maximum number of iterations that\n",
      "     |      ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` should run for.\n",
      "     |  \n",
      "     |  alpha : float, default=0.0001\n",
      "     |      Strength of the squared L2 regularization. Note that the penalty is\n",
      "     |      equal to ``alpha * ||w||^2``.\n",
      "     |      Must be in the range `[0, inf)`.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      This is useful if the stored attributes of a previously used model\n",
      "     |      has to be reused. If set to False, then the coefficients will\n",
      "     |      be rewritten for every call to fit.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether or not to fit the intercept. This can be set to False\n",
      "     |      if the data is already centered around the origin.\n",
      "     |  \n",
      "     |  tol : float, default=1e-05\n",
      "     |      The iteration will stop when\n",
      "     |      ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``\n",
      "     |      where pg_i is the i-th component of the projected gradient.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape (n_features,)\n",
      "     |      Features got by optimizing the L2-regularized Huber loss.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Bias.\n",
      "     |  \n",
      "     |  scale_ : float\n",
      "     |      The value by which ``|y - Xw - c|`` is scaled down.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations that\n",
      "     |      ``scipy.optimize.minimize(method=\"L-BFGS-B\")`` has run for.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |  \n",
      "     |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      "     |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      "     |  \n",
      "     |  outliers_ : array, shape (n_samples,)\n",
      "     |      A boolean mask which is set to True where the samples are identified\n",
      "     |      as outliers.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.\n",
      "     |  TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\n",
      "     |  SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\n",
      "     |         Concomitant scale estimates, p. 172\n",
      "     |  .. [2] Art B. Owen (2006), `A robust hybrid of lasso and ridge regression.\n",
      "     |         <https://artowen.su.domains/reports/hhu.pdf>`_\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import HuberRegressor, LinearRegression\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> X, y, coef = make_regression(\n",
      "     |  ...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\n",
      "     |  >>> X[:4] = rng.uniform(10, 20, (4, 2))\n",
      "     |  >>> y[:4] = rng.uniform(10, 20, 4)\n",
      "     |  >>> huber = HuberRegressor().fit(X, y)\n",
      "     |  >>> huber.score(X, y)\n",
      "     |  -7.284\n",
      "     |  >>> huber.predict(X[:1,])\n",
      "     |  array([806.7200])\n",
      "     |  >>> linear = LinearRegression().fit(X, y)\n",
      "     |  >>> print(\"True coefficients:\", coef)\n",
      "     |  True coefficients: [20.4923...  34.1698...]\n",
      "     |  >>> print(\"Huber coefficients:\", huber.coef_)\n",
      "     |  Huber coefficients: [17.7906... 31.0106...]\n",
      "     |  >>> print(\"Linear Regression coefficients:\", linear.coef_)\n",
      "     |  Linear Regression coefficients: [-1.9221...  7.0226...]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HuberRegressor\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Training vector, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like, shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,)\n",
      "     |          Weight given to each sample.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted `HuberRegressor` estimator.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._huber.HuberRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._huber.HuberRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._huber.HuberRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._huber.HuberRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class Lars(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  Lars(*, fit_intercept=True, verbose=False, precompute='auto', n_nonzero_coefs=500, eps=np.float64(2.220446049250313e-16), copy_X=True, fit_path=True, jitter=None, random_state=None)\n",
      "     |  \n",
      "     |  Least Angle Regression model a.k.a. LAR.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like , default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  n_nonzero_coefs : int, default=500\n",
      "     |      Target number of non-zero coefficients. Use ``np.inf`` for no limit.\n",
      "     |  \n",
      "     |  eps : float, default=np.finfo(float).eps\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  fit_path : bool, default=True\n",
      "     |      If True the full path is stored in the ``coef_path_`` attribute.\n",
      "     |      If you compute the solution for a large problem or many targets,\n",
      "     |      setting ``fit_path`` to ``False`` will lead to a speedup, especially\n",
      "     |      with a small alpha.\n",
      "     |  \n",
      "     |  jitter : float, default=None\n",
      "     |      Upper bound on a uniform noise parameter to be added to the\n",
      "     |      `y` values, to satisfy the model's assumption of\n",
      "     |      one-at-a-time computations. Might help with stability.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Determines random number generation for jittering. Pass an int\n",
      "     |      for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n",
      "     |      Maximum of covariances (in absolute value) at each iteration.\n",
      "     |      ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "     |      number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "     |      is smaller. If this is a list of array-like, the length of the outer\n",
      "     |      list is `n_targets`.\n",
      "     |  \n",
      "     |  active_ : list of shape (n_alphas,) or list of such lists\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |      If this is a list of list, the length of the outer list is `n_targets`.\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays\n",
      "     |      The varying values of the coefficients along the path. It is not\n",
      "     |      present if the ``fit_path`` parameter is ``False``. If this is a list\n",
      "     |      of array-like, the length of the outer list is `n_targets`.\n",
      "     |  \n",
      "     |  coef_ : array-like of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the formulation formula).\n",
      "     |  \n",
      "     |  intercept_ : float or array-like of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      The number of iterations taken by lars_path to find the\n",
      "     |      grid of alphas for each target.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path: Compute Least Angle Regression or Lasso\n",
      "     |      path using LARS algorithm.\n",
      "     |  LarsCV : Cross-validated Least Angle Regression model.\n",
      "     |  sklearn.decomposition.sparse_encode : Sparse coding.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> reg = linear_model.Lars(n_nonzero_coefs=1)\n",
      "     |  >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])\n",
      "     |  Lars(n_nonzero_coefs=1)\n",
      "     |  >>> print(reg.coef_)\n",
      "     |  [ 0. -1.11]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, verbose=False, precompute='auto', n_nonzero_coefs=500, eps=np.float64(2.220446049250313e-16), copy_X=True, fit_path=True, jitter=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, Xy=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),                 default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._least_angle.Lars, *, Xy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.Lars from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      Xy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``Xy`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._least_angle.Lars, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.Lars from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  method = 'lar'\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class LarsCV(Lars)\n",
      "     |  LarsCV(*, fit_intercept=True, verbose=False, max_iter=500, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=np.float64(2.220446049250313e-16), copy_X=True)\n",
      "     |  \n",
      "     |  Cross-validated Least Angle Regression model.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like , default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram matrix\n",
      "     |      cannot be passed as argument since we will use only subsets of X.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  max_n_alphas : int, default=1000\n",
      "     |      The maximum number of points on the path used to compute the\n",
      "     |      residuals in the cross-validation.\n",
      "     |  \n",
      "     |  n_jobs : int or None, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  eps : float, default=np.finfo(float).eps\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  active_ : list of length n_alphas or list of such lists\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |      If this is a list of lists, the outer list length is `n_targets`.\n",
      "     |  \n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas)\n",
      "     |      the varying values of the coefficients along the path\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the estimated regularization parameter alpha\n",
      "     |  \n",
      "     |  alphas_ : array-like of shape (n_alphas,)\n",
      "     |      the different values of alpha along the path\n",
      "     |  \n",
      "     |  cv_alphas_ : array-like of shape (n_cv_alphas,)\n",
      "     |      all the values of alpha along the path for the different folds\n",
      "     |  \n",
      "     |  mse_path_ : array-like of shape (n_folds, n_cv_alphas)\n",
      "     |      the mean square error on left-out for each fold along the path\n",
      "     |      (alpha values given by ``cv_alphas``)\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      the number of iterations run by Lars with the optimal alpha.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path : Compute Least Angle Regression or Lasso\n",
      "     |      path using LARS algorithm.\n",
      "     |  lasso_path : Compute Lasso path with coordinate descent.\n",
      "     |  Lasso : Linear Model trained with L1 prior as\n",
      "     |      regularizer (aka the Lasso).\n",
      "     |  LassoCV : Lasso linear model with iterative fitting\n",
      "     |      along a regularization path.\n",
      "     |  LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |  LassoLarsIC : Lasso model fit with Lars using BIC\n",
      "     |      or AIC for model selection.\n",
      "     |  sklearn.decomposition.sparse_encode : Sparse coding.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  In `fit`, once the best parameter `alpha` is found through\n",
      "     |  cross-validation, the model is fit again using the entire training set.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import LarsCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_samples=200, noise=4.0, random_state=0)\n",
      "     |  >>> reg = LarsCV(cv=5).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9996\n",
      "     |  >>> reg.alpha_\n",
      "     |  np.float64(0.2961)\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([154.3996])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LarsCV\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, verbose=False, max_iter=500, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=np.float64(2.220446049250313e-16), copy_X=True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, **params)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      **params : dict, default=None\n",
      "     |          Parameters to be passed to the CV splitter.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |              Only available if `enable_metadata_routing=True`,\n",
      "     |              which can be set by using\n",
      "     |              ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._least_angle.LarsCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LarsCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  method = 'lar'\n",
      "     |  \n",
      "     |  parameter = 'random_state'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Lars:\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._least_angle.LarsCV, *, Xy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LarsCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      Xy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``Xy`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class Lasso(ElasticNet)\n",
      "     |  Lasso(alpha=1.0, *, fit_intercept=True, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Linear Model trained with L1 prior as regularizer (aka the Lasso).\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Technically the Lasso model is optimizing the same objective function as\n",
      "     |  the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the L1 term, controlling regularization\n",
      "     |      strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.\n",
      "     |  \n",
      "     |      When `alpha = 0`, the objective is equivalent to ordinary least\n",
      "     |      squares, solved by the :class:`LinearRegression` object. For numerical\n",
      "     |      reasons, using `alpha = 0` with the `Lasso` object is not advised.\n",
      "     |      Instead, you should use the :class:`LinearRegression` object.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to False, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  precompute : bool or array-like of shape (n_features, n_features),                 default=False\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. The Gram matrix can also be passed as argument.\n",
      "     |      For sparse input this option is always ``False`` to preserve sparsity.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``, see Notes below.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the cost function formula).\n",
      "     |  \n",
      "     |  dual_gap_ : float or ndarray of shape (n_targets,)\n",
      "     |      Given param alpha, the dual gaps at the end of the optimization,\n",
      "     |      same shape as each observation of y.\n",
      "     |  \n",
      "     |  sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features)\n",
      "     |      Readonly property derived from ``coef_``.\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int or list of int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path : Regularization path using LARS.\n",
      "     |  lasso_path : Regularization path using Lasso.\n",
      "     |  LassoLars : Lasso Path along the regularization parameter using LARS algorithm.\n",
      "     |  LassoCV : Lasso alpha parameter by cross-validation.\n",
      "     |  LassoLarsCV : Lasso least angle parameter algorithm by cross-validation.\n",
      "     |  sklearn.decomposition.sparse_encode : Sparse coding array estimator.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |  should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  Regularization improves the conditioning of the problem and\n",
      "     |  reduces the variance of the estimates. Larger values specify stronger\n",
      "     |  regularization. Alpha corresponds to `1 / (2C)` in other linear\n",
      "     |  models such as :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |  :class:`~sklearn.svm.LinearSVC`.\n",
      "     |  \n",
      "     |  The precise stopping criteria based on `tol` are the following: First, check that\n",
      "     |  that maximum coordinate update, i.e. :math:`\\max_j |w_j^{new} - w_j^{old}|`\n",
      "     |  is smaller than `tol` times the maximum absolute coefficient, :math:`\\max_j |w_j|`.\n",
      "     |  If so, then additionally check whether the dual gap is smaller than `tol` times\n",
      "     |  :math:`||y||_2^2 / n_{\\text{samples}}`.\n",
      "     |  \n",
      "     |  The target can be a 2-dimensional array, resulting in the optimization of the\n",
      "     |  following objective::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^2_F + alpha * ||W||_11\n",
      "     |  \n",
      "     |  where :math:`||W||_{1,1}` is the sum of the magnitude of the matrix coefficients.\n",
      "     |  It should not be confused with :class:`~sklearn.linear_model.MultiTaskLasso` which\n",
      "     |  instead penalizes the :math:`L_{2,1}` norm of the coefficients, yielding row-wise\n",
      "     |  sparsity in the coefficients.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.Lasso(alpha=0.1)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n",
      "     |  Lasso(alpha=0.1)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [0.85 0.  ]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  0.15\n",
      "     |  \n",
      "     |  -   :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_and_elasticnet.py`\n",
      "     |      compares Lasso with other L1-based regression models (ElasticNet and ARD\n",
      "     |      Regression) for sparse signal recovery in the presence of noise and\n",
      "     |      feature correlation.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._coordinate_descent.Lasso, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.Lasso from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._coordinate_descent.Lasso, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.Lasso from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params) from sklearn.linear_model._coordinate_descent\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : array-like, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array-like of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      "     |      MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |      ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |      ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from sklearn.linear_model import enet_path\n",
      "     |      >>> from sklearn.datasets import make_regression\n",
      "     |      >>> X, y, true_coef = make_regression(\n",
      "     |      ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n",
      "     |      ... )\n",
      "     |      >>> true_coef\n",
      "     |      array([ 0.        ,  0.        ,  0.        , 97.9, 45.7])\n",
      "     |      >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n",
      "     |      >>> alphas.shape\n",
      "     |      (3,)\n",
      "     |      >>> estimated_coef\n",
      "     |       array([[ 0.,  0.787,  0.568],\n",
      "     |              [ 0.,  1.120,  0.620],\n",
      "     |              [-0., -2.129, -1.128],\n",
      "     |              [ 0., 23.046, 88.939],\n",
      "     |              [ 0., 10.637, 41.566]])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ElasticNet:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True)\n",
      "     |      Fit model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix, sparse array} of (n_samples, n_features)\n",
      "     |          Data.\n",
      "     |      \n",
      "     |          Note that large sparse matrices and arrays requiring `int64`\n",
      "     |          indices are not accepted.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : float or array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. Internally, the `sample_weight` vector will be\n",
      "     |          rescaled to sum to `n_samples`.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.23\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      Sparse representation of the fitted `coef_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class LassoCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  LassoCV(*, eps=0.001, n_alphas='deprecated', alphas='warn', fit_intercept=True, precompute='auto', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Lasso linear model with iterative fitting along a regularization path.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The best model is selected by cross-validation.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.7\n",
      "     |          `n_alphas` was deprecated in 1.7 and will be removed in 1.9. Use `alphas`\n",
      "     |          instead.\n",
      "     |  \n",
      "     |  alphas : array-like or int, default=None\n",
      "     |      Values of alphas to test along the regularization path.\n",
      "     |      If int, `alphas` values are generated automatically.\n",
      "     |      If array-like, list of alpha values to use.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.7\n",
      "     |          `alphas` accepts an integer value which removes the need to pass\n",
      "     |          `n_alphas`.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.7\n",
      "     |          `alphas=None` was deprecated in 1.7 and will be removed in 1.9, at which\n",
      "     |          point the default value will be set to 100.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      If positive, restrict regression coefficients to be positive.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the cost function formula).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_alphas, n_folds)\n",
      "     |      Mean square error for the test set on each fold, varying alpha.\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,)\n",
      "     |      The grid of alphas used for fitting.\n",
      "     |  \n",
      "     |  dual_gap_ : float or ndarray of shape (n_targets,)\n",
      "     |      The dual gap at the end of the optimization for the optimal alpha\n",
      "     |      (``alpha_``).\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path : Compute Least Angle Regression or Lasso path using LARS\n",
      "     |      algorithm.\n",
      "     |  lasso_path : Compute Lasso path with coordinate descent.\n",
      "     |  Lasso : The Lasso is a linear model that estimates sparse coefficients.\n",
      "     |  LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |  LassoCV : Lasso linear model with iterative fitting along a regularization\n",
      "     |      path.\n",
      "     |  LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  In `fit`, once the best parameter `alpha` is found through\n",
      "     |  cross-validation, the model is fit again using the entire training set.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the `X` argument of the `fit`\n",
      "     |  method should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |  \n",
      "     |  For an example, see :ref:`examples/linear_model/plot_lasso_model_selection.py\n",
      "     |  <sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py>`.\n",
      "     |  \n",
      "     |  :class:`LassoCV` leads to different results than a hyperparameter\n",
      "     |  search using :class:`~sklearn.model_selection.GridSearchCV` with a\n",
      "     |  :class:`Lasso` model. In :class:`LassoCV`, a model for a given\n",
      "     |  penalty `alpha` is warm started using the coefficients of the\n",
      "     |  closest model (trained at the previous iteration) on the\n",
      "     |  regularization path. It tends to speed up the hyperparameter\n",
      "     |  search.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import LassoCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(noise=4, random_state=0)\n",
      "     |  >>> reg = LassoCV(cv=5, random_state=0).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9993\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-78.4951])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, eps=0.001, n_alphas='deprecated', alphas='warn', fit_intercept=True, precompute='auto', max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, positive=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, **params)\n",
      "     |      Fit Lasso model with coordinate descent.\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data\n",
      "     |          to avoid unnecessary memory duplication. If y is mono-output,\n",
      "     |          X can be sparse. Note that large sparse matrices and arrays\n",
      "     |          requiring `int64` indices are not accepted.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : float or array-like of shape (n_samples,),                 default=None\n",
      "     |          Sample weights used for fitting and evaluation of the weighted\n",
      "     |          mean squared error of each cv-fold. Note that the cross validated\n",
      "     |          MSE that is finally used to find the best model is the unweighted\n",
      "     |          mean over the (weighted) MSEs of each test fold.\n",
      "     |      \n",
      "     |      **params : dict, default=None\n",
      "     |          Parameters to be passed to the CV splitter.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |              Only available if `enable_metadata_routing=True`,\n",
      "     |              which can be set by using\n",
      "     |              ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of fitted model.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._coordinate_descent.LassoCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.LassoCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._coordinate_descent.LassoCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.LassoCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params) from sklearn.linear_model._coordinate_descent\n",
      "     |      Compute Lasso path with coordinate descent.\n",
      "     |      \n",
      "     |      The Lasso optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <lasso>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : array-like, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If ``None`` alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array-like of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      lars_path : Compute Least Angle Regression or Lasso path using LARS\n",
      "     |          algorithm.\n",
      "     |      Lasso : The Lasso is a linear model that estimates sparse coefficients.\n",
      "     |      LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |      LassoCV : Lasso linear model with iterative fitting along a regularization\n",
      "     |          path.\n",
      "     |      LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n",
      "     |      sklearn.decomposition.sparse_encode : Estimator that can be used to\n",
      "     |          transform signals into sparse linear combination of atoms from a fixed.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n",
      "     |      \n",
      "     |      To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |      should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |      \n",
      "     |      Note that in certain cases, the Lars solver may be significantly\n",
      "     |      faster to implement this functionality. In particular, linear\n",
      "     |      interpolation can be used to retrieve model coefficients between the\n",
      "     |      values output by lars_path\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      Comparing lasso_path and lars_path with interpolation:\n",
      "     |      \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> from sklearn.linear_model import lasso_path\n",
      "     |      >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "     |      >>> y = np.array([1, 2, 3.1])\n",
      "     |      >>> # Use lasso_path to compute a coefficient path\n",
      "     |      >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "     |      >>> print(coef_path)\n",
      "     |      [[0.         0.         0.46874778]\n",
      "     |       [0.2159048  0.4425765  0.23689075]]\n",
      "     |      \n",
      "     |      >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "     |      >>> # same path\n",
      "     |      >>> from sklearn.linear_model import lars_path\n",
      "     |      >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "     |      >>> from scipy import interpolate\n",
      "     |      >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "     |      ...                                             coef_path_lars[:, ::-1])\n",
      "     |      >>> print(coef_path_continuous([5., 1., .5]))\n",
      "     |      [[0.         0.         0.46915237]\n",
      "     |       [0.2159048  0.4425765  0.23668876]]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class LassoLars(Lars)\n",
      "     |  LassoLars(alpha=1.0, *, fit_intercept=True, verbose=False, precompute='auto', max_iter=500, eps=np.float64(2.220446049250313e-16), copy_X=True, fit_path=True, positive=False, jitter=None, random_state=None)\n",
      "     |  \n",
      "     |  Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |  \n",
      "     |  It is a Linear Model trained with an L1 prior as regularizer.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the penalty term. Defaults to 1.0.\n",
      "     |      ``alpha = 0`` is equivalent to an ordinary least square, solved\n",
      "     |      by :class:`LinearRegression`. For numerical reasons, using\n",
      "     |      ``alpha = 0`` with the LassoLars object is not advised and you\n",
      "     |      should prefer the LinearRegression object.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like, default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  eps : float, default=np.finfo(float).eps\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  fit_path : bool, default=True\n",
      "     |      If ``True`` the full path is stored in the ``coef_path_`` attribute.\n",
      "     |      If you compute the solution for a large problem or many targets,\n",
      "     |      setting ``fit_path`` to ``False`` will lead to a speedup, especially\n",
      "     |      with a small alpha.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients will not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |  \n",
      "     |  jitter : float, default=None\n",
      "     |      Upper bound on a uniform noise parameter to be added to the\n",
      "     |      `y` values, to satisfy the model's assumption of\n",
      "     |      one-at-a-time computations. Might help with stability.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Determines random number generation for jittering. Pass an int\n",
      "     |      for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`. Ignored if `jitter` is None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n",
      "     |      Maximum of covariances (in absolute value) at each iteration.\n",
      "     |      ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "     |      number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "     |      is smaller. If this is a list of array-like, the length of the outer\n",
      "     |      list is `n_targets`.\n",
      "     |  \n",
      "     |  active_ : list of length n_alphas or list of such lists\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |      If this is a list of list, the length of the outer list is `n_targets`.\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas + 1) or list             of such arrays\n",
      "     |      If a list is passed it's expected to be one of n_targets such arrays.\n",
      "     |      The varying values of the coefficients along the path. It is not\n",
      "     |      present if the ``fit_path`` parameter is ``False``. If this is a list\n",
      "     |      of array-like, the length of the outer list is `n_targets`.\n",
      "     |  \n",
      "     |  coef_ : array-like of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the formulation formula).\n",
      "     |  \n",
      "     |  intercept_ : float or array-like of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      The number of iterations taken by lars_path to find the\n",
      "     |      grid of alphas for each target.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path : Compute Least Angle Regression or Lasso\n",
      "     |      path using LARS algorithm.\n",
      "     |  lasso_path : Compute Lasso path with coordinate descent.\n",
      "     |  Lasso : Linear Model trained with L1 prior as\n",
      "     |      regularizer (aka the Lasso).\n",
      "     |  LassoCV : Lasso linear model with iterative fitting\n",
      "     |      along a regularization path.\n",
      "     |  LassoLarsCV: Cross-validated Lasso, using the LARS algorithm.\n",
      "     |  LassoLarsIC : Lasso model fit with Lars using BIC\n",
      "     |      or AIC for model selection.\n",
      "     |  sklearn.decomposition.sparse_encode : Sparse coding.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> reg = linear_model.LassoLars(alpha=0.01)\n",
      "     |  >>> reg.fit([[-1, 1], [0, 0], [1, 1]], [-1, 0, -1])\n",
      "     |  LassoLars(alpha=0.01)\n",
      "     |  >>> print(reg.coef_)\n",
      "     |  [ 0.         -0.955]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLars\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, verbose=False, precompute='auto', max_iter=500, eps=np.float64(2.220446049250313e-16), copy_X=True, fit_path=True, positive=False, jitter=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._least_angle.LassoLars, *, Xy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LassoLars from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      Xy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``Xy`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._least_angle.LassoLars, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LassoLars from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Lars:\n",
      "     |  \n",
      "     |  fit(self, X, y, Xy=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),                 default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class LassoLarsCV(LarsCV)\n",
      "     |  LassoLarsCV(*, fit_intercept=True, verbose=False, max_iter=500, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=np.float64(2.220446049250313e-16), copy_X=True, positive=False)\n",
      "     |  \n",
      "     |  Cross-validated Lasso, using the LARS algorithm.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  precompute : bool or 'auto' , default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram matrix\n",
      "     |      cannot be passed as argument since we will use only subsets of X.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  max_n_alphas : int, default=1000\n",
      "     |      The maximum number of points on the path used to compute the\n",
      "     |      residuals in the cross-validation.\n",
      "     |  \n",
      "     |  n_jobs : int or None, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  eps : float, default=np.finfo(float).eps\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients do not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |      As a consequence using LassoLarsCV only makes sense for problems where\n",
      "     |      a sparse solution is expected and/or reached.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  coef_path_ : array-like of shape (n_features, n_alphas)\n",
      "     |      the varying values of the coefficients along the path\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the estimated regularization parameter alpha\n",
      "     |  \n",
      "     |  alphas_ : array-like of shape (n_alphas,)\n",
      "     |      the different values of alpha along the path\n",
      "     |  \n",
      "     |  cv_alphas_ : array-like of shape (n_cv_alphas,)\n",
      "     |      all the values of alpha along the path for the different folds\n",
      "     |  \n",
      "     |  mse_path_ : array-like of shape (n_folds, n_cv_alphas)\n",
      "     |      the mean square error on left-out for each fold along the path\n",
      "     |      (alpha values given by ``cv_alphas``)\n",
      "     |  \n",
      "     |  n_iter_ : array-like or int\n",
      "     |      the number of iterations run by Lars with the optimal alpha.\n",
      "     |  \n",
      "     |  active_ : list of int\n",
      "     |      Indices of active variables at the end of the path.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path : Compute Least Angle Regression or Lasso\n",
      "     |      path using LARS algorithm.\n",
      "     |  lasso_path : Compute Lasso path with coordinate descent.\n",
      "     |  Lasso : Linear Model trained with L1 prior as\n",
      "     |      regularizer (aka the Lasso).\n",
      "     |  LassoCV : Lasso linear model with iterative fitting\n",
      "     |      along a regularization path.\n",
      "     |  LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |  LassoLarsIC : Lasso model fit with Lars using BIC\n",
      "     |      or AIC for model selection.\n",
      "     |  sklearn.decomposition.sparse_encode : Sparse coding.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The object solves the same problem as the\n",
      "     |  :class:`~sklearn.linear_model.LassoCV` object. However, unlike the\n",
      "     |  :class:`~sklearn.linear_model.LassoCV`, it find the relevant alphas values\n",
      "     |  by itself. In general, because of this property, it will be more stable.\n",
      "     |  However, it is more fragile to heavily multicollinear datasets.\n",
      "     |  \n",
      "     |  It is more efficient than the :class:`~sklearn.linear_model.LassoCV` if\n",
      "     |  only a small number of features are selected compared to the total number,\n",
      "     |  for instance if there are very few samples compared to the number of\n",
      "     |  features.\n",
      "     |  \n",
      "     |  In `fit`, once the best parameter `alpha` is found through\n",
      "     |  cross-validation, the model is fit again using the entire training set.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import LassoLarsCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(noise=4.0, random_state=0)\n",
      "     |  >>> reg = LassoLarsCV(cv=5).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9993\n",
      "     |  >>> reg.alpha_\n",
      "     |  np.float64(0.3972)\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-78.4831])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLarsCV\n",
      "     |      LarsCV\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, verbose=False, max_iter=500, precompute='auto', cv=None, max_n_alphas=1000, n_jobs=None, eps=np.float64(2.220446049250313e-16), copy_X=True, positive=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._least_angle.LassoLarsCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LassoLarsCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LarsCV:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, **params)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      **params : dict, default=None\n",
      "     |          Parameters to be passed to the CV splitter.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |              Only available if `enable_metadata_routing=True`,\n",
      "     |              which can be set by using\n",
      "     |              ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from LarsCV:\n",
      "     |  \n",
      "     |  parameter = 'random_state'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Lars:\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._least_angle.LassoLarsCV, *, Xy: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LassoLarsCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      Xy : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``Xy`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class LassoLarsIC(LassoLars)\n",
      "     |  LassoLarsIC(criterion='aic', *, fit_intercept=True, verbose=False, precompute='auto', max_iter=500, eps=np.float64(2.220446049250313e-16), copy_X=True, positive=False, noise_variance=None)\n",
      "     |  \n",
      "     |  Lasso model fit with Lars using BIC or AIC for model selection.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |  \n",
      "     |  AIC is the Akaike information criterion [2]_ and BIC is the Bayes\n",
      "     |  Information criterion [3]_. Such criteria are useful to select the value\n",
      "     |  of the regularization parameter by making a trade-off between the\n",
      "     |  goodness of fit and the complexity of the model. A good model should\n",
      "     |  explain well the data while being simple.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <lasso_lars_ic>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  criterion : {'aic', 'bic'}, default='aic'\n",
      "     |      The type of criterion to use.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  precompute : bool, 'auto' or array-like, default='auto'\n",
      "     |      Whether to use a precomputed Gram matrix to speed up\n",
      "     |      calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |      matrix can also be passed as argument.\n",
      "     |  \n",
      "     |  max_iter : int, default=500\n",
      "     |      Maximum number of iterations to perform. Can be used for\n",
      "     |      early stopping.\n",
      "     |  \n",
      "     |  eps : float, default=np.finfo(float).eps\n",
      "     |      The machine-precision regularization in the computation of the\n",
      "     |      Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "     |      systems. Unlike the ``tol`` parameter in some iterative\n",
      "     |      optimization-based algorithms, this parameter does not control\n",
      "     |      the tolerance of the optimization.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      Restrict coefficients to be >= 0. Be aware that you might want to\n",
      "     |      remove fit_intercept which is set True by default.\n",
      "     |      Under the positive restriction the model coefficients do not converge\n",
      "     |      to the ordinary-least-squares solution for small values of alpha.\n",
      "     |      Only coefficients up to the smallest alpha value (``alphas_[alphas_ >\n",
      "     |      0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso\n",
      "     |      algorithm are typically in congruence with the solution of the\n",
      "     |      coordinate descent Lasso estimator.\n",
      "     |      As a consequence using LassoLarsIC only makes sense for problems where\n",
      "     |      a sparse solution is expected and/or reached.\n",
      "     |  \n",
      "     |  noise_variance : float, default=None\n",
      "     |      The estimated noise variance of the data. If `None`, an unbiased\n",
      "     |      estimate is computed by an OLS model. However, it is only possible\n",
      "     |      in the case where `n_samples > n_features + fit_intercept`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.1\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array-like of shape (n_features,)\n",
      "     |      parameter vector (w in the formulation formula)\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      independent term in decision function.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      the alpha parameter chosen by the information criterion\n",
      "     |  \n",
      "     |  alphas_ : array-like of shape (n_alphas + 1,) or list of such arrays\n",
      "     |      Maximum of covariances (in absolute value) at each iteration.\n",
      "     |      ``n_alphas`` is either ``max_iter``, ``n_features`` or the\n",
      "     |      number of nodes in the path with ``alpha >= alpha_min``, whichever\n",
      "     |      is smaller. If a list, it will be of length `n_targets`.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      number of iterations run by lars_path to find the grid of\n",
      "     |      alphas.\n",
      "     |  \n",
      "     |  criterion_ : array-like of shape (n_alphas,)\n",
      "     |      The value of the information criteria ('aic', 'bic') across all\n",
      "     |      alphas. The alpha which has the smallest information criterion is\n",
      "     |      chosen, as specified in [1]_.\n",
      "     |  \n",
      "     |  noise_variance_ : float\n",
      "     |      The estimated noise variance from the data used to compute the\n",
      "     |      criterion.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.1\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  lars_path : Compute Least Angle Regression or Lasso\n",
      "     |      path using LARS algorithm.\n",
      "     |  lasso_path : Compute Lasso path with coordinate descent.\n",
      "     |  Lasso : Linear Model trained with L1 prior as\n",
      "     |      regularizer (aka the Lasso).\n",
      "     |  LassoCV : Lasso linear model with iterative fitting\n",
      "     |      along a regularization path.\n",
      "     |  LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |  LassoLarsCV: Cross-validated Lasso, using the LARS algorithm.\n",
      "     |  sklearn.decomposition.sparse_encode : Sparse coding.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The number of degrees of freedom is computed as in [1]_.\n",
      "     |  \n",
      "     |  To have more details regarding the mathematical formulation of the\n",
      "     |  AIC and BIC criteria, please refer to :ref:`User Guide <lasso_lars_ic>`.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] :arxiv:`Zou, Hui, Trevor Hastie, and Robert Tibshirani.\n",
      "     |          \"On the degrees of freedom of the lasso.\"\n",
      "     |          The Annals of Statistics 35.5 (2007): 2173-2192.\n",
      "     |          <0712.0881>`\n",
      "     |  \n",
      "     |  .. [2] `Wikipedia entry on the Akaike information criterion\n",
      "     |          <https://en.wikipedia.org/wiki/Akaike_information_criterion>`_\n",
      "     |  \n",
      "     |  .. [3] `Wikipedia entry on the Bayesian information criterion\n",
      "     |          <https://en.wikipedia.org/wiki/Bayesian_information_criterion>`_\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> reg = linear_model.LassoLarsIC(criterion='bic')\n",
      "     |  >>> X = [[-2, 2], [-1, 1], [0, 0], [1, 1], [2, 2]]\n",
      "     |  >>> y = [-2.2222, -1.1111, 0, -1.1111, -2.2222]\n",
      "     |  >>> reg.fit(X, y)\n",
      "     |  LassoLarsIC(criterion='bic')\n",
      "     |  >>> print(reg.coef_)\n",
      "     |  [ 0.  -1.11]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LassoLarsIC\n",
      "     |      LassoLars\n",
      "     |      Lars\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, criterion='aic', *, fit_intercept=True, verbose=False, precompute='auto', max_iter=500, eps=np.float64(2.220446049250313e-16), copy_X=True, positive=False, noise_variance=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, copy_X=None)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      copy_X : bool, default=None\n",
      "     |          If provided, this parameter will override the choice\n",
      "     |          of copy_X made at instance creation.\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._least_angle.LassoLarsIC, *, copy_X: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LassoLarsIC from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      copy_X : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``copy_X`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._least_angle.LassoLarsIC, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._least_angle.LassoLarsIC from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  parameter = 'random_state'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from LassoLars:\n",
      "     |  \n",
      "     |  method = 'lasso'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Lars:\n",
      "     |  \n",
      "     |  positive = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, LinearModel)\n",
      "     |  LinearRegression(*, fit_intercept=True, copy_X=True, tol=1e-06, n_jobs=None, positive=False)\n",
      "     |  \n",
      "     |  Ordinary least squares Linear Regression.\n",
      "     |  \n",
      "     |  LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
      "     |  to minimize the residual sum of squares between the observed targets in\n",
      "     |  the dataset, and the targets predicted by the linear approximation.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to False, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  tol : float, default=1e-6\n",
      "     |      The precision of the solution (`coef_`) is determined by `tol` which\n",
      "     |      specifies a different convergence criterion for the `lsqr` solver.\n",
      "     |      `tol` is set as `atol` and `btol` of `scipy.sparse.linalg.lsqr` when\n",
      "     |      fitting on sparse training data. This parameter has no effect when fitting\n",
      "     |      on dense data.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.7\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to use for the computation. This will only provide\n",
      "     |      speedup in case of sufficiently large problems, that is if firstly\n",
      "     |      `n_targets > 1` and secondly `X` is sparse or if `positive` is set\n",
      "     |      to `True`. ``None`` means 1 unless in a\n",
      "     |      :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      "     |      processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive. This\n",
      "     |      option is only supported for dense arrays.\n",
      "     |  \n",
      "     |      For a comparison between a linear regression model with positive constraints\n",
      "     |      on the regression coefficients and a linear regression without such constraints,\n",
      "     |      see :ref:`sphx_glr_auto_examples_linear_model_plot_nnls.py`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
      "     |      Estimated coefficients for the linear regression problem.\n",
      "     |      If multiple targets are passed during the fit (y 2D), this\n",
      "     |      is a 2D array of shape (n_targets, n_features), while if only\n",
      "     |      one target is passed, this is a 1D array of length n_features.\n",
      "     |  \n",
      "     |  rank_ : int\n",
      "     |      Rank of matrix `X`. Only available when `X` is dense.\n",
      "     |  \n",
      "     |  singular_ : array of shape (min(X, y),)\n",
      "     |      Singular values of `X`. Only available when `X` is dense.\n",
      "     |  \n",
      "     |  intercept_ : float or array of shape (n_targets,)\n",
      "     |      Independent term in the linear model. Set to 0.0 if\n",
      "     |      `fit_intercept = False`.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Ridge : Ridge regression addresses some of the\n",
      "     |      problems of Ordinary Least Squares by imposing a penalty on the\n",
      "     |      size of the coefficients with l2 regularization.\n",
      "     |  Lasso : The Lasso is a linear model that estimates\n",
      "     |      sparse coefficients with l1 regularization.\n",
      "     |  ElasticNet : Elastic-Net is a linear regression\n",
      "     |      model trained with both l1 and l2 -norm regularization of the\n",
      "     |      coefficients.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  From the implementation point of view, this is just plain Ordinary\n",
      "     |  Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n",
      "     |  (scipy.optimize.nnls) wrapped as a predictor object.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import LinearRegression\n",
      "     |  >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
      "     |  >>> # y = 1 * x_0 + 2 * x_1 + 3\n",
      "     |  >>> y = np.dot(X, np.array([1, 2])) + 3\n",
      "     |  >>> reg = LinearRegression().fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  1.0\n",
      "     |  >>> reg.coef_\n",
      "     |  array([1., 2.])\n",
      "     |  >>> reg.intercept_\n",
      "     |  np.float64(3.0)\n",
      "     |  >>> reg.predict(np.array([[3, 5]]))\n",
      "     |  array([16.])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinearRegression\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, copy_X=True, tol=1e-06, n_jobs=None, positive=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             parameter *sample_weight* support to LinearRegression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted Estimator.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._base.LinearRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._base.LinearRegression from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._base.LinearRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._base.LinearRegression from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      "     |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='deprecated', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      "     |  \n",
      "     |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      "     |  \n",
      "     |  This class implements regularized logistic regression using the\n",
      "     |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      "     |  that regularization is applied by default**. It can handle both dense\n",
      "     |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      "     |  floats for optimal performance; any other input format will be converted\n",
      "     |  (and copied).\n",
      "     |  \n",
      "     |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      "     |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      "     |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      "     |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      "     |  'saga' solver.\n",
      "     |  \n",
      "     |  For :term:`multiclass` problems, all solvers but 'liblinear' optimize the\n",
      "     |  (penalized) multinomial loss. 'liblinear' only handle binary classification but can\n",
      "     |  be extended to handle multiclass by using\n",
      "     |  :class:`~sklearn.multiclass.OneVsRestClassifier`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
      "     |      Specify the norm of the penalty:\n",
      "     |  \n",
      "     |      - `None`: no penalty is added;\n",
      "     |      - `'l2'`: add a L2 penalty term and it is the default choice;\n",
      "     |      - `'l1'`: add a L1 penalty term;\n",
      "     |      - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
      "     |  \n",
      "     |      .. warning::\n",
      "     |         Some penalties may not work with some solvers. See the parameter\n",
      "     |         `solver` below, to know the compatibility between the penalty and\n",
      "     |         solver.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      "     |  \n",
      "     |  dual : bool, default=False\n",
      "     |      Dual (constrained) or primal (regularized, see also\n",
      "     |      :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation\n",
      "     |      is only implemented for l2 penalty with liblinear solver. Prefer dual=False when\n",
      "     |      n_samples > n_features.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  C : float, default=1.0\n",
      "     |      Inverse of regularization strength; must be a positive float.\n",
      "     |      Like in support vector machines, smaller values specify stronger\n",
      "     |      regularization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the decision function.\n",
      "     |  \n",
      "     |  intercept_scaling : float, default=1\n",
      "     |      Useful only when the solver `liblinear` is used\n",
      "     |      and `self.fit_intercept` is set to `True`. In this case, `x` becomes\n",
      "     |      `[x, self.intercept_scaling]`,\n",
      "     |      i.e. a \"synthetic\" feature with constant value equal to\n",
      "     |      `intercept_scaling` is appended to the instance vector.\n",
      "     |      The intercept becomes\n",
      "     |      ``intercept_scaling * synthetic_feature_weight``.\n",
      "     |  \n",
      "     |      .. note::\n",
      "     |          The synthetic feature weight is subject to L1 or L2\n",
      "     |          regularization as all other features.\n",
      "     |          To lessen the effect of regularization on synthetic feature weight\n",
      "     |          (and therefore on the intercept) `intercept_scaling` has to be increased.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *class_weight='balanced'*\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      "     |      data. See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
      "     |  \n",
      "     |      Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
      "     |      To choose a solver, you might want to consider the following aspects:\n",
      "     |  \n",
      "     |      - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
      "     |        and 'saga' are faster for large ones;\n",
      "     |      - For :term:`multiclass` problems, all solvers except 'liblinear' minimize the\n",
      "     |        full multinomial loss;\n",
      "     |      - 'liblinear' can only handle binary classification by default. To apply a\n",
      "     |        one-versus-rest scheme for the multiclass setting one can wrap it with the\n",
      "     |        :class:`~sklearn.multiclass.OneVsRestClassifier`.\n",
      "     |      - 'newton-cholesky' is a good choice for\n",
      "     |        `n_samples` >> `n_features * n_classes`, especially with one-hot encoded\n",
      "     |        categorical features with rare categories. Be aware that the memory usage\n",
      "     |        of this solver has a quadratic dependency on `n_features * n_classes`\n",
      "     |        because it explicitly computes the full Hessian matrix.\n",
      "     |  \n",
      "     |      .. warning::\n",
      "     |         The choice of the algorithm depends on the penalty chosen and on\n",
      "     |         (multinomial) multiclass support:\n",
      "     |  \n",
      "     |         ================= ============================== ======================\n",
      "     |         solver            penalty                        multinomial multiclass\n",
      "     |         ================= ============================== ======================\n",
      "     |         'lbfgs'           'l2', None                     yes\n",
      "     |         'liblinear'       'l1', 'l2'                     no\n",
      "     |         'newton-cg'       'l2', None                     yes\n",
      "     |         'newton-cholesky' 'l2', None                     yes\n",
      "     |         'sag'             'l2', None                     yes\n",
      "     |         'saga'            'elasticnet', 'l1', 'l2', None yes\n",
      "     |         ================= ============================== ======================\n",
      "     |  \n",
      "     |      .. note::\n",
      "     |         'sag' and 'saga' fast convergence is only guaranteed on features\n",
      "     |         with approximately the same scale. You can preprocess the data with\n",
      "     |         a scaler from :mod:`sklearn.preprocessing`.\n",
      "     |  \n",
      "     |      .. seealso::\n",
      "     |         Refer to the :ref:`User Guide <Logistic_regression>` for more\n",
      "     |         information regarding :class:`LogisticRegression` and more specifically the\n",
      "     |         :ref:`Table <logistic_regression_solvers>`\n",
      "     |         summarizing solver/penalty supports.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient (SAG) descent solver. Multinomial support in\n",
      "     |         version 0.18.\n",
      "     |      .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      "     |      .. versionadded:: 1.2\n",
      "     |         newton-cholesky solver. Multinomial support in version 1.6.\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      Maximum number of iterations taken for the solvers to converge.\n",
      "     |  \n",
      "     |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      "     |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      "     |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      "     |      across the entire probability distribution, *even when the data is\n",
      "     |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      "     |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      "     |      and otherwise selects 'multinomial'.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      "     |      .. deprecated:: 1.5\n",
      "     |         ``multi_class`` was deprecated in version 1.5 and will be removed in 1.8.\n",
      "     |         From then on, the recommended 'multinomial' will always be used for\n",
      "     |         `n_classes >= 3`.\n",
      "     |         Solvers that do not support 'multinomial' will raise an error.\n",
      "     |         Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegression())` if you\n",
      "     |         still want to use OvR.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      "     |      number for verbosity.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPU cores used when parallelizing over classes if\n",
      "     |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      "     |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      "     |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "     |      context. ``-1`` means using all processors.\n",
      "     |      See :term:`Glossary <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=None\n",
      "     |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      "     |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      "     |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      "     |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      "     |      combination of L1 and L2.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes, )\n",
      "     |      A list of class labels known to the classifier.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      "     |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      "     |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      "     |      Intercept (a.k.a. bias) added to the decision function.\n",
      "     |  \n",
      "     |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "     |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      "     |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      "     |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      "     |      outcome 0 (False).\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      "     |      Actual number of iterations for all classes. If binary or multinomial,\n",
      "     |      it returns only 1 element. For liblinear solver, only the maximum\n",
      "     |      number of iteration across all classes is given.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |  \n",
      "     |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      "     |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      "     |      the parameter ``loss=\"log_loss\"``).\n",
      "     |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The underlying C implementation uses a random number generator to\n",
      "     |  select features when fitting the model. It is thus not uncommon,\n",
      "     |  to have slightly different results for the same input data. If\n",
      "     |  that happens, try with a smaller tol parameter.\n",
      "     |  \n",
      "     |  Predict output may not match that of standalone liblinear in certain\n",
      "     |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      "     |  in the narrative documentation.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      "     |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      "     |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      "     |  \n",
      "     |  LIBLINEAR -- A Library for Large Linear Classification\n",
      "     |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      "     |  \n",
      "     |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      "     |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      "     |      https://hal.inria.fr/hal-00860051/document\n",
      "     |  \n",
      "     |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      "     |          :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
      "     |          for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
      "     |  \n",
      "     |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      "     |      methods for logistic regression and maximum entropy models.\n",
      "     |      Machine Learning 85(1-2):41-75.\n",
      "     |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      "     |  >>> clf.predict(X[:2, :])\n",
      "     |  array([0, 0])\n",
      "     |  >>> clf.predict_proba(X[:2, :])\n",
      "     |  array([[9.82e-01, 1.82e-02, 1.44e-08],\n",
      "     |         [9.72e-01, 2.82e-02, 3.02e-08]])\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.97\n",
      "     |  \n",
      "     |  For a comparison of the LogisticRegression with other classifiers see:\n",
      "     |  :ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogisticRegression\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='deprecated', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vector, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,) default=None\n",
      "     |          Array of weights that are assigned to individual samples.\n",
      "     |          If not provided, then each sample is given unit weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             *sample_weight* support to LogisticRegression.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict logarithm of probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      "     |      the softmax function is used to find the predicted probability of\n",
      "     |      each class.\n",
      "     |      Else use a one-vs-rest approach, i.e. calculate the probability\n",
      "     |      of each class assuming it to be positive using the logistic function\n",
      "     |      and normalize these values across all the classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._logistic.LogisticRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegression from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._logistic.LogisticRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegression from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the confidence scores.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      "     |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      "     |          this class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the predictions.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          Vector containing the class labels for each sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class LogisticRegressionCV(LogisticRegression, sklearn.linear_model._base.LinearClassifierMixin, sklearn.base.BaseEstimator)\n",
      "     |  LogisticRegressionCV(*, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='deprecated', random_state=None, l1_ratios=None)\n",
      "     |  \n",
      "     |  Logistic Regression CV (aka logit, MaxEnt) classifier.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  This class implements logistic regression using liblinear, newton-cg, sag\n",
      "     |  or lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n",
      "     |  regularization with primal formulation. The liblinear solver supports both\n",
      "     |  L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n",
      "     |  Elastic-Net penalty is only supported by the saga solver.\n",
      "     |  \n",
      "     |  For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\n",
      "     |  is selected by the cross-validator\n",
      "     |  :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\n",
      "     |  using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      "     |  solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  Cs : int or list of floats, default=10\n",
      "     |      Each of the values in Cs describes the inverse of regularization\n",
      "     |      strength. If Cs is as an int, then a grid of Cs values are chosen\n",
      "     |      in a logarithmic scale between 1e-4 and 1e4.\n",
      "     |      Like in support vector machines, smaller values specify stronger\n",
      "     |      regularization.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the decision function.\n",
      "     |  \n",
      "     |  cv : int or cross-validation generator, default=None\n",
      "     |      The default cross-validation generator used is Stratified K-Folds.\n",
      "     |      If an integer is provided, then it is the number of folds used.\n",
      "     |      See the module :mod:`sklearn.model_selection` module for the\n",
      "     |      list of possible cross-validation objects.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  dual : bool, default=False\n",
      "     |      Dual (constrained) or primal (regularized, see also\n",
      "     |      :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation\n",
      "     |      is only implemented for l2 penalty with liblinear solver. Prefer dual=False when\n",
      "     |      n_samples > n_features.\n",
      "     |  \n",
      "     |  penalty : {'l1', 'l2', 'elasticnet'}, default='l2'\n",
      "     |      Specify the norm of the penalty:\n",
      "     |  \n",
      "     |      - `'l2'`: add a L2 penalty term (used by default);\n",
      "     |      - `'l1'`: add a L1 penalty term;\n",
      "     |      - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
      "     |  \n",
      "     |      .. warning::\n",
      "     |         Some penalties may not work with some solvers. See the parameter\n",
      "     |         `solver` below, to know the compatibility between the penalty and\n",
      "     |         solver.\n",
      "     |  \n",
      "     |  scoring : str or callable, default=None\n",
      "     |      The scoring method to use for cross-validation. Options:\n",
      "     |  \n",
      "     |      - str: see :ref:`scoring_string_names` for options.\n",
      "     |      - callable: a scorer callable object (e.g., function) with signature\n",
      "     |        ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n",
      "     |      - `None`: :ref:`accuracy <accuracy_score>` is used.\n",
      "     |  \n",
      "     |  solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
      "     |  \n",
      "     |      Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
      "     |      To choose a solver, you might want to consider the following aspects:\n",
      "     |  \n",
      "     |      - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
      "     |        and 'saga' are faster for large ones;\n",
      "     |      - For multiclass problems, all solvers except 'liblinear' minimize the full\n",
      "     |        multinomial loss;\n",
      "     |      - 'liblinear' might be slower in :class:`LogisticRegressionCV`\n",
      "     |        because it does not handle warm-starting.\n",
      "     |      - 'liblinear' can only handle binary classification by default. To apply a\n",
      "     |        one-versus-rest scheme for the multiclass setting one can wrap it with the\n",
      "     |        :class:`~sklearn.multiclass.OneVsRestClassifier`.\n",
      "     |      - 'newton-cholesky' is a good choice for\n",
      "     |        `n_samples` >> `n_features * n_classes`, especially with one-hot encoded\n",
      "     |        categorical features with rare categories. Be aware that the memory usage\n",
      "     |        of this solver has a quadratic dependency on `n_features * n_classes`\n",
      "     |        because it explicitly computes the full Hessian matrix.\n",
      "     |  \n",
      "     |      .. warning::\n",
      "     |         The choice of the algorithm depends on the penalty chosen and on\n",
      "     |         (multinomial) multiclass support:\n",
      "     |  \n",
      "     |         ================= ============================== ======================\n",
      "     |         solver            penalty                        multinomial multiclass\n",
      "     |         ================= ============================== ======================\n",
      "     |         'lbfgs'           'l2'                           yes\n",
      "     |         'liblinear'       'l1', 'l2'                     no\n",
      "     |         'newton-cg'       'l2'                           yes\n",
      "     |         'newton-cholesky' 'l2',                          yes\n",
      "     |         'sag'             'l2',                          yes\n",
      "     |         'saga'            'elasticnet', 'l1', 'l2'       yes\n",
      "     |         ================= ============================== ======================\n",
      "     |  \n",
      "     |      .. note::\n",
      "     |         'sag' and 'saga' fast convergence is only guaranteed on features\n",
      "     |         with approximately the same scale. You can preprocess the data with\n",
      "     |         a scaler from :mod:`sklearn.preprocessing`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient (SAG) descent solver. Multinomial support in\n",
      "     |         version 0.18.\n",
      "     |      .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |      .. versionadded:: 1.2\n",
      "     |         newton-cholesky solver. Multinomial support in version 1.6.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Tolerance for stopping criteria.\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      Maximum number of iterations of the optimization algorithm.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         class_weight == 'balanced'\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPU cores used during the cross-validation loop.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n",
      "     |      positive number for verbosity.\n",
      "     |  \n",
      "     |  refit : bool, default=True\n",
      "     |      If set to True, the scores are averaged across all folds, and the\n",
      "     |      coefs and the C that corresponds to the best score is taken, and a\n",
      "     |      final refit is done using these parameters.\n",
      "     |      Otherwise the coefs, intercepts and C that correspond to the\n",
      "     |      best scores across folds are averaged.\n",
      "     |  \n",
      "     |  intercept_scaling : float, default=1\n",
      "     |      Useful only when the solver `liblinear` is used\n",
      "     |      and `self.fit_intercept` is set to `True`. In this case, `x` becomes\n",
      "     |      `[x, self.intercept_scaling]`,\n",
      "     |      i.e. a \"synthetic\" feature with constant value equal to\n",
      "     |      `intercept_scaling` is appended to the instance vector.\n",
      "     |      The intercept becomes\n",
      "     |      ``intercept_scaling * synthetic_feature_weight``.\n",
      "     |  \n",
      "     |      .. note::\n",
      "     |          The synthetic feature weight is subject to L1 or L2\n",
      "     |          regularization as all other features.\n",
      "     |          To lessen the effect of regularization on synthetic feature weight\n",
      "     |          (and therefore on the intercept) `intercept_scaling` has to be increased.\n",
      "     |  \n",
      "     |  multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n",
      "     |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      "     |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      "     |      across the entire probability distribution, *even when the data is\n",
      "     |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      "     |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      "     |      and otherwise selects 'multinomial'.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      "     |      .. deprecated:: 1.5\n",
      "     |         ``multi_class`` was deprecated in version 1.5 and will be removed in 1.8.\n",
      "     |         From then on, the recommended 'multinomial' will always be used for\n",
      "     |         `n_classes >= 3`.\n",
      "     |         Solvers that do not support 'multinomial' will raise an error.\n",
      "     |         Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegressionCV())` if you\n",
      "     |         still want to use OvR.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\n",
      "     |      Note that this only applies to the solver and not the cross-validation\n",
      "     |      generator. See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  l1_ratios : list of float, default=None\n",
      "     |      The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n",
      "     |      Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n",
      "     |      using ``penalty='l2'``, while 1 is equivalent to using\n",
      "     |      ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n",
      "     |      of L1 and L2.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes, )\n",
      "     |      A list of class labels known to the classifier.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      `coef_` is of shape (1, n_features) when the given problem\n",
      "     |      is binary.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      "     |      Intercept (a.k.a. bias) added to the decision function.\n",
      "     |  \n",
      "     |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "     |      `intercept_` is of shape(1,) when the problem is binary.\n",
      "     |  \n",
      "     |  Cs_ : ndarray of shape (n_cs)\n",
      "     |      Array of C i.e. inverse of regularization parameter values used\n",
      "     |      for cross-validation.\n",
      "     |  \n",
      "     |  l1_ratios_ : ndarray of shape (n_l1_ratios)\n",
      "     |      Array of l1_ratios used for cross-validation. If no l1_ratio is used\n",
      "     |      (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n",
      "     |  \n",
      "     |  coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\n",
      "     |      dict with classes as the keys, and the path of coefficients obtained\n",
      "     |      during cross-validating across each fold and then across each Cs\n",
      "     |      after doing an OvR for the corresponding class as values.\n",
      "     |      If the 'multi_class' option is set to 'multinomial', then\n",
      "     |      the coefs_paths are the coefficients corresponding to each class.\n",
      "     |      Each dict value has shape ``(n_folds, n_cs, n_features)`` or\n",
      "     |      ``(n_folds, n_cs, n_features + 1)`` depending on whether the\n",
      "     |      intercept is fit or not. If ``penalty='elasticnet'``, the shape is\n",
      "     |      ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n",
      "     |      ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\n",
      "     |  \n",
      "     |  scores_ : dict\n",
      "     |      dict with classes as the keys, and the values as the\n",
      "     |      grid of scores obtained during cross-validating each fold, after doing\n",
      "     |      an OvR for the corresponding class. If the 'multi_class' option\n",
      "     |      given is 'multinomial' then the same scores are repeated across\n",
      "     |      all classes, since this is the multinomial class. Each dict value\n",
      "     |      has shape ``(n_folds, n_cs)`` or ``(n_folds, n_cs, n_l1_ratios)`` if\n",
      "     |      ``penalty='elasticnet'``.\n",
      "     |  \n",
      "     |  C_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
      "     |      Array of C that maps to the best scores across every class. If refit is\n",
      "     |      set to False, then for each class, the best C is the average of the\n",
      "     |      C's that correspond to the best scores for each fold.\n",
      "     |      `C_` is of shape(n_classes,) when the problem is binary.\n",
      "     |  \n",
      "     |  l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
      "     |      Array of l1_ratio that maps to the best scores across every class. If\n",
      "     |      refit is set to False, then for each class, the best l1_ratio is the\n",
      "     |      average of the l1_ratio's that correspond to the best scores for each\n",
      "     |      fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\n",
      "     |  \n",
      "     |  n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n",
      "     |      Actual number of iterations for all classes, folds and Cs.\n",
      "     |      In the binary or multinomial cases, the first dimension is equal to 1.\n",
      "     |      If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\n",
      "     |      n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  LogisticRegression : Logistic regression without tuning the\n",
      "     |      hyperparameter `C`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.linear_model import LogisticRegressionCV\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
      "     |  >>> clf.predict(X[:2, :])\n",
      "     |  array([0, 0])\n",
      "     |  >>> clf.predict_proba(X[:2, :]).shape\n",
      "     |  (2, 3)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.98...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogisticRegressionCV\n",
      "     |      LogisticRegression\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='deprecated', random_state=None, l1_ratios=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, **params)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vector, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target vector relative to X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,) default=None\n",
      "     |          Array of weights that are assigned to individual samples.\n",
      "     |          If not provided, then each sample is given unit weight.\n",
      "     |      \n",
      "     |      **params : dict\n",
      "     |          Parameters to pass to the underlying splitter and scorer.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted LogisticRegressionCV estimator.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None, **score_params)\n",
      "     |      Score using the `scoring` option on the given test data and labels.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      **score_params : dict\n",
      "     |          Parameters to pass to the `score` method of the underlying scorer.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Score of self.predict(X) w.r.t. y.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._logistic.LogisticRegressionCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegressionCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._logistic.LogisticRegressionCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegressionCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  param = 'l1_ratio'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LogisticRegression:\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict logarithm of probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      The returned estimates for all classes are ordered by the\n",
      "     |      label of classes.\n",
      "     |      \n",
      "     |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      "     |      the softmax function is used to find the predicted probability of\n",
      "     |      each class.\n",
      "     |      Else use a one-vs-rest approach, i.e. calculate the probability\n",
      "     |      of each class assuming it to be positive using the logistic function\n",
      "     |      and normalize these values across all the classes.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Vector to be scored, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in ``self.classes_``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the confidence scores.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      "     |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      "     |          this class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the predictions.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          Vector containing the class labels for each sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class MultiTaskElasticNet(Lasso)\n",
      "     |  MultiTaskElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer.\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskElasticNet is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |      + alpha * l1_ratio * ||W||_21\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = sum_i sqrt(sum_j W_ij ^ 2)\n",
      "     |  \n",
      "     |  i.e. the sum of norms of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_elastic_net>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the L1/L2 term. Defaults to 1.0.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.5\n",
      "     |      The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n",
      "     |      For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n",
      "     |      is an L2 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_targets, n_features)\n",
      "     |      Parameter vector (W in the cost function formula). If a 1D y is\n",
      "     |      passed in at fit (non multi-task usage), ``coef_`` is then a 1D array.\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  dual_gap_ : float\n",
      "     |      The dual gaps at the end of the optimization.\n",
      "     |  \n",
      "     |  eps_ : float\n",
      "     |      The tolerance scaled scaled by the variance of the target `y`.\n",
      "     |  \n",
      "     |  sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n",
      "     |      Sparse representation of the `coef_`.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\n",
      "     |      cross-validation.\n",
      "     |  ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |  MultiTaskLasso : Multi-task Lasso model trained with L1/L2\n",
      "     |      mixed-norm as regularizer.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X and y arguments of the fit\n",
      "     |  method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskElasticNet(alpha=0.1)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]], [[0, 0], [1, 1], [2, 2]])\n",
      "     |  MultiTaskElasticNet(alpha=0.1)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.45663524 0.45612256]\n",
      "     |   [0.45663524 0.45612256]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [0.0872422 0.0872422]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskElasticNet\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit MultiTaskElasticNet model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Data.\n",
      "     |      y : ndarray of shape (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  param = 'positive'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Lasso:\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._coordinate_descent.MultiTaskElasticNet, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskElasticNet from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._coordinate_descent.MultiTaskElasticNet, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskElasticNet from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Lasso:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params) from sklearn.linear_model._coordinate_descent\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : array-like, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array-like of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      "     |      MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |      ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |      ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from sklearn.linear_model import enet_path\n",
      "     |      >>> from sklearn.datasets import make_regression\n",
      "     |      >>> X, y, true_coef = make_regression(\n",
      "     |      ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n",
      "     |      ... )\n",
      "     |      >>> true_coef\n",
      "     |      array([ 0.        ,  0.        ,  0.        , 97.9, 45.7])\n",
      "     |      >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n",
      "     |      >>> alphas.shape\n",
      "     |      (3,)\n",
      "     |      >>> estimated_coef\n",
      "     |       array([[ 0.,  0.787,  0.568],\n",
      "     |              [ 0.,  1.120,  0.620],\n",
      "     |              [-0., -2.129, -1.128],\n",
      "     |              [ 0., 23.046, 88.939],\n",
      "     |              [ 0., 10.637, 41.566]])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      Sparse representation of the fitted `coef_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class MultiTaskElasticNetCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  MultiTaskElasticNetCV(*, l1_ratio=0.5, eps=0.001, n_alphas='deprecated', alphas='warn', fit_intercept=True, max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskElasticNet is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n",
      "     |      + alpha * l1_ratio * ||W||_21\n",
      "     |      + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_elastic_net>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.15\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  l1_ratio : float or list of float, default=0.5\n",
      "     |      The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.\n",
      "     |      For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it\n",
      "     |      is an L2 penalty.\n",
      "     |      For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.\n",
      "     |      This parameter can be a list, in which case the different\n",
      "     |      values are tested by cross-validation and the one giving the best\n",
      "     |      prediction score is used. Note that a good choice of list of\n",
      "     |      values for l1_ratio is often to put more values close to 1\n",
      "     |      (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,\n",
      "     |      .9, .95, .99, 1]``.\n",
      "     |  \n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.7\n",
      "     |          `n_alphas` was deprecated in 1.7 and will be removed in 1.9. Use `alphas`\n",
      "     |          instead.\n",
      "     |  \n",
      "     |  alphas : array-like or int, default=None\n",
      "     |      Values of alphas to test along the regularization path, used for each l1_ratio.\n",
      "     |      If int, `alphas` values are generated automatically.\n",
      "     |      If array-like, list of alpha values to use.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.7\n",
      "     |          `alphas` accepts an integer value which removes the need to pass\n",
      "     |          `n_alphas`.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.7\n",
      "     |          `alphas=None` was deprecated in 1.7 and will be removed in 1.9, at which\n",
      "     |          point the default value will be set to 100.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=0\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation. Note that this is\n",
      "     |      used only if multiple values for l1_ratio are given.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_targets, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation.\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)\n",
      "     |      Mean square error for the test set on each fold, varying alpha.\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)\n",
      "     |      The grid of alphas used for fitting, for each l1_ratio.\n",
      "     |  \n",
      "     |  l1_ratio_ : float\n",
      "     |      Best l1_ratio obtained by cross-validation.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  dual_gap_ : float\n",
      "     |      The dual gap at the end of the optimization for the optimal alpha.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNet : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |  ElasticNetCV : Elastic net model with best model selection by\n",
      "     |      cross-validation.\n",
      "     |  MultiTaskLassoCV : Multi-task Lasso model trained with L1 norm\n",
      "     |      as regularizer and built-in cross-validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  In `fit`, once the best parameters `l1_ratio` and `alpha` are found through\n",
      "     |  cross-validation, the model is fit again using the entire training set.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the `X` and `y` arguments of the\n",
      "     |  `fit` method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskElasticNetCV(cv=3)\n",
      "     |  >>> clf.fit([[0,0], [1, 1], [2, 2]],\n",
      "     |  ...         [[0, 0], [1, 1], [2, 2]])\n",
      "     |  MultiTaskElasticNetCV(cv=3)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.52875032 0.46958558]\n",
      "     |   [0.52875032 0.46958558]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [0.00166409 0.00166409]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskElasticNetCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, l1_ratio=0.5, eps=0.001, n_alphas='deprecated', alphas='warn', fit_intercept=True, max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, **params)\n",
      "     |      Fit MultiTaskElasticNet model with coordinate descent.\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      y : ndarray of shape (n_samples, n_targets)\n",
      "     |          Training target variable. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      **params : dict, default=None\n",
      "     |          Parameters to be passed to the CV splitter.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |              Only available if `enable_metadata_routing=True`,\n",
      "     |              which can be set by using\n",
      "     |              ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns MultiTaskElasticNet instance.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params) from sklearn.linear_model._coordinate_descent\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : array-like, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array-like of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      "     |      MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |      ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |      ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from sklearn.linear_model import enet_path\n",
      "     |      >>> from sklearn.datasets import make_regression\n",
      "     |      >>> X, y, true_coef = make_regression(\n",
      "     |      ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n",
      "     |      ... )\n",
      "     |      >>> true_coef\n",
      "     |      array([ 0.        ,  0.        ,  0.        , 97.9, 45.7])\n",
      "     |      >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n",
      "     |      >>> alphas.shape\n",
      "     |      (3,)\n",
      "     |      >>> estimated_coef\n",
      "     |       array([[ 0.,  0.787,  0.568],\n",
      "     |              [ 0.,  1.120,  0.620],\n",
      "     |              [-0., -2.129, -1.128],\n",
      "     |              [ 0., 23.046, 88.939],\n",
      "     |              [ 0., 10.637, 41.566]])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class MultiTaskLasso(MultiTaskElasticNet)\n",
      "     |  MultiTaskLasso(alpha=1.0, *, fit_intercept=True, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n",
      "     |  \n",
      "     |  The optimization objective for Lasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Constant that multiplies the L1/L2 term. Defaults to 1.0.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_targets, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance.\n",
      "     |  \n",
      "     |  dual_gap_ : ndarray of shape (n_alphas,)\n",
      "     |      The dual gaps at the end of the optimization for each alpha.\n",
      "     |  \n",
      "     |  eps_ : float\n",
      "     |      The tolerance scaled scaled by the variance of the target `y`.\n",
      "     |  \n",
      "     |  sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n",
      "     |      Sparse representation of the `coef_`.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Lasso: Linear Model trained with L1 prior as regularizer (aka the Lasso).\n",
      "     |  MultiTaskLassoCV: Multi-task L1 regularized linear model with built-in\n",
      "     |      cross-validation.\n",
      "     |  MultiTaskElasticNetCV: Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the X and y arguments of the fit\n",
      "     |  method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.MultiTaskLasso(alpha=0.1)\n",
      "     |  >>> clf.fit([[0, 1], [1, 2], [2, 4]], [[0, 0], [1, 1], [2, 3]])\n",
      "     |  MultiTaskLasso(alpha=0.1)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.         0.60809415]\n",
      "     |  [0.         0.94592424]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [-0.41888636 -0.87382323]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskLasso\n",
      "     |      MultiTaskElasticNet\n",
      "     |      Lasso\n",
      "     |      ElasticNet\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from MultiTaskElasticNet:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit MultiTaskElasticNet model with coordinate descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Data.\n",
      "     |      y : ndarray of shape (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Coordinate descent is an algorithm that considers each column of\n",
      "     |      data at a time hence it will automatically convert the X input\n",
      "     |      as a Fortran-contiguous numpy array if necessary.\n",
      "     |      \n",
      "     |      To avoid memory re-allocation it is advised to allocate the\n",
      "     |      initial data in memory directly using that format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from MultiTaskElasticNet:\n",
      "     |  \n",
      "     |  param = 'positive'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Lasso:\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._coordinate_descent.MultiTaskLasso, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskLasso from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._coordinate_descent.MultiTaskLasso, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskLasso from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from Lasso:\n",
      "     |  \n",
      "     |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params) from sklearn.linear_model._coordinate_descent\n",
      "     |      Compute elastic net path with coordinate descent.\n",
      "     |      \n",
      "     |      The elastic net optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "     |          + alpha * l1_ratio * ||w||_1\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "     |          + alpha * l1_ratio * ||W||_21\n",
      "     |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      l1_ratio : float, default=0.5\n",
      "     |          Number between 0 and 1 passed to elastic net (scaling between\n",
      "     |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : array-like, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If None alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array-like of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      check_input : bool, default=True\n",
      "     |          If set to False, the input validation checks are skipped (including the\n",
      "     |          Gram matrix when provided). It is assumed that they are handled\n",
      "     |          by the caller.\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |          (Is returned when ``return_n_iter`` is set to True).\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      "     |      MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "     |      ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "     |      ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> from sklearn.linear_model import enet_path\n",
      "     |      >>> from sklearn.datasets import make_regression\n",
      "     |      >>> X, y, true_coef = make_regression(\n",
      "     |      ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n",
      "     |      ... )\n",
      "     |      >>> true_coef\n",
      "     |      array([ 0.        ,  0.        ,  0.        , 97.9, 45.7])\n",
      "     |      >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n",
      "     |      >>> alphas.shape\n",
      "     |      (3,)\n",
      "     |      >>> estimated_coef\n",
      "     |       array([[ 0.,  0.787,  0.568],\n",
      "     |              [ 0.,  1.120,  0.620],\n",
      "     |              [-0., -2.129, -1.128],\n",
      "     |              [ 0., 23.046, 88.939],\n",
      "     |              [ 0., 10.637, 41.566]])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from ElasticNet:\n",
      "     |  \n",
      "     |  sparse_coef_\n",
      "     |      Sparse representation of the fitted `coef_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class MultiTaskLassoCV(sklearn.base.RegressorMixin, LinearModelCV)\n",
      "     |  MultiTaskLassoCV(*, eps=0.001, n_alphas='deprecated', alphas='warn', fit_intercept=True, max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |  \n",
      "     |  Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  The optimization objective for MultiTaskLasso is::\n",
      "     |  \n",
      "     |      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21\n",
      "     |  \n",
      "     |  Where::\n",
      "     |  \n",
      "     |      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |  \n",
      "     |  i.e. the sum of norm of each row.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.15\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  eps : float, default=1e-3\n",
      "     |      Length of the path. ``eps=1e-3`` means that\n",
      "     |      ``alpha_min / alpha_max = 1e-3``.\n",
      "     |  \n",
      "     |  n_alphas : int, default=100\n",
      "     |      Number of alphas along the regularization path.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.7\n",
      "     |          `n_alphas` was deprecated in 1.7 and will be removed in 1.9. Use `alphas`\n",
      "     |          instead.\n",
      "     |  \n",
      "     |  alphas : array-like or int, default=None\n",
      "     |      Values of alphas to test along the regularization path.\n",
      "     |      If int, `alphas` values are generated automatically.\n",
      "     |      If array-like, list of alpha values to use.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.7\n",
      "     |          `alphas` accepts an integer value which removes the need to pass\n",
      "     |          `n_alphas`.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.7\n",
      "     |          `alphas=None` was deprecated in 1.7 and will be removed in 1.9, at which\n",
      "     |          point the default value will be set to 100.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of iterations.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The tolerance for the optimization: if the updates are\n",
      "     |      smaller than ``tol``, the optimization code checks the\n",
      "     |      dual gap for optimality and continues until it is smaller\n",
      "     |      than ``tol``.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - int, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Amount of verbosity.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation. Note that this is\n",
      "     |      used only if multiple values for l1_ratio are given.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The seed of the pseudo random number generator that selects a random\n",
      "     |      feature to update. Used when ``selection`` == 'random'.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      "     |      If set to 'random', a random coefficient is updated every iteration\n",
      "     |      rather than looping over features sequentially by default. This\n",
      "     |      (setting to 'random') often leads to significantly faster convergence\n",
      "     |      especially when tol is higher than 1e-4.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_targets, n_features)\n",
      "     |      Parameter vector (W in the cost function formula).\n",
      "     |      Note that ``coef_`` stores the transpose of ``W``, ``W.T``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      The amount of penalization chosen by cross validation.\n",
      "     |  \n",
      "     |  mse_path_ : ndarray of shape (n_alphas, n_folds)\n",
      "     |      Mean square error for the test set on each fold, varying alpha.\n",
      "     |  \n",
      "     |  alphas_ : ndarray of shape (n_alphas,)\n",
      "     |      The grid of alphas used for fitting.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations run by the coordinate descent solver to reach\n",
      "     |      the specified tolerance for the optimal alpha.\n",
      "     |  \n",
      "     |  dual_gap_ : float\n",
      "     |      The dual gap at the end of the optimization for the optimal alpha.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2\n",
      "     |      mixed-norm as regularizer.\n",
      "     |  ElasticNetCV : Elastic net model with best model selection by\n",
      "     |      cross-validation.\n",
      "     |  MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in\n",
      "     |      cross-validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The algorithm used to fit the model is coordinate descent.\n",
      "     |  \n",
      "     |  In `fit`, once the best parameter `alpha` is found through\n",
      "     |  cross-validation, the model is fit again using the entire training set.\n",
      "     |  \n",
      "     |  To avoid unnecessary memory duplication the `X` and `y` arguments of the\n",
      "     |  `fit` method should be directly passed as Fortran-contiguous numpy arrays.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import MultiTaskLassoCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> from sklearn.metrics import r2_score\n",
      "     |  >>> X, y = make_regression(n_targets=2, noise=4, random_state=0)\n",
      "     |  >>> reg = MultiTaskLassoCV(cv=5, random_state=0).fit(X, y)\n",
      "     |  >>> r2_score(y, reg.predict(X))\n",
      "     |  0.9994\n",
      "     |  >>> reg.alpha_\n",
      "     |  np.float64(0.5713)\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([[153.7971,  94.9015]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiTaskLassoCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      LinearModelCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, eps=0.001, n_alphas='deprecated', alphas='warn', fit_intercept=True, max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=None, random_state=None, selection='cyclic')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, **params)\n",
      "     |      Fit MultiTaskLasso model with coordinate descent.\n",
      "     |      \n",
      "     |      Fit is on grid of alphas and best alpha estimated by cross-validation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Data.\n",
      "     |      y : ndarray of shape (n_samples, n_targets)\n",
      "     |          Target. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      **params : dict, default=None\n",
      "     |          Parameters to be passed to the CV splitter.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |              Only available if `enable_metadata_routing=True`,\n",
      "     |              which can be set by using\n",
      "     |              ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of fitted model.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._coordinate_descent.MultiTaskLassoCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskLassoCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  path = lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params) from sklearn.linear_model._coordinate_descent\n",
      "     |      Compute Lasso path with coordinate descent.\n",
      "     |      \n",
      "     |      The Lasso optimization function varies for mono and multi-outputs.\n",
      "     |      \n",
      "     |      For mono-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "     |      \n",
      "     |      For multi-output tasks it is::\n",
      "     |      \n",
      "     |          (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "     |      \n",
      "     |      Where::\n",
      "     |      \n",
      "     |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "     |      \n",
      "     |      i.e. the sum of norm of each row.\n",
      "     |      \n",
      "     |      Read more in the :ref:`User Guide <lasso>`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "     |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "     |          can be sparse.\n",
      "     |      \n",
      "     |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      eps : float, default=1e-3\n",
      "     |          Length of the path. ``eps=1e-3`` means that\n",
      "     |          ``alpha_min / alpha_max = 1e-3``.\n",
      "     |      \n",
      "     |      n_alphas : int, default=100\n",
      "     |          Number of alphas along the regularization path.\n",
      "     |      \n",
      "     |      alphas : array-like, default=None\n",
      "     |          List of alphas where to compute the models.\n",
      "     |          If ``None`` alphas are set automatically.\n",
      "     |      \n",
      "     |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "     |          Whether to use a precomputed Gram matrix to speed up\n",
      "     |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "     |          matrix can also be passed as argument.\n",
      "     |      \n",
      "     |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "     |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "     |          only when the Gram matrix is precomputed.\n",
      "     |      \n",
      "     |      copy_X : bool, default=True\n",
      "     |          If ``True``, X will be copied; else, it may be overwritten.\n",
      "     |      \n",
      "     |      coef_init : array-like of shape (n_features, ), default=None\n",
      "     |          The initial values of the coefficients.\n",
      "     |      \n",
      "     |      verbose : bool or int, default=False\n",
      "     |          Amount of verbosity.\n",
      "     |      \n",
      "     |      return_n_iter : bool, default=False\n",
      "     |          Whether to return the number of iterations or not.\n",
      "     |      \n",
      "     |      positive : bool, default=False\n",
      "     |          If set to True, forces coefficients to be positive.\n",
      "     |          (Only allowed when ``y.ndim == 1``).\n",
      "     |      \n",
      "     |      **params : kwargs\n",
      "     |          Keyword arguments passed to the coordinate descent solver.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      alphas : ndarray of shape (n_alphas,)\n",
      "     |          The alphas along the path where models are computed.\n",
      "     |      \n",
      "     |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "     |          Coefficients along the path.\n",
      "     |      \n",
      "     |      dual_gaps : ndarray of shape (n_alphas,)\n",
      "     |          The dual gaps at the end of the optimization for each alpha.\n",
      "     |      \n",
      "     |      n_iters : list of int\n",
      "     |          The number of iterations taken by the coordinate descent optimizer to\n",
      "     |          reach the specified tolerance for each alpha.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      lars_path : Compute Least Angle Regression or Lasso path using LARS\n",
      "     |          algorithm.\n",
      "     |      Lasso : The Lasso is a linear model that estimates sparse coefficients.\n",
      "     |      LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |      LassoCV : Lasso linear model with iterative fitting along a regularization\n",
      "     |          path.\n",
      "     |      LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n",
      "     |      sklearn.decomposition.sparse_encode : Estimator that can be used to\n",
      "     |          transform signals into sparse linear combination of atoms from a fixed.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For an example, see\n",
      "     |      :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n",
      "     |      <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n",
      "     |      \n",
      "     |      To avoid unnecessary memory duplication the X argument of the fit method\n",
      "     |      should be directly passed as a Fortran-contiguous numpy array.\n",
      "     |      \n",
      "     |      Note that in certain cases, the Lars solver may be significantly\n",
      "     |      faster to implement this functionality. In particular, linear\n",
      "     |      interpolation can be used to retrieve model coefficients between the\n",
      "     |      values output by lars_path\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      Comparing lasso_path and lars_path with interpolation:\n",
      "     |      \n",
      "     |      >>> import numpy as np\n",
      "     |      >>> from sklearn.linear_model import lasso_path\n",
      "     |      >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "     |      >>> y = np.array([1, 2, 3.1])\n",
      "     |      >>> # Use lasso_path to compute a coefficient path\n",
      "     |      >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "     |      >>> print(coef_path)\n",
      "     |      [[0.         0.         0.46874778]\n",
      "     |       [0.2159048  0.4425765  0.23689075]]\n",
      "     |      \n",
      "     |      >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "     |      >>> # same path\n",
      "     |      >>> from sklearn.linear_model import lars_path\n",
      "     |      >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "     |      >>> from scipy import interpolate\n",
      "     |      >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "     |      ...                                             coef_path_lars[:, ::-1])\n",
      "     |      >>> print(coef_path_continuous([5., 1., .5]))\n",
      "     |      [[0.         0.         0.46915237]\n",
      "     |       [0.2159048  0.4425765  0.23668876]]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LinearModelCV:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._coordinate_descent.MultiTaskLassoCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._coordinate_descent.MultiTaskLassoCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class OrthogonalMatchingPursuit(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  OrthogonalMatchingPursuit(*, n_nonzero_coefs=None, tol=None, fit_intercept=True, precompute='auto')\n",
      "     |  \n",
      "     |  Orthogonal Matching Pursuit model (OMP).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <omp>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_nonzero_coefs : int, default=None\n",
      "     |      Desired number of non-zero entries in the solution. Ignored if `tol` is set.\n",
      "     |      When `None` and `tol` is also `None`, this value is either set to 10% of\n",
      "     |      `n_features` or 1, whichever is greater.\n",
      "     |  \n",
      "     |  tol : float, default=None\n",
      "     |      Maximum squared norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  precompute : 'auto' or bool, default='auto'\n",
      "     |      Whether to use a precomputed Gram and Xy matrix to speed up\n",
      "     |      calculations. Improves performance when :term:`n_targets` or\n",
      "     |      :term:`n_samples` is very large. Note that if you already have such\n",
      "     |      matrices, you can pass them directly to the fit method.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the formula).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int or array-like\n",
      "     |      Number of active features across every target.\n",
      "     |  \n",
      "     |  n_nonzero_coefs_ : int or None\n",
      "     |      The number of non-zero coefficients in the solution or `None` when `tol` is\n",
      "     |      set. If `n_nonzero_coefs` is None and `tol` is None this value is either set\n",
      "     |      to 10% of `n_features` or 1, whichever is greater.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.\n",
      "     |  orthogonal_mp_gram :  Solves n_targets Orthogonal Matching Pursuit\n",
      "     |      problems using only the Gram matrix X.T * X and the product X.T * y.\n",
      "     |  lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.\n",
      "     |  Lars : Least Angle Regression model a.k.a. LAR.\n",
      "     |  LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |  sklearn.decomposition.sparse_encode : Generic sparse coding.\n",
      "     |      Each column of the result is the solution to a Lasso problem.\n",
      "     |  OrthogonalMatchingPursuitCV : Cross-validated\n",
      "     |      Orthogonal Matching Pursuit model (OMP).\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n",
      "     |  Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "     |  Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "     |  (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "     |  \n",
      "     |  This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "     |  M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "     |  Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "     |  https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import OrthogonalMatchingPursuit\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(noise=4, random_state=0)\n",
      "     |  >>> reg = OrthogonalMatchingPursuit().fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9991\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-78.3854])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OrthogonalMatchingPursuit\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_nonzero_coefs=None, tol=None, fit_intercept=True, precompute='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._omp.OrthogonalMatchingPursuit, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._omp.OrthogonalMatchingPursuit from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class OrthogonalMatchingPursuitCV(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  OrthogonalMatchingPursuitCV(*, copy=True, fit_intercept=True, max_iter=None, cv=None, n_jobs=None, verbose=False)\n",
      "     |  \n",
      "     |  Cross-validated Orthogonal Matching Pursuit model (OMP).\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <omp>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  copy : bool, default=True\n",
      "     |      Whether the design matrix X must be copied by the algorithm. A false\n",
      "     |      value is only helpful if X is already Fortran-ordered, otherwise a\n",
      "     |      copy is made anyway.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  max_iter : int, default=None\n",
      "     |      Maximum numbers of iterations to perform, therefore maximum features\n",
      "     |      to include. 10% of ``n_features`` but at least 5 if available.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  verbose : bool or int, default=False\n",
      "     |      Sets the verbosity amount.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Parameter vector (w in the problem formulation).\n",
      "     |  \n",
      "     |  n_nonzero_coefs_ : int\n",
      "     |      Estimated number of non-zero coefficients giving the best mean squared\n",
      "     |      error over the cross-validation folds.\n",
      "     |  \n",
      "     |  n_iter_ : int or array-like\n",
      "     |      Number of active features across every target for the model refit with\n",
      "     |      the best hyperparameters got by cross-validating across all folds.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.\n",
      "     |  orthogonal_mp_gram : Solves n_targets Orthogonal Matching Pursuit\n",
      "     |      problems using only the Gram matrix X.T * X and the product X.T * y.\n",
      "     |  lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.\n",
      "     |  Lars : Least Angle Regression model a.k.a. LAR.\n",
      "     |  LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "     |  OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model (OMP).\n",
      "     |  LarsCV : Cross-validated Least Angle Regression model.\n",
      "     |  LassoLarsCV : Cross-validated Lasso model fit with Least Angle Regression.\n",
      "     |  sklearn.decomposition.sparse_encode : Generic sparse coding.\n",
      "     |      Each column of the result is the solution to a Lasso problem.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  In `fit`, once the optimal number of non-zero coefficients is found through\n",
      "     |  cross-validation, the model is fit again using the entire training set.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import OrthogonalMatchingPursuitCV\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_features=100, n_informative=10,\n",
      "     |  ...                        noise=4, random_state=0)\n",
      "     |  >>> reg = OrthogonalMatchingPursuitCV(cv=5).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9991\n",
      "     |  >>> reg.n_nonzero_coefs_\n",
      "     |  np.int64(10)\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-78.3854])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OrthogonalMatchingPursuitCV\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, copy=True, fit_intercept=True, max_iter=None, cv=None, n_jobs=None, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, **fit_params)\n",
      "     |      Fit the model using X, y as training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Parameters to pass to the underlying splitter.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |              Only available if `enable_metadata_routing=True`,\n",
      "     |              which can be set by using\n",
      "     |              ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.4\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._omp.OrthogonalMatchingPursuitCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._omp.OrthogonalMatchingPursuitCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class PassiveAggressiveClassifier(sklearn.linear_model._stochastic_gradient.BaseSGDClassifier)\n",
      "     |  PassiveAggressiveClassifier(*, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='hinge', n_jobs=None, random_state=None, warm_start=False, class_weight=None, average=False)\n",
      "     |  \n",
      "     |  Passive Aggressive Classifier.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <passive_aggressive>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  C : float, default=1.0\n",
      "     |      Maximum step size (regularization). Defaults to 1.0.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`~sklearn.linear_model.PassiveAggressiveClassifier.partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol).\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a stratified fraction of training data as validation and terminate\n",
      "     |      training when validation score is not improving by at least `tol` for\n",
      "     |      `n_iter_no_change` consecutive epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if early_stopping is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  loss : str, default=\"hinge\"\n",
      "     |      The loss function to be used:\n",
      "     |      hinge: equivalent to PA-I in the reference paper.\n",
      "     |      squared_hinge: equivalent to PA-II in the reference paper.\n",
      "     |  \n",
      "     |  n_jobs : int or None, default=None\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used to shuffle the training data, when ``shuffle`` is set to\n",
      "     |      ``True``. Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\" or None,             default=None\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         parameter *class_weight* to automatically weight samples.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So average=10 will begin averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |         parameter *average* to use weights averaging in SGD.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |      For multiclass fits, it is the maximum over every binary fit.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The unique classes labels.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples + 1)``.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SGDClassifier : Incrementally trained logistic regression.\n",
      "     |  Perceptron : Linear perceptron classifier.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Online Passive-Aggressive Algorithms\n",
      "     |  <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n",
      "     |  K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import PassiveAggressiveClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
      "     |  >>> clf = PassiveAggressiveClassifier(max_iter=1000, random_state=0,\n",
      "     |  ... tol=1e-3)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  PassiveAggressiveClassifier(random_state=0)\n",
      "     |  >>> print(clf.coef_)\n",
      "     |  [[0.26642044 0.45070924 0.67251877 0.64185414]]\n",
      "     |  >>> print(clf.intercept_)\n",
      "     |  [1.84127814]\n",
      "     |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PassiveAggressiveClassifier\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGDClassifier\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='hinge', n_jobs=None, random_state=None, warm_start=False, class_weight=None, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_classes, n_features)\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : ndarray of shape (n_classes,)\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Subset of the training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Subset of the target values.\n",
      "     |      \n",
      "     |      classes : ndarray of shape (n_classes,)\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier, *, coef_init: Union[bool, NoneType, str] = '$UNCHANGED$', intercept_init: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      coef_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``coef_init`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      intercept_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``intercept_init`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_partial_fit_request(self: sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier, *, classes: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``partial_fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      classes : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``classes`` parameter in ``partial_fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model._stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the confidence scores.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      "     |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      "     |          this class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the predictions.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          Vector containing the class labels for each sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class PassiveAggressiveRegressor(sklearn.linear_model._stochastic_gradient.BaseSGDRegressor)\n",
      "     |  PassiveAggressiveRegressor(*, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='epsilon_insensitive', epsilon=0.1, random_state=None, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  Passive Aggressive Regressor.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <passive_aggressive>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  C : float, default=1.0\n",
      "     |      Maximum step size (regularization). Defaults to 1.0.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered. Defaults to True.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`~sklearn.linear_model.PassiveAggressiveRegressor.partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol).\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation.\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a fraction of training data as validation and terminate\n",
      "     |      training when validation score is not improving by at least tol for\n",
      "     |      n_iter_no_change consecutive epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if early_stopping is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  loss : str, default=\"epsilon_insensitive\"\n",
      "     |      The loss function to be used:\n",
      "     |      epsilon_insensitive: equivalent to PA-I in the reference paper.\n",
      "     |      squared_epsilon_insensitive: equivalent to PA-II in the reference\n",
      "     |      paper.\n",
      "     |  \n",
      "     |  epsilon : float, default=0.1\n",
      "     |      If the difference between the current prediction and the correct label\n",
      "     |      is below this threshold, the model is not updated.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used to shuffle the training data, when ``shuffle`` is set to\n",
      "     |      ``True``. Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So average=10 will begin averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |         parameter *average* to use weights averaging in SGD.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples + 1)``.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  SGDRegressor : Linear model fitted by minimizing a regularized\n",
      "     |      empirical loss with SGD.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  Online Passive-Aggressive Algorithms\n",
      "     |  <http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>\n",
      "     |  K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006).\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import PassiveAggressiveRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  \n",
      "     |  >>> X, y = make_regression(n_features=4, random_state=0)\n",
      "     |  >>> regr = PassiveAggressiveRegressor(max_iter=100, random_state=0,\n",
      "     |  ... tol=1e-3)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  PassiveAggressiveRegressor(max_iter=100, random_state=0)\n",
      "     |  >>> print(regr.coef_)\n",
      "     |  [20.48736655 34.18818427 67.59122734 87.94731329]\n",
      "     |  >>> print(regr.intercept_)\n",
      "     |  [-0.02306214]\n",
      "     |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "     |  [-0.02306214]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PassiveAggressiveRegressor\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGDRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, C=1.0, fit_intercept=True, max_iter=1000, tol=0.001, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, shuffle=True, verbose=0, loss='epsilon_insensitive', epsilon=0.1, random_state=None, warm_start=False, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      coef_init : array, shape = [n_features]\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : array, shape = [1]\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y)\n",
      "     |      Fit linear model with Passive Aggressive algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Subset of training data.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Subset of target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor, *, coef_init: Union[bool, NoneType, str] = '$UNCHANGED$', intercept_init: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      coef_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``coef_init`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      intercept_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``intercept_init`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGDRegressor:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ndarray of shape (n_samples,)\n",
      "     |         Predicted target values per element in X.\n",
      "     |  \n",
      "     |  set_partial_fit_request(self: sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``partial_fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model._stochastic_gradient.BaseSGDRegressor:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class Perceptron(sklearn.linear_model._stochastic_gradient.BaseSGDClassifier)\n",
      "     |  Perceptron(*, penalty=None, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, eta0=1.0, n_jobs=None, random_state=0, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False)\n",
      "     |  \n",
      "     |  Linear perceptron classifier.\n",
      "     |  \n",
      "     |  The implementation is a wrapper around :class:`~sklearn.linear_model.SGDClassifier`\n",
      "     |  by fixing the `loss` and `learning_rate` parameters as::\n",
      "     |  \n",
      "     |      SGDClassifier(loss=\"perceptron\", learning_rate=\"constant\")\n",
      "     |  \n",
      "     |  Other available parameters are described below and are forwarded to\n",
      "     |  :class:`~sklearn.linear_model.SGDClassifier`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <perceptron>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  penalty : {'l2','l1','elasticnet'}, default=None\n",
      "     |      The penalty (aka regularization term) to be used.\n",
      "     |  \n",
      "     |  alpha : float, default=0.0001\n",
      "     |      Constant that multiplies the regularization term if regularization is\n",
      "     |      used.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.15\n",
      "     |      The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`.\n",
      "     |      `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1.\n",
      "     |      Only used if `penalty='elasticnet'`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol).\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  eta0 : float, default=1\n",
      "     |      Constant by which the updates are multiplied.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=0\n",
      "     |      Used to shuffle the training data, when ``shuffle`` is set to\n",
      "     |      ``True``. Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a stratified fraction of training data as validation and terminate\n",
      "     |      training when validation score is not improving by at least `tol` for\n",
      "     |      `n_iter_no_change` consecutive epochs.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if early_stopping is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution. See\n",
      "     |      :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The unique classes labels.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |      For multiclass fits, it is the maximum over every binary fit.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples + 1)``.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.linear_model.SGDClassifier : Linear classifiers\n",
      "     |      (SVM, logistic regression, etc.) with SGD training.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  ``Perceptron`` is a classification algorithm which shares the same\n",
      "     |  underlying implementation with ``SGDClassifier``. In fact,\n",
      "     |  ``Perceptron()`` is equivalent to `SGDClassifier(loss=\"perceptron\",\n",
      "     |  eta0=1, learning_rate=\"constant\", penalty=None)`.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  https://en.wikipedia.org/wiki/Perceptron and references therein.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_digits\n",
      "     |  >>> from sklearn.linear_model import Perceptron\n",
      "     |  >>> X, y = load_digits(return_X_y=True)\n",
      "     |  >>> clf = Perceptron(tol=1e-3, random_state=0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  Perceptron()\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.939...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Perceptron\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGDClassifier\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      sklearn.linear_model._stochastic_gradient.BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, penalty=None, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, eta0=1.0, n_jobs=None, random_state=0, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._perceptron.Perceptron, *, coef_init: Union[bool, NoneType, str] = '$UNCHANGED$', intercept_init: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._perceptron.Perceptron from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      coef_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``coef_init`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      intercept_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``intercept_init`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_partial_fit_request(self: sklearn.linear_model._perceptron.Perceptron, *, classes: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._perceptron.Perceptron from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``partial_fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      classes : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``classes`` parameter in ``partial_fit``.\n",
      "     |      \n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._perceptron.Perceptron, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._perceptron.Perceptron from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_classes, n_features), default=None\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : ndarray of shape (n_classes,), default=None\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed. These weights will\n",
      "     |          be multiplied with class_weight (passed through the\n",
      "     |          constructor) if class_weight is specified.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Perform one epoch of stochastic gradient descent on given samples.\n",
      "     |      \n",
      "     |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      "     |      guaranteed that a minimum of the cost function is reached after calling\n",
      "     |      it once. Matters such as objective convergence, early stopping, and\n",
      "     |      learning rate adjustments should be handled by the user.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of the training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Subset of the target values.\n",
      "     |      \n",
      "     |      classes : ndarray of shape (n_classes,), default=None\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.linear_model._stochastic_gradient.BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the confidence scores.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      "     |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      "     |          this class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the predictions.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          Vector containing the class labels for each sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class PoissonRegressor(_GeneralizedLinearRegressor)\n",
      "     |  PoissonRegressor(*, alpha=1.0, fit_intercept=True, solver='lbfgs', max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |  \n",
      "     |  Generalized Linear Model with a Poisson distribution.\n",
      "     |  \n",
      "     |  This regressor uses the 'log' link function.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <Generalized_linear_models>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1\n",
      "     |      Constant that multiplies the L2 penalty term and determines the\n",
      "     |      regularization strength. ``alpha = 0`` is equivalent to unpenalized\n",
      "     |      GLMs. In this case, the design matrix `X` must have full column rank\n",
      "     |      (no collinearities).\n",
      "     |      Values of `alpha` must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the linear predictor (`X @ coef + intercept`).\n",
      "     |  \n",
      "     |  solver : {'lbfgs', 'newton-cholesky'}, default='lbfgs'\n",
      "     |      Algorithm to use in the optimization problem:\n",
      "     |  \n",
      "     |      'lbfgs'\n",
      "     |          Calls scipy's L-BFGS-B optimizer.\n",
      "     |  \n",
      "     |      'newton-cholesky'\n",
      "     |          Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to\n",
      "     |          iterated reweighted least squares) with an inner Cholesky based solver.\n",
      "     |          This solver is a good choice for `n_samples` >> `n_features`, especially\n",
      "     |          with one-hot encoded categorical features with rare categories. Be aware\n",
      "     |          that the memory usage of this solver has a quadratic dependency on\n",
      "     |          `n_features` because it explicitly computes the Hessian matrix.\n",
      "     |  \n",
      "     |          .. versionadded:: 1.2\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      The maximal number of iterations for the solver.\n",
      "     |      Values must be in the range `[1, inf)`.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Stopping criterion. For the lbfgs solver,\n",
      "     |      the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n",
      "     |      where ``g_j`` is the j-th component of the gradient (derivative) of\n",
      "     |      the objective function.\n",
      "     |      Values must be in the range `(0.0, inf)`.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      If set to ``True``, reuse the solution of the previous call to ``fit``\n",
      "     |      as initialization for ``coef_`` and ``intercept_`` .\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the lbfgs solver set verbose to any positive number for verbosity.\n",
      "     |      Values must be in the range `[0, inf)`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features,)\n",
      "     |      Estimated coefficients for the linear predictor (`X @ coef_ +\n",
      "     |      intercept_`) in the GLM.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Intercept (a.k.a. bias) added to linear predictor.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Actual number of iterations used in the solver.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  TweedieRegressor : Generalized Linear Model with a Tweedie distribution.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.PoissonRegressor()\n",
      "     |  >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n",
      "     |  >>> y = [12, 17, 22, 21]\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  PoissonRegressor()\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  np.float64(0.990)\n",
      "     |  >>> clf.coef_\n",
      "     |  array([0.121, 0.158])\n",
      "     |  >>> clf.intercept_\n",
      "     |  np.float64(2.088)\n",
      "     |  >>> clf.predict([[1, 1], [3, 4]])\n",
      "     |  array([10.676, 21.875])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PoissonRegressor\n",
      "     |      _GeneralizedLinearRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, alpha=1.0, fit_intercept=True, solver='lbfgs', max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._glm.glm.PoissonRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._glm.glm.PoissonRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._glm.glm.PoissonRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._glm.glm.PoissonRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _GeneralizedLinearRegressor:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit a Generalized Linear Model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted model.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using GLM with feature matrix X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array of shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Compute D^2, the percentage of deviance explained.\n",
      "     |      \n",
      "     |      D^2 is a generalization of the coefficient of determination R^2.\n",
      "     |      R^2 uses squared error and D^2 uses the deviance of this GLM, see the\n",
      "     |      :ref:`User Guide <regression_metrics>`.\n",
      "     |      \n",
      "     |      D^2 is defined as\n",
      "     |      :math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n",
      "     |      :math:`D_{null}` is the null deviance, i.e. the deviance of a model\n",
      "     |      with intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\n",
      "     |      The mean :math:`\\bar{y}` is averaged by sample_weight.\n",
      "     |      Best possible score is 1.0 and it can be negative (because the model\n",
      "     |      can be arbitrarily worse).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True values of target.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          D^2 of self.predict(X) w.r.t. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class QuantileRegressor(sklearn.linear_model._base.LinearModel, sklearn.base.RegressorMixin, sklearn.base.BaseEstimator)\n",
      "     |  QuantileRegressor(*, quantile=0.5, alpha=1.0, fit_intercept=True, solver='highs', solver_options=None)\n",
      "     |  \n",
      "     |  Linear regression model that predicts conditional quantiles.\n",
      "     |  \n",
      "     |  The linear :class:`QuantileRegressor` optimizes the pinball loss for a\n",
      "     |  desired `quantile` and is robust to outliers.\n",
      "     |  \n",
      "     |  This model uses an L1 regularization like\n",
      "     |  :class:`~sklearn.linear_model.Lasso`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <quantile_regression>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  quantile : float, default=0.5\n",
      "     |      The quantile that the model tries to predict. It must be strictly\n",
      "     |      between 0 and 1. If 0.5 (default), the model predicts the 50%\n",
      "     |      quantile, i.e. the median.\n",
      "     |  \n",
      "     |  alpha : float, default=1.0\n",
      "     |      Regularization constant that multiplies the L1 penalty term.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether or not to fit the intercept.\n",
      "     |  \n",
      "     |  solver : {'highs-ds', 'highs-ipm', 'highs', 'interior-point',             'revised simplex'}, default='highs'\n",
      "     |      Method used by :func:`scipy.optimize.linprog` to solve the linear\n",
      "     |      programming formulation.\n",
      "     |  \n",
      "     |      It is recommended to use the highs methods because\n",
      "     |      they are the fastest ones. Solvers \"highs-ds\", \"highs-ipm\" and \"highs\"\n",
      "     |      support sparse input data and, in fact, always convert to sparse csc.\n",
      "     |  \n",
      "     |      From `scipy>=1.11.0`, \"interior-point\" is not available anymore.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.4\n",
      "     |         The default of `solver` changed to `\"highs\"` in version 1.4.\n",
      "     |  \n",
      "     |  solver_options : dict, default=None\n",
      "     |      Additional parameters passed to :func:`scipy.optimize.linprog` as\n",
      "     |      options. If `None` and if `solver='interior-point'`, then\n",
      "     |      `{\"lstsq\": True}` is passed to :func:`scipy.optimize.linprog` for the\n",
      "     |      sake of stability.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features,)\n",
      "     |      Estimated coefficients for the features.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      The intercept of the model, aka bias term.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations performed by the solver.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Lasso : The Lasso is a linear model that estimates sparse coefficients\n",
      "     |      with l1 regularization.\n",
      "     |  HuberRegressor : Linear regression model that is robust to outliers.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import QuantileRegressor\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> n_samples, n_features = 10, 2\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> y = rng.randn(n_samples)\n",
      "     |  >>> X = rng.randn(n_samples, n_features)\n",
      "     |  >>> # the two following lines are optional in practice\n",
      "     |  >>> from sklearn.utils.fixes import sp_version, parse_version\n",
      "     |  >>> reg = QuantileRegressor(quantile=0.8).fit(X, y)\n",
      "     |  >>> np.mean(y <= reg.predict(X))\n",
      "     |  np.float64(0.8)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      QuantileRegressor\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, quantile=0.5, alpha=1.0, fit_intercept=True, solver='highs', solver_options=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the model according to the given training data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._quantile.QuantileRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._quantile.QuantileRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._quantile.QuantileRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._quantile.QuantileRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class RANSACRegressor(sklearn.base.MetaEstimatorMixin, sklearn.base.RegressorMixin, sklearn.base.MultiOutputMixin, sklearn.base.BaseEstimator)\n",
      "     |  RANSACRegressor(estimator=None, *, min_samples=None, residual_threshold=None, is_data_valid=None, is_model_valid=None, max_trials=100, max_skips=inf, stop_n_inliers=inf, stop_score=inf, stop_probability=0.99, loss='absolute_error', random_state=None)\n",
      "     |  \n",
      "     |  RANSAC (RANdom SAmple Consensus) algorithm.\n",
      "     |  \n",
      "     |  RANSAC is an iterative algorithm for the robust estimation of parameters\n",
      "     |  from a subset of inliers from the complete data set.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ransac_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : object, default=None\n",
      "     |      Base estimator object which implements the following methods:\n",
      "     |  \n",
      "     |      * `fit(X, y)`: Fit model to given training data and target values.\n",
      "     |      * `score(X, y)`: Returns the mean accuracy on the given test data,\n",
      "     |        which is used for the stop criterion defined by `stop_score`.\n",
      "     |        Additionally, the score is used to decide which of two equally\n",
      "     |        large consensus sets is chosen as the better one.\n",
      "     |      * `predict(X)`: Returns predicted values using the linear model,\n",
      "     |        which is used to compute residual error using loss function.\n",
      "     |  \n",
      "     |      If `estimator` is None, then\n",
      "     |      :class:`~sklearn.linear_model.LinearRegression` is used for\n",
      "     |      target values of dtype float.\n",
      "     |  \n",
      "     |      Note that the current implementation only supports regression\n",
      "     |      estimators.\n",
      "     |  \n",
      "     |  min_samples : int (>= 1) or float ([0, 1]), default=None\n",
      "     |      Minimum number of samples chosen randomly from original data. Treated\n",
      "     |      as an absolute number of samples for `min_samples >= 1`, treated as a\n",
      "     |      relative number `ceil(min_samples * X.shape[0])` for\n",
      "     |      `min_samples < 1`. This is typically chosen as the minimal number of\n",
      "     |      samples necessary to estimate the given `estimator`. By default a\n",
      "     |      :class:`~sklearn.linear_model.LinearRegression` estimator is assumed and\n",
      "     |      `min_samples` is chosen as ``X.shape[1] + 1``. This parameter is highly\n",
      "     |      dependent upon the model, so if a `estimator` other than\n",
      "     |      :class:`~sklearn.linear_model.LinearRegression` is used, the user must\n",
      "     |      provide a value.\n",
      "     |  \n",
      "     |  residual_threshold : float, default=None\n",
      "     |      Maximum residual for a data sample to be classified as an inlier.\n",
      "     |      By default the threshold is chosen as the MAD (median absolute\n",
      "     |      deviation) of the target values `y`. Points whose residuals are\n",
      "     |      strictly equal to the threshold are considered as inliers.\n",
      "     |  \n",
      "     |  is_data_valid : callable, default=None\n",
      "     |      This function is called with the randomly selected data before the\n",
      "     |      model is fitted to it: `is_data_valid(X, y)`. If its return value is\n",
      "     |      False the current randomly chosen sub-sample is skipped.\n",
      "     |  \n",
      "     |  is_model_valid : callable, default=None\n",
      "     |      This function is called with the estimated model and the randomly\n",
      "     |      selected data: `is_model_valid(model, X, y)`. If its return value is\n",
      "     |      False the current randomly chosen sub-sample is skipped.\n",
      "     |      Rejecting samples with this function is computationally costlier than\n",
      "     |      with `is_data_valid`. `is_model_valid` should therefore only be used if\n",
      "     |      the estimated model is needed for making the rejection decision.\n",
      "     |  \n",
      "     |  max_trials : int, default=100\n",
      "     |      Maximum number of iterations for random sample selection.\n",
      "     |  \n",
      "     |  max_skips : int, default=np.inf\n",
      "     |      Maximum number of iterations that can be skipped due to finding zero\n",
      "     |      inliers or invalid data defined by ``is_data_valid`` or invalid models\n",
      "     |      defined by ``is_model_valid``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  stop_n_inliers : int, default=np.inf\n",
      "     |      Stop iteration if at least this number of inliers are found.\n",
      "     |  \n",
      "     |  stop_score : float, default=np.inf\n",
      "     |      Stop iteration if score is greater equal than this threshold.\n",
      "     |  \n",
      "     |  stop_probability : float in range [0, 1], default=0.99\n",
      "     |      RANSAC iteration stops if at least one outlier-free set of the training\n",
      "     |      data is sampled in RANSAC. This requires to generate at least N\n",
      "     |      samples (iterations)::\n",
      "     |  \n",
      "     |          N >= log(1 - probability) / log(1 - e**m)\n",
      "     |  \n",
      "     |      where the probability (confidence) is typically set to high value such\n",
      "     |      as 0.99 (the default) and e is the current fraction of inliers w.r.t.\n",
      "     |      the total number of samples.\n",
      "     |  \n",
      "     |  loss : str, callable, default='absolute_error'\n",
      "     |      String inputs, 'absolute_error' and 'squared_error' are supported which\n",
      "     |      find the absolute error and squared error per sample respectively.\n",
      "     |  \n",
      "     |      If ``loss`` is a callable, then it should be a function that takes\n",
      "     |      two arrays as inputs, the true and predicted value and returns a 1-D\n",
      "     |      array with the i-th value of the array corresponding to the loss\n",
      "     |      on ``X[i]``.\n",
      "     |  \n",
      "     |      If the loss on a sample is greater than the ``residual_threshold``,\n",
      "     |      then this sample is classified as an outlier.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      The generator used to initialize the centers.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : object\n",
      "     |      Final model fitted on the inliers predicted by the \"best\" model found\n",
      "     |      during RANSAC sampling (copy of the `estimator` object).\n",
      "     |  \n",
      "     |  n_trials_ : int\n",
      "     |      Number of random selection trials until one of the stop criteria is\n",
      "     |      met. It is always ``<= max_trials``.\n",
      "     |  \n",
      "     |  inlier_mask_ : bool array of shape [n_samples]\n",
      "     |      Boolean mask of inliers classified as ``True``.\n",
      "     |  \n",
      "     |  n_skips_no_inliers_ : int\n",
      "     |      Number of iterations skipped due to finding zero inliers.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  n_skips_invalid_data_ : int\n",
      "     |      Number of iterations skipped due to invalid data defined by\n",
      "     |      ``is_data_valid``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  n_skips_invalid_model_ : int\n",
      "     |      Number of iterations skipped due to an invalid model defined by\n",
      "     |      ``is_model_valid``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  HuberRegressor : Linear regression model that is robust to outliers.\n",
      "     |  TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\n",
      "     |  SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] https://en.wikipedia.org/wiki/RANSAC\n",
      "     |  .. [2] https://www.sri.com/wp-content/uploads/2021/12/ransac-publication.pdf\n",
      "     |  .. [3] https://bmva-archive.org.uk/bmvc/2009/Papers/Paper355/Paper355.pdf\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import RANSACRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(\n",
      "     |  ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n",
      "     |  >>> reg = RANSACRegressor(random_state=0).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9885\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-31.9417])\n",
      "     |  \n",
      "     |  For a more detailed example, see\n",
      "     |  :ref:`sphx_glr_auto_examples_linear_model_plot_ransac.py`\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RANSACRegressor\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator=None, *, min_samples=None, residual_threshold=None, is_data_valid=None, is_model_valid=None, max_trials=100, max_skips=inf, stop_n_inliers=inf, stop_score=inf, stop_probability=0.99, loss='absolute_error', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, **fit_params)\n",
      "     |      Fit estimator using RANSAC algorithm.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample\n",
      "     |          raises error if sample_weight is passed and estimator\n",
      "     |          fit method does not support it.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Parameters routed to the `fit` method of the sub-estimator via the\n",
      "     |          metadata routing API.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.5\n",
      "     |      \n",
      "     |              Only available if\n",
      "     |              `sklearn.set_config(enable_metadata_routing=True)` is set. See\n",
      "     |              :ref:`Metadata Routing User Guide <metadata_routing>` for more\n",
      "     |              details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted `RANSACRegressor` estimator.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      ValueError\n",
      "     |          If no valid consensus set could be found. This occurs if\n",
      "     |          `is_data_valid` and `is_model_valid` return False for all\n",
      "     |          `max_trials` randomly chosen sub-samples.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  predict(self, X, **params)\n",
      "     |      Predict using the estimated model.\n",
      "     |      \n",
      "     |      This is a wrapper for `estimator_.predict(X)`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      **params : dict\n",
      "     |          Parameters routed to the `predict` method of the sub-estimator via\n",
      "     |          the metadata routing API.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.5\n",
      "     |      \n",
      "     |              Only available if\n",
      "     |              `sklearn.set_config(enable_metadata_routing=True)` is set. See\n",
      "     |              :ref:`Metadata Routing User Guide <metadata_routing>` for more\n",
      "     |              details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array, shape = [n_samples] or [n_samples, n_targets]\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y, **params)\n",
      "     |      Return the score of the prediction.\n",
      "     |      \n",
      "     |      This is a wrapper for `estimator_.score(X, y)`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : (array-like or sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      **params : dict\n",
      "     |          Parameters routed to the `score` method of the sub-estimator via\n",
      "     |          the metadata routing API.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.5\n",
      "     |      \n",
      "     |              Only available if\n",
      "     |              `sklearn.set_config(enable_metadata_routing=True)` is set. See\n",
      "     |              :ref:`Metadata Routing User Guide <metadata_routing>` for more\n",
      "     |              details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      z : float\n",
      "     |          Score of the prediction.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._ransac.RANSACRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ransac.RANSACRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class Ridge(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, _BaseRidge)\n",
      "     |  Ridge(alpha=1.0, *, fit_intercept=True, copy_X=True, max_iter=None, tol=0.0001, solver='auto', positive=False, random_state=None)\n",
      "     |  \n",
      "     |  Linear least squares with l2 regularization.\n",
      "     |  \n",
      "     |  Minimizes the objective function::\n",
      "     |  \n",
      "     |  ||y - Xw||^2_2 + alpha * ||w||^2_2\n",
      "     |  \n",
      "     |  This model solves a regression model where the loss function is\n",
      "     |  the linear least squares function and regularization is given by\n",
      "     |  the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n",
      "     |  This estimator has built-in support for multi-variate regression\n",
      "     |  (i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : {float, ndarray of shape (n_targets,)}, default=1.0\n",
      "     |      Constant that multiplies the L2 term, controlling regularization\n",
      "     |      strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.\n",
      "     |  \n",
      "     |      When `alpha = 0`, the objective is equivalent to ordinary least\n",
      "     |      squares, solved by the :class:`LinearRegression` object. For numerical\n",
      "     |      reasons, using `alpha = 0` with the `Ridge` object is not advised.\n",
      "     |      Instead, you should use the :class:`LinearRegression` object.\n",
      "     |  \n",
      "     |      If an array is passed, penalties are assumed to be specific to the\n",
      "     |      targets. Hence they must correspond in number.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to fit the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. ``X`` and ``y`` are expected to be centered).\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=None\n",
      "     |      Maximum number of iterations for conjugate gradient solver.\n",
      "     |      For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      "     |      by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n",
      "     |      For 'lbfgs' solver, the default value is 15000.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The precision of the solution (`coef_`) is determined by `tol` which\n",
      "     |      specifies a different convergence criterion for each solver:\n",
      "     |  \n",
      "     |      - 'svd': `tol` has no impact.\n",
      "     |  \n",
      "     |      - 'cholesky': `tol` has no impact.\n",
      "     |  \n",
      "     |      - 'sparse_cg': norm of residuals smaller than `tol`.\n",
      "     |  \n",
      "     |      - 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr,\n",
      "     |        which control the norm of the residual vector in terms of the norms of\n",
      "     |        matrix and coefficients.\n",
      "     |  \n",
      "     |      - 'sag' and 'saga': relative change of coef smaller than `tol`.\n",
      "     |  \n",
      "     |      - 'lbfgs': maximum of the absolute (projected) gradient=max|residuals|\n",
      "     |        smaller than `tol`.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.2\n",
      "     |         Default value changed from 1e-3 to 1e-4 for consistency with other linear\n",
      "     |         models.\n",
      "     |  \n",
      "     |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\n",
      "     |      Solver to use in the computational routines:\n",
      "     |  \n",
      "     |      - 'auto' chooses the solver automatically based on the type of data.\n",
      "     |  \n",
      "     |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "     |        coefficients. It is the most stable solver, in particular more stable\n",
      "     |        for singular matrices than 'cholesky' at the cost of being slower.\n",
      "     |  \n",
      "     |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "     |        obtain a closed-form solution.\n",
      "     |  \n",
      "     |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "     |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "     |        more appropriate than 'cholesky' for large-scale data\n",
      "     |        (possibility to set `tol` and `max_iter`).\n",
      "     |  \n",
      "     |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "     |        scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
      "     |        procedure.\n",
      "     |  \n",
      "     |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      "     |        its improved, unbiased version named SAGA. Both methods also use an\n",
      "     |        iterative procedure, and are often faster than other solvers when\n",
      "     |        both n_samples and n_features are large. Note that 'sag' and\n",
      "     |        'saga' fast convergence is only guaranteed on features with\n",
      "     |        approximately the same scale. You can preprocess the data with a\n",
      "     |        scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |      - 'lbfgs' uses L-BFGS-B algorithm implemented in\n",
      "     |        `scipy.optimize.minimize`. It can be used only when `positive`\n",
      "     |        is True.\n",
      "     |  \n",
      "     |      All solvers except 'svd' support both dense and sparse data. However, only\n",
      "     |      'lsqr', 'sag', 'sparse_cg', and 'lbfgs' support sparse input when\n",
      "     |      `fit_intercept` is True.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         Stochastic Average Gradient descent solver.\n",
      "     |      .. versionadded:: 0.19\n",
      "     |         SAGA solver.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |      Only 'lbfgs' solver is supported in this case.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         `random_state` to support Stochastic Average Gradient.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  n_iter_ : None or ndarray of shape (n_targets,)\n",
      "     |      Actual number of iterations for each target. Available only for\n",
      "     |      sag and lsqr solvers. Other solvers will return None.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  solver_ : str\n",
      "     |      The solver that was used at fit time by the computational\n",
      "     |      routines.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.5\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RidgeClassifier : Ridge classifier.\n",
      "     |  RidgeCV : Ridge regression with built-in cross validation.\n",
      "     |  :class:`~sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n",
      "     |      combines ridge regression with the kernel trick.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Regularization improves the conditioning of the problem and\n",
      "     |  reduces the variance of the estimates. Larger values specify stronger\n",
      "     |  regularization. Alpha corresponds to ``1 / (2C)`` in other linear\n",
      "     |  models such as :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |  :class:`~sklearn.svm.LinearSVC`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import Ridge\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> y = rng.randn(n_samples)\n",
      "     |  >>> X = rng.randn(n_samples, n_features)\n",
      "     |  >>> clf = Ridge(alpha=1.0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  Ridge()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Ridge\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      _BaseRidge\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, copy_X=True, max_iter=None, tol=0.0001, solver='auto', positive=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge regression model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._ridge.Ridge, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.Ridge from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._ridge.Ridge, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.Ridge from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class RidgeCV(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, _BaseRidgeCV)\n",
      "     |  RidgeCV(alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, scoring=None, cv=None, gcv_mode=None, store_cv_results=False, alpha_per_target=False)\n",
      "     |  \n",
      "     |  Ridge regression with built-in cross-validation.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  By default, it performs efficient Leave-One-Out Cross-Validation.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n",
      "     |      Array of alpha values to try.\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |      :class:`~sklearn.svm.LinearSVC`.\n",
      "     |      If using Leave-One-Out cross-validation, alphas must be strictly positive.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  scoring : str, callable, default=None\n",
      "     |      The scoring method to use for cross-validation. Options:\n",
      "     |  \n",
      "     |      - str: see :ref:`scoring_string_names` for options.\n",
      "     |      - callable: a scorer callable object (e.g., function) with signature\n",
      "     |        ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n",
      "     |      - `None`: negative :ref:`mean squared error <mean_squared_error>` if cv is\n",
      "     |        None (i.e. when using leave-one-out cross-validation), or\n",
      "     |        :ref:`coefficient of determination <r2_score>` (:math:`R^2`) otherwise.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the efficient Leave-One-Out cross-validation\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, if ``y`` is binary or multiclass,\n",
      "     |      :class:`~sklearn.model_selection.StratifiedKFold` is used, else,\n",
      "     |      :class:`~sklearn.model_selection.KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  gcv_mode : {'auto', 'svd', 'eigen'}, default='auto'\n",
      "     |      Flag indicating which strategy to use when performing\n",
      "     |      Leave-One-Out Cross-Validation. Options are::\n",
      "     |  \n",
      "     |          'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'\n",
      "     |          'svd' : force use of singular value decomposition of X when X is\n",
      "     |              dense, eigenvalue decomposition of X^T.X when X is sparse.\n",
      "     |          'eigen' : force computation via eigendecomposition of X.X^T\n",
      "     |  \n",
      "     |      The 'auto' mode is the default and is intended to pick the cheaper\n",
      "     |      option of the two depending on the shape of the training data.\n",
      "     |  \n",
      "     |  store_cv_results : bool, default=False\n",
      "     |      Flag indicating if the cross-validation values corresponding to\n",
      "     |      each alpha should be stored in the ``cv_results_`` attribute (see\n",
      "     |      below). This flag is only compatible with ``cv=None`` (i.e. using\n",
      "     |      Leave-One-Out Cross-Validation).\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.5\n",
      "     |          Parameter name changed from `store_cv_values` to `store_cv_results`.\n",
      "     |  \n",
      "     |  alpha_per_target : bool, default=False\n",
      "     |      Flag indicating whether to optimize the alpha value (picked from the\n",
      "     |      `alphas` parameter list) for each target separately (for multi-output\n",
      "     |      settings: multiple prediction targets). When set to `True`, after\n",
      "     |      fitting, the `alpha_` attribute will contain a value for each target.\n",
      "     |      When set to `False`, a single alpha is used for all targets.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_results_ : ndarray of shape (n_samples, n_alphas) or             shape (n_samples, n_targets, n_alphas), optional\n",
      "     |      Cross-validation values for each alpha (only available if\n",
      "     |      ``store_cv_results=True`` and ``cv=None``). After ``fit()`` has been\n",
      "     |      called, this attribute will contain the mean squared errors if\n",
      "     |      `scoring is None` otherwise it will contain standardized per point\n",
      "     |      prediction values.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.5\n",
      "     |          `cv_values_` changed to `cv_results_`.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (n_features) or (n_targets, n_features)\n",
      "     |      Weight vector(s).\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  alpha_ : float or ndarray of shape (n_targets,)\n",
      "     |      Estimated regularization parameter, or, if ``alpha_per_target=True``,\n",
      "     |      the estimated regularization parameter for each target.\n",
      "     |  \n",
      "     |  best_score_ : float or ndarray of shape (n_targets,)\n",
      "     |      Score of base estimator with best alpha, or, if\n",
      "     |      ``alpha_per_target=True``, a score for each target.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Ridge : Ridge regression.\n",
      "     |  RidgeClassifier : Classifier based on ridge regression on {-1, 1} labels.\n",
      "     |  RidgeClassifierCV : Ridge classifier with built-in cross validation.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_diabetes\n",
      "     |  >>> from sklearn.linear_model import RidgeCV\n",
      "     |  >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |  >>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.5166...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeCV\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      _BaseRidgeCV\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, **params)\n",
      "     |      Fit Ridge regression model with cv.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training data. If using GCV, will be cast to float64\n",
      "     |          if necessary.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |      **params : dict, default=None\n",
      "     |          Parameters to be passed to the underlying scorer.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.5\n",
      "     |              Only available if `enable_metadata_routing=True`,\n",
      "     |              which can be set by using\n",
      "     |              ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      When sample_weight is provided, the selected hyperparameter may depend\n",
      "     |      on whether we use leave-one-out cross-validation (cv=None)\n",
      "     |      or another form of cross-validation, because only leave-one-out\n",
      "     |      cross-validation takes the sample weights into account when computing\n",
      "     |      the validation score.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._ridge.RidgeCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.RidgeCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._ridge.RidgeCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.RidgeCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseRidgeCV:\n",
      "     |  \n",
      "     |  __init__(self, alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, scoring=None, cv=None, gcv_mode=None, store_cv_results=False, alpha_per_target=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class RidgeClassifier(_RidgeClassifierMixin, _BaseRidge)\n",
      "     |  RidgeClassifier(alpha=1.0, *, fit_intercept=True, copy_X=True, max_iter=None, tol=0.0001, class_weight=None, solver='auto', positive=False, random_state=None)\n",
      "     |  \n",
      "     |  Classifier using Ridge regression.\n",
      "     |  \n",
      "     |  This classifier first converts the target values into ``{-1, 1}`` and\n",
      "     |  then treats the problem as a regression task (multi-output regression in\n",
      "     |  the multiclass case).\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alpha : float, default=1.0\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |      :class:`~sklearn.svm.LinearSVC`.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set to false, no\n",
      "     |      intercept will be used in calculations (e.g. data is expected to be\n",
      "     |      already centered).\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |  max_iter : int, default=None\n",
      "     |      Maximum number of iterations for conjugate gradient solver.\n",
      "     |      The default value is determined by scipy.sparse.linalg.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      The precision of the solution (`coef_`) is determined by `tol` which\n",
      "     |      specifies a different convergence criterion for each solver:\n",
      "     |  \n",
      "     |      - 'svd': `tol` has no impact.\n",
      "     |  \n",
      "     |      - 'cholesky': `tol` has no impact.\n",
      "     |  \n",
      "     |      - 'sparse_cg': norm of residuals smaller than `tol`.\n",
      "     |  \n",
      "     |      - 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr,\n",
      "     |        which control the norm of the residual vector in terms of the norms of\n",
      "     |        matrix and coefficients.\n",
      "     |  \n",
      "     |      - 'sag' and 'saga': relative change of coef smaller than `tol`.\n",
      "     |  \n",
      "     |      - 'lbfgs': maximum of the absolute (projected) gradient=max|residuals|\n",
      "     |        smaller than `tol`.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.2\n",
      "     |         Default value changed from 1e-3 to 1e-4 for consistency with other linear\n",
      "     |         models.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |  solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\n",
      "     |      Solver to use in the computational routines:\n",
      "     |  \n",
      "     |      - 'auto' chooses the solver automatically based on the type of data.\n",
      "     |  \n",
      "     |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "     |        coefficients. It is the most stable solver, in particular more stable\n",
      "     |        for singular matrices than 'cholesky' at the cost of being slower.\n",
      "     |  \n",
      "     |      - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "     |        obtain a closed-form solution.\n",
      "     |  \n",
      "     |      - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "     |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "     |        more appropriate than 'cholesky' for large-scale data\n",
      "     |        (possibility to set `tol` and `max_iter`).\n",
      "     |  \n",
      "     |      - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "     |        scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
      "     |        procedure.\n",
      "     |  \n",
      "     |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      "     |        its unbiased and more flexible version named SAGA. Both methods\n",
      "     |        use an iterative procedure, and are often faster than other solvers\n",
      "     |        when both n_samples and n_features are large. Note that 'sag' and\n",
      "     |        'saga' fast convergence is only guaranteed on features with\n",
      "     |        approximately the same scale. You can preprocess the data with a\n",
      "     |        scaler from sklearn.preprocessing.\n",
      "     |  \n",
      "     |        .. versionadded:: 0.17\n",
      "     |           Stochastic Average Gradient descent solver.\n",
      "     |        .. versionadded:: 0.19\n",
      "     |           SAGA solver.\n",
      "     |  \n",
      "     |      - 'lbfgs' uses L-BFGS-B algorithm implemented in\n",
      "     |        `scipy.optimize.minimize`. It can be used only when `positive`\n",
      "     |        is True.\n",
      "     |  \n",
      "     |  positive : bool, default=False\n",
      "     |      When set to ``True``, forces the coefficients to be positive.\n",
      "     |      Only 'lbfgs' solver is supported in this case.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      ``coef_`` is of shape (1, n_features) when the given problem is binary.\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  n_iter_ : None or ndarray of shape (n_targets,)\n",
      "     |      Actual number of iterations for each target. Available only for\n",
      "     |      sag and lsqr solvers. Other solvers will return None.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  solver_ : str\n",
      "     |      The solver that was used at fit time by the computational\n",
      "     |      routines.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.5\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Ridge : Ridge regression.\n",
      "     |  RidgeClassifierCV :  Ridge classifier with built-in cross validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For multi-class classification, n_class classifiers are trained in\n",
      "     |  a one-versus-all approach. Concretely, this is implemented by taking\n",
      "     |  advantage of the multi-variate response support in Ridge.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_breast_cancer\n",
      "     |  >>> from sklearn.linear_model import RidgeClassifier\n",
      "     |  >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "     |  >>> clf = RidgeClassifier().fit(X, y)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.9595...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeClassifier\n",
      "     |      _RidgeClassifierMixin\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseRidge\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alpha=1.0, *, fit_intercept=True, copy_X=True, max_iter=None, tol=0.0001, class_weight=None, solver='auto', positive=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit Ridge classifier model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.17\n",
      "     |             *sample_weight* support to RidgeClassifier.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Instance of the estimator.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._ridge.RidgeClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.RidgeClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._ridge.RidgeClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.RidgeClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _RidgeClassifierMixin:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, spare matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to predict the targets.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Vector or matrix containing the predictions. In binary and\n",
      "     |          multiclass problems, this is a vector containing `n_samples`. In\n",
      "     |          a multilabel problem, it returns a matrix of shape\n",
      "     |          `(n_samples, n_outputs)`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _RidgeClassifierMixin:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |      Classes labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the confidence scores.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      "     |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      "     |          this class would be predicted.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class RidgeClassifierCV(_RidgeClassifierMixin, _BaseRidgeCV)\n",
      "     |  RidgeClassifierCV(alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, scoring=None, cv=None, class_weight=None, store_cv_results=False)\n",
      "     |  \n",
      "     |  Ridge classifier with built-in cross-validation.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  By default, it performs Leave-One-Out Cross-Validation. Currently,\n",
      "     |  only the n_features > n_samples case is handled efficiently.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  alphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n",
      "     |      Array of alpha values to try.\n",
      "     |      Regularization strength; must be a positive float. Regularization\n",
      "     |      improves the conditioning of the problem and reduces the variance of\n",
      "     |      the estimates. Larger values specify stronger regularization.\n",
      "     |      Alpha corresponds to ``1 / (2C)`` in other linear models such as\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "     |      :class:`~sklearn.svm.LinearSVC`.\n",
      "     |      If using Leave-One-Out cross-validation, alphas must be strictly positive.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations\n",
      "     |      (i.e. data is expected to be centered).\n",
      "     |  \n",
      "     |  scoring : str, callable, default=None\n",
      "     |      The scoring method to use for cross-validation. Options:\n",
      "     |  \n",
      "     |      - str: see :ref:`scoring_string_names` for options.\n",
      "     |      - callable: a scorer callable object (e.g., function) with signature\n",
      "     |        ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n",
      "     |      - `None`: negative :ref:`mean squared error <mean_squared_error>` if cv is\n",
      "     |        None (i.e. when using leave-one-out cross-validation), or\n",
      "     |        :ref:`accuracy <accuracy_score>` otherwise.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the efficient Leave-One-Out cross-validation\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |  store_cv_results : bool, default=False\n",
      "     |      Flag indicating if the cross-validation results corresponding to\n",
      "     |      each alpha should be stored in the ``cv_results_`` attribute (see\n",
      "     |      below). This flag is only compatible with ``cv=None`` (i.e. using\n",
      "     |      Leave-One-Out Cross-Validation).\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.5\n",
      "     |          Parameter name changed from `store_cv_values` to `store_cv_results`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_results_ : ndarray of shape (n_samples, n_targets, n_alphas), optional\n",
      "     |      Cross-validation results for each alpha (only if ``store_cv_results=True`` and\n",
      "     |      ``cv=None``). After ``fit()`` has been called, this attribute will\n",
      "     |      contain the mean squared errors if `scoring is None` otherwise it\n",
      "     |      will contain standardized per point prediction values.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.5\n",
      "     |          `cv_values_` changed to `cv_results_`.\n",
      "     |  \n",
      "     |  coef_ : ndarray of shape (1, n_features) or (n_targets, n_features)\n",
      "     |      Coefficient of the features in the decision function.\n",
      "     |  \n",
      "     |      ``coef_`` is of shape (1, n_features) when the given problem is binary.\n",
      "     |  \n",
      "     |  intercept_ : float or ndarray of shape (n_targets,)\n",
      "     |      Independent term in decision function. Set to 0.0 if\n",
      "     |      ``fit_intercept = False``.\n",
      "     |  \n",
      "     |  alpha_ : float\n",
      "     |      Estimated regularization parameter.\n",
      "     |  \n",
      "     |  best_score_ : float\n",
      "     |      Score of base estimator with best alpha.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  Ridge : Ridge regression.\n",
      "     |  RidgeClassifier : Ridge classifier.\n",
      "     |  RidgeCV : Ridge regression with built-in cross validation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  For multi-class classification, n_class classifiers are trained in\n",
      "     |  a one-versus-all approach. Concretely, this is implemented by taking\n",
      "     |  advantage of the multi-variate response support in Ridge.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_breast_cancer\n",
      "     |  >>> from sklearn.linear_model import RidgeClassifierCV\n",
      "     |  >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "     |  >>> clf = RidgeClassifierCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.9630...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RidgeClassifierCV\n",
      "     |      _RidgeClassifierMixin\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseRidgeCV\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, alphas=(0.1, 1.0, 10.0), *, fit_intercept=True, scoring=None, cv=None, class_weight=None, store_cv_results=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, **params)\n",
      "     |      Fit Ridge classifier with cv.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples\n",
      "     |          and `n_features` is the number of features. When using GCV,\n",
      "     |          will be cast to float64 if necessary.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values. Will be cast to X's dtype if necessary.\n",
      "     |      \n",
      "     |      sample_weight : float or ndarray of shape (n_samples,), default=None\n",
      "     |          Individual weights for each sample. If given a float, every sample\n",
      "     |          will have the same weight.\n",
      "     |      \n",
      "     |      **params : dict, default=None\n",
      "     |          Parameters to be passed to the underlying scorer.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.5\n",
      "     |              Only available if `enable_metadata_routing=True`,\n",
      "     |              which can be set by using\n",
      "     |              ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._ridge.RidgeClassifierCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.RidgeClassifierCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._ridge.RidgeClassifierCV, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._ridge.RidgeClassifierCV from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  param = 'alpha_per_target'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _RidgeClassifierMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, spare matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to predict the targets.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          Vector or matrix containing the predictions. In binary and\n",
      "     |          multiclass problems, this is a vector containing `n_samples`. In\n",
      "     |          a multilabel problem, it returns a matrix of shape\n",
      "     |          `(n_samples, n_outputs)`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _RidgeClassifierMixin:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |      Classes labels.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the confidence scores.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      "     |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      "     |          this class would be predicted.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseRidgeCV:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class SGDClassifier(BaseSGDClassifier)\n",
      "     |  SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
      "     |  \n",
      "     |  This estimator implements regularized linear models with stochastic\n",
      "     |  gradient descent (SGD) learning: the gradient of the loss is estimated\n",
      "     |  each sample at a time and the model is updated along the way with a\n",
      "     |  decreasing strength schedule (aka learning rate). SGD allows minibatch\n",
      "     |  (online/out-of-core) learning via the `partial_fit` method.\n",
      "     |  For best results using the default learning rate schedule, the data should\n",
      "     |  have zero mean and unit variance.\n",
      "     |  \n",
      "     |  This implementation works with data represented as dense or sparse arrays\n",
      "     |  of floating point values for the features. The model it fits can be\n",
      "     |  controlled with the loss parameter; by default, it fits a linear support\n",
      "     |  vector machine (SVM).\n",
      "     |  \n",
      "     |  The regularizer is a penalty added to the loss function that shrinks model\n",
      "     |  parameters towards the zero vector using either the squared euclidean norm\n",
      "     |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      "     |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      "     |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      "     |  online feature selection.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <sgd>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : {'hinge', 'log_loss', 'modified_huber', 'squared_hinge',        'perceptron', 'squared_error', 'huber', 'epsilon_insensitive',        'squared_epsilon_insensitive'}, default='hinge'\n",
      "     |      The loss function to be used.\n",
      "     |  \n",
      "     |      - 'hinge' gives a linear SVM.\n",
      "     |      - 'log_loss' gives logistic regression, a probabilistic classifier.\n",
      "     |      - 'modified_huber' is another smooth loss that brings tolerance to\n",
      "     |        outliers as well as probability estimates.\n",
      "     |      - 'squared_hinge' is like hinge but is quadratically penalized.\n",
      "     |      - 'perceptron' is the linear loss used by the perceptron algorithm.\n",
      "     |      - The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and\n",
      "     |        'squared_epsilon_insensitive' are designed for regression but can be useful\n",
      "     |        in classification as well; see\n",
      "     |        :class:`~sklearn.linear_model.SGDRegressor` for a description.\n",
      "     |  \n",
      "     |      More details about the losses formulas can be found in the :ref:`User Guide\n",
      "     |      <sgd_mathematical_formulation>` and you can find a visualisation of the loss\n",
      "     |      functions in\n",
      "     |      :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_loss_functions.py`.\n",
      "     |  \n",
      "     |  penalty : {'l2', 'l1', 'elasticnet', None}, default='l2'\n",
      "     |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      "     |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      "     |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      "     |      not achievable with 'l2'. No penalty is added when set to `None`.\n",
      "     |  \n",
      "     |      You can see a visualisation of the penalties in\n",
      "     |      :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_penalties.py`.\n",
      "     |  \n",
      "     |  alpha : float, default=0.0001\n",
      "     |      Constant that multiplies the regularization term. The higher the\n",
      "     |      value, the stronger the regularization. Also used to compute the\n",
      "     |      learning rate when `learning_rate` is set to 'optimal'.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.15\n",
      "     |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      "     |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      "     |      Only used if `penalty` is 'elasticnet'.\n",
      "     |      Values must be in the range `[0.0, 1.0]` or can be `None` if\n",
      "     |      `penalty` is not `elasticnet`.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.7\n",
      "     |          `l1_ratio` can be `None` when `penalty` is not \"elasticnet\".\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |      Values must be in the range `[1, inf)`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, training will stop\n",
      "     |      when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
      "     |      epochs.\n",
      "     |      Convergence is checked against the training loss or the\n",
      "     |      validation loss depending on the `early_stopping` parameter.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |      Values must be in the range `[0, inf)`.\n",
      "     |  \n",
      "     |  epsilon : float, default=0.1\n",
      "     |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |      For 'huber', determines the threshold at which it becomes less\n",
      "     |      important to get the prediction exactly right.\n",
      "     |      For epsilon-insensitive, any differences between the current prediction\n",
      "     |      and the correct label are ignored if they are less than this threshold.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      "     |      multi-class problems) computation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |      Integer values must be in the range `[0, 2**32 - 1]`.\n",
      "     |  \n",
      "     |  learning_rate : str, default='optimal'\n",
      "     |      The learning rate schedule:\n",
      "     |  \n",
      "     |      - 'constant': `eta = eta0`\n",
      "     |      - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      "     |        where `t0` is chosen by a heuristic proposed by Leon Bottou.\n",
      "     |      - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      "     |      - 'adaptive': `eta = eta0`, as long as the training keeps decreasing.\n",
      "     |        Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      "     |        training loss by tol or fail to increase validation score by tol if\n",
      "     |        `early_stopping` is `True`, the current learning rate is divided by 5.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'adaptive' option.\n",
      "     |  \n",
      "     |  eta0 : float, default=0.0\n",
      "     |      The initial learning rate for the 'constant', 'invscaling' or\n",
      "     |      'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n",
      "     |      the default schedule 'optimal'.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |  power_t : float, default=0.5\n",
      "     |      The exponent for inverse scaling learning rate.\n",
      "     |      Values must be in the range `(-inf, inf)`.\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation\n",
      "     |      score is not improving. If set to `True`, it will automatically set aside\n",
      "     |      a stratified fraction of training data as validation and terminate\n",
      "     |      training when validation score returned by the `score` method is not\n",
      "     |      improving by at least tol for n_iter_no_change consecutive epochs.\n",
      "     |  \n",
      "     |      See :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_early_stopping.py` for an\n",
      "     |      example of the effects of early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'early_stopping' option\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if `early_stopping` is True.\n",
      "     |      Values must be in the range `(0.0, 1.0)`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'validation_fraction' option\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before stopping\n",
      "     |      fitting.\n",
      "     |      Convergence is checked against the training loss or the\n",
      "     |      validation loss depending on the `early_stopping` parameter.\n",
      "     |      Integer values must be in the range `[1, max_iter)`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'n_iter_no_change' option\n",
      "     |  \n",
      "     |  class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
      "     |      Preset for the class_weight fit parameter.\n",
      "     |  \n",
      "     |      Weights associated with classes. If not given, all classes\n",
      "     |      are supposed to have weight one.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |      If a dynamic learning rate is used, the learning rate is adapted\n",
      "     |      depending on the number of samples already seen. Calling ``fit`` resets\n",
      "     |      this counter, while ``partial_fit`` will result in increasing the\n",
      "     |      existing counter.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to `True`, computes the averaged SGD weights across all\n",
      "     |      updates and stores the result in the ``coef_`` attribute. If set to\n",
      "     |      an int greater than 1, averaging will begin once the total number of\n",
      "     |      samples seen reaches `average`. So ``average=10`` will begin\n",
      "     |      averaging after seeing 10 samples.\n",
      "     |      Integer values must be in the range `[1, n_samples]`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      "     |      Constants in decision function.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations before reaching the stopping criterion.\n",
      "     |      For multiclass fits, it is the maximum over every binary fit.\n",
      "     |  \n",
      "     |  classes_ : array of shape (n_classes,)\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples + 1)``.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.svm.LinearSVC : Linear support vector classification.\n",
      "     |  LogisticRegression : Logistic regression.\n",
      "     |  Perceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\n",
      "     |      ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\n",
      "     |      penalty=None)``.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import SGDClassifier\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "     |  >>> Y = np.array([1, 1, 2, 2])\n",
      "     |  >>> # Always scale the input. The most convenient way is to use a pipeline.\n",
      "     |  >>> clf = make_pipeline(StandardScaler(),\n",
      "     |  ...                     SGDClassifier(max_iter=1000, tol=1e-3))\n",
      "     |  >>> clf.fit(X, Y)\n",
      "     |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "     |                  ('sgdclassifier', SGDClassifier())])\n",
      "     |  >>> print(clf.predict([[-0.8, -1]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGDClassifier\n",
      "     |      BaseSGDClassifier\n",
      "     |      sklearn.linear_model._base.LinearClassifierMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Log of probability estimates.\n",
      "     |      \n",
      "     |      This method is only available for log loss and modified Huber loss.\n",
      "     |      \n",
      "     |      When loss=\"modified_huber\", probability estimates may be hard zeros\n",
      "     |      and ones, so taking the logarithm is not possible.\n",
      "     |      \n",
      "     |      See ``predict_proba`` for details.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data for prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      T : array-like, shape (n_samples, n_classes)\n",
      "     |          Returns the log-probability of the sample for each class in the\n",
      "     |          model, where classes are ordered as they are in\n",
      "     |          `self.classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Probability estimates.\n",
      "     |      \n",
      "     |      This method is only available for log loss and modified Huber loss.\n",
      "     |      \n",
      "     |      Multiclass probability estimates are derived from binary (one-vs.-rest)\n",
      "     |      estimates by simple normalization, as recommended by Zadrozny and\n",
      "     |      Elkan.\n",
      "     |      \n",
      "     |      Binary probability estimates for loss=\"modified_huber\" are given by\n",
      "     |      (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions\n",
      "     |      it is necessary to perform proper probability calibration by wrapping\n",
      "     |      the classifier with\n",
      "     |      :class:`~sklearn.calibration.CalibratedClassifierCV` instead.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Input data for prediction.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ndarray of shape (n_samples, n_classes)\n",
      "     |          Returns the probability of the sample for each class in the model,\n",
      "     |          where classes are ordered as they are in `self.classes_`.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      Zadrozny and Elkan, \"Transforming classifier scores into multiclass\n",
      "     |      probability estimates\", SIGKDD'02,\n",
      "     |      https://dl.acm.org/doi/pdf/10.1145/775047.775151\n",
      "     |      \n",
      "     |      The justification for the formula in the loss=\"modified_huber\"\n",
      "     |      case is in the appendix B in:\n",
      "     |      http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._stochastic_gradient.SGDClassifier, *, coef_init: Union[bool, NoneType, str] = '$UNCHANGED$', intercept_init: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      coef_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``coef_init`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      intercept_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``intercept_init`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_partial_fit_request(self: sklearn.linear_model._stochastic_gradient.SGDClassifier, *, classes: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``partial_fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      classes : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``classes`` parameter in ``partial_fit``.\n",
      "     |      \n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._stochastic_gradient.SGDClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGDClassifier:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_classes, n_features), default=None\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : ndarray of shape (n_classes,), default=None\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed. These weights will\n",
      "     |          be multiplied with class_weight (passed through the\n",
      "     |          constructor) if class_weight is specified.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      "     |      Perform one epoch of stochastic gradient descent on given samples.\n",
      "     |      \n",
      "     |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      "     |      guaranteed that a minimum of the cost function is reached after calling\n",
      "     |      it once. Matters such as objective convergence, early stopping, and\n",
      "     |      learning rate adjustments should be handled by the user.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of the training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Subset of the target values.\n",
      "     |      \n",
      "     |      classes : ndarray of shape (n_classes,), default=None\n",
      "     |          Classes across all calls to partial_fit.\n",
      "     |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      "     |          target vector of the entire dataset.\n",
      "     |          This argument is required for the first call to partial_fit\n",
      "     |          and can be omitted in the subsequent calls.\n",
      "     |          Note that y doesn't need to contain all labels in `classes`.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseSGDClassifier:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict confidence scores for samples.\n",
      "     |      \n",
      "     |      The confidence score for a sample is proportional to the signed\n",
      "     |      distance of that sample to the hyperplane.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the confidence scores.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "     |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      "     |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      "     |          this class would be predicted.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The data matrix for which we want to get the predictions.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,)\n",
      "     |          Vector containing the class labels for each sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class SGDOneClassSVM(sklearn.base.OutlierMixin, BaseSGD)\n",
      "     |  SGDOneClassSVM(nu=0.5, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  Solves linear One-Class SVM using Stochastic Gradient Descent.\n",
      "     |  \n",
      "     |  This implementation is meant to be used with a kernel approximation\n",
      "     |  technique (e.g. `sklearn.kernel_approximation.Nystroem`) to obtain results\n",
      "     |  similar to `sklearn.svm.OneClassSVM` which uses a Gaussian kernel by\n",
      "     |  default.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <sgd_online_one_class_svm>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  nu : float, default=0.5\n",
      "     |      The nu parameter of the One Class SVM: an upper bound on the\n",
      "     |      fraction of training errors and a lower bound of the fraction of\n",
      "     |      support vectors. Should be in the interval (0, 1]. By default 0.5\n",
      "     |      will be taken.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. Defaults to True.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      `partial_fit`. Defaults to 1000.\n",
      "     |      Values must be in the range `[1, inf)`.\n",
      "     |  \n",
      "     |  tol : float or None, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, the iterations will stop\n",
      "     |      when (loss > previous_loss - tol). Defaults to 1e-3.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |      Defaults to True.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      The seed of the pseudo random number generator to use when shuffling\n",
      "     |      the data.  If int, random_state is the seed used by the random number\n",
      "     |      generator; If RandomState instance, random_state is the random number\n",
      "     |      generator; If None, the random number generator is the RandomState\n",
      "     |      instance used by `np.random`.\n",
      "     |  \n",
      "     |  learning_rate : {'constant', 'optimal', 'invscaling', 'adaptive'}, default='optimal'\n",
      "     |      The learning rate schedule to use with `fit`. (If using `partial_fit`,\n",
      "     |      learning rate must be controlled directly).\n",
      "     |  \n",
      "     |      - 'constant': `eta = eta0`\n",
      "     |      - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      "     |        where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      "     |      - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      "     |      - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n",
      "     |        Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      "     |        training loss by tol or fail to increase validation score by tol if\n",
      "     |        early_stopping is True, the current learning rate is divided by 5.\n",
      "     |  \n",
      "     |  eta0 : float, default=0.0\n",
      "     |      The initial learning rate for the 'constant', 'invscaling' or\n",
      "     |      'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n",
      "     |      the default schedule 'optimal'.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |  power_t : float, default=0.5\n",
      "     |      The exponent for inverse scaling learning rate.\n",
      "     |      Values must be in the range `(-inf, inf)`.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |      If a dynamic learning rate is used, the learning rate is adapted\n",
      "     |      depending on the number of samples already seen. Calling ``fit`` resets\n",
      "     |      this counter, while ``partial_fit``  will result in increasing the\n",
      "     |      existing counter.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to True, computes the averaged SGD weights and stores the\n",
      "     |      result in the ``coef_`` attribute. If set to an int greater than 1,\n",
      "     |      averaging will begin once the total number of samples seen reaches\n",
      "     |      average. So ``average=10`` will begin averaging after seeing 10\n",
      "     |      samples.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (1, n_features)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  offset_ : ndarray of shape (1,)\n",
      "     |      Offset used to define the decision function from the raw scores.\n",
      "     |      We have the relation: decision_function = score_samples - offset.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations to reach the stopping criterion.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples + 1)``.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This estimator has a linear complexity in the number of training samples\n",
      "     |  and is thus better suited than the `sklearn.svm.OneClassSVM`\n",
      "     |  implementation for datasets with a large number of training samples (say\n",
      "     |  > 10,000).\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "     |  >>> clf = linear_model.SGDOneClassSVM(random_state=42)\n",
      "     |  >>> clf.fit(X)\n",
      "     |  SGDOneClassSVM(random_state=42)\n",
      "     |  \n",
      "     |  >>> print(clf.predict([[4, 4]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGDOneClassSVM\n",
      "     |      sklearn.base.OutlierMixin\n",
      "     |      BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, nu=0.5, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, warm_start=False, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Signed distance to the separating hyperplane.\n",
      "     |      \n",
      "     |      Signed distance is positive for an inlier and negative for an\n",
      "     |      outlier.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Testing data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      dec : array-like, shape (n_samples,)\n",
      "     |          Decision function values of the samples.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, coef_init=None, offset_init=None, sample_weight=None)\n",
      "     |      Fit linear One-Class SVM with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      This solves an equivalent optimization problem of the\n",
      "     |      One-Class SVM primal optimization problem and returns a weight vector\n",
      "     |      w and an offset rho such that the decision function is given by\n",
      "     |      <w, x> - rho.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      coef_init : array, shape (n_classes, n_features)\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      offset_init : array, shape (n_classes,)\n",
      "     |          The initial offset to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed. These weights will\n",
      "     |          be multiplied with class_weight (passed through the\n",
      "     |          constructor) if class_weight is specified.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns a fitted instance of self.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y=None, sample_weight=None)\n",
      "     |      Fit linear One-Class SVM with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of the training data.\n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), optional\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns a fitted instance of self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Return labels (1 inlier, -1 outlier) of the samples.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Testing data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array, shape (n_samples,)\n",
      "     |          Labels of the samples.\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Raw scoring function of the samples.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Testing data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score_samples : array-like, shape (n_samples,)\n",
      "     |          Unshiffted scoring function values of the samples.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._stochastic_gradient.SGDOneClassSVM, *, coef_init: Union[bool, NoneType, str] = '$UNCHANGED$', offset_init: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDOneClassSVM from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      coef_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``coef_init`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      offset_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``offset_init`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_partial_fit_request(self: sklearn.linear_model._stochastic_gradient.SGDOneClassSVM, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDOneClassSVM from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``partial_fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  loss_functions = {'hinge': (<class 'sklearn.linear_model._sgd_fast.Hin...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.OutlierMixin:\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None, **kwargs)\n",
      "     |      Perform fit on X and returns labels for X.\n",
      "     |      \n",
      "     |      Returns -1 for outliers and 1 for inliers.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Arguments to be passed to ``fit``.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          1 for inliers, -1 for outliers.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.OutlierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class SGDRegressor(BaseSGDRegressor)\n",
      "     |  SGDRegressor(loss='squared_error', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)\n",
      "     |  \n",
      "     |  Linear model fitted by minimizing a regularized empirical loss with SGD.\n",
      "     |  \n",
      "     |  SGD stands for Stochastic Gradient Descent: the gradient of the loss is\n",
      "     |  estimated each sample at a time and the model is updated along the way with\n",
      "     |  a decreasing strength schedule (aka learning rate).\n",
      "     |  \n",
      "     |  The regularizer is a penalty added to the loss function that shrinks model\n",
      "     |  parameters towards the zero vector using either the squared euclidean norm\n",
      "     |  L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
      "     |  parameter update crosses the 0.0 value because of the regularizer, the\n",
      "     |  update is truncated to 0.0 to allow for learning sparse models and achieve\n",
      "     |  online feature selection.\n",
      "     |  \n",
      "     |  This implementation works with data represented as dense numpy arrays of\n",
      "     |  floating point values for the features.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <sgd>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : str, default='squared_error'\n",
      "     |      The loss function to be used. The possible values are 'squared_error',\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'\n",
      "     |  \n",
      "     |      The 'squared_error' refers to the ordinary least squares fit.\n",
      "     |      'huber' modifies 'squared_error' to focus less on getting outliers\n",
      "     |      correct by switching from squared to linear loss past a distance of\n",
      "     |      epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is\n",
      "     |      linear past that; this is the loss function used in SVR.\n",
      "     |      'squared_epsilon_insensitive' is the same but becomes squared loss past\n",
      "     |      a tolerance of epsilon.\n",
      "     |  \n",
      "     |      More details about the losses formulas can be found in the\n",
      "     |      :ref:`User Guide <sgd_mathematical_formulation>`.\n",
      "     |  \n",
      "     |  penalty : {'l2', 'l1', 'elasticnet', None}, default='l2'\n",
      "     |      The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
      "     |      which is the standard regularizer for linear SVM models. 'l1' and\n",
      "     |      'elasticnet' might bring sparsity to the model (feature selection)\n",
      "     |      not achievable with 'l2'. No penalty is added when set to `None`.\n",
      "     |  \n",
      "     |      You can see a visualisation of the penalties in\n",
      "     |      :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_penalties.py`.\n",
      "     |  \n",
      "     |  alpha : float, default=0.0001\n",
      "     |      Constant that multiplies the regularization term. The higher the\n",
      "     |      value, the stronger the regularization. Also used to compute the\n",
      "     |      learning rate when `learning_rate` is set to 'optimal'.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |  l1_ratio : float, default=0.15\n",
      "     |      The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
      "     |      l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
      "     |      Only used if `penalty` is 'elasticnet'.\n",
      "     |      Values must be in the range `[0.0, 1.0]` or can be `None` if\n",
      "     |      `penalty` is not `elasticnet`.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.7\n",
      "     |          `l1_ratio` can be `None` when `penalty` is not \"elasticnet\".\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether the intercept should be estimated or not. If False, the\n",
      "     |      data is assumed to be already centered.\n",
      "     |  \n",
      "     |  max_iter : int, default=1000\n",
      "     |      The maximum number of passes over the training data (aka epochs).\n",
      "     |      It only impacts the behavior in the ``fit`` method, and not the\n",
      "     |      :meth:`partial_fit` method.\n",
      "     |      Values must be in the range `[1, inf)`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  tol : float or None, default=1e-3\n",
      "     |      The stopping criterion. If it is not None, training will stop\n",
      "     |      when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
      "     |      epochs.\n",
      "     |      Convergence is checked against the training loss or the\n",
      "     |      validation loss depending on the `early_stopping` parameter.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  shuffle : bool, default=True\n",
      "     |      Whether or not the training data should be shuffled after each epoch.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level.\n",
      "     |      Values must be in the range `[0, inf)`.\n",
      "     |  \n",
      "     |  epsilon : float, default=0.1\n",
      "     |      Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
      "     |      'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
      "     |      For 'huber', determines the threshold at which it becomes less\n",
      "     |      important to get the prediction exactly right.\n",
      "     |      For epsilon-insensitive, any differences between the current prediction\n",
      "     |      and the correct label are ignored if they are less than this threshold.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance, default=None\n",
      "     |      Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  learning_rate : str, default='invscaling'\n",
      "     |      The learning rate schedule:\n",
      "     |  \n",
      "     |      - 'constant': `eta = eta0`\n",
      "     |      - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
      "     |        where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
      "     |      - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
      "     |      - 'adaptive': eta = eta0, as long as the training keeps decreasing.\n",
      "     |        Each time n_iter_no_change consecutive epochs fail to decrease the\n",
      "     |        training loss by tol or fail to increase validation score by tol if\n",
      "     |        early_stopping is True, the current learning rate is divided by 5.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'adaptive' option.\n",
      "     |  \n",
      "     |  eta0 : float, default=0.01\n",
      "     |      The initial learning rate for the 'constant', 'invscaling' or\n",
      "     |      'adaptive' schedules. The default value is 0.01.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |  power_t : float, default=0.25\n",
      "     |      The exponent for inverse scaling learning rate.\n",
      "     |      Values must be in the range `(-inf, inf)`.\n",
      "     |  \n",
      "     |  early_stopping : bool, default=False\n",
      "     |      Whether to use early stopping to terminate training when validation\n",
      "     |      score is not improving. If set to True, it will automatically set aside\n",
      "     |      a fraction of training data as validation and terminate\n",
      "     |      training when validation score returned by the `score` method is not\n",
      "     |      improving by at least `tol` for `n_iter_no_change` consecutive\n",
      "     |      epochs.\n",
      "     |  \n",
      "     |      See :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_early_stopping.py` for an\n",
      "     |      example of the effects of early stopping.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'early_stopping' option\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if `early_stopping` is True.\n",
      "     |      Values must be in the range `(0.0, 1.0)`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'validation_fraction' option\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=5\n",
      "     |      Number of iterations with no improvement to wait before stopping\n",
      "     |      fitting.\n",
      "     |      Convergence is checked against the training loss or the\n",
      "     |      validation loss depending on the `early_stopping` parameter.\n",
      "     |      Integer values must be in the range `[1, max_iter)`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |          Added 'n_iter_no_change' option\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit as\n",
      "     |      initialization, otherwise, just erase the previous solution.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      Repeatedly calling fit or partial_fit when warm_start is True can\n",
      "     |      result in a different solution than when calling fit a single time\n",
      "     |      because of the way the data is shuffled.\n",
      "     |      If a dynamic learning rate is used, the learning rate is adapted\n",
      "     |      depending on the number of samples already seen. Calling ``fit`` resets\n",
      "     |      this counter, while ``partial_fit``  will result in increasing the\n",
      "     |      existing counter.\n",
      "     |  \n",
      "     |  average : bool or int, default=False\n",
      "     |      When set to True, computes the averaged SGD weights across all\n",
      "     |      updates and stores the result in the ``coef_`` attribute. If set to\n",
      "     |      an int greater than 1, averaging will begin once the total number of\n",
      "     |      samples seen reaches `average`. So ``average=10`` will begin\n",
      "     |      averaging after seeing 10 samples.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,)\n",
      "     |      Weights assigned to the features.\n",
      "     |  \n",
      "     |  intercept_ : ndarray of shape (1,)\n",
      "     |      The intercept term.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      The actual number of iterations before reaching the stopping criterion.\n",
      "     |  \n",
      "     |  t_ : int\n",
      "     |      Number of weight updates performed during training.\n",
      "     |      Same as ``(n_iter_ * n_samples + 1)``.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  HuberRegressor : Linear regression model that is robust to outliers.\n",
      "     |  Lars : Least Angle Regression model.\n",
      "     |  Lasso : Linear Model trained with L1 prior as regularizer.\n",
      "     |  RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.\n",
      "     |  Ridge : Linear least squares with l2 regularization.\n",
      "     |  sklearn.svm.SVR : Epsilon-Support Vector Regression.\n",
      "     |  TheilSenRegressor : Theil-Sen Estimator robust multivariate regression model.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import SGDRegressor\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> n_samples, n_features = 10, 5\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> y = rng.randn(n_samples)\n",
      "     |  >>> X = rng.randn(n_samples, n_features)\n",
      "     |  >>> # Always scale the input. The most convenient way is to use a pipeline.\n",
      "     |  >>> reg = make_pipeline(StandardScaler(),\n",
      "     |  ...                     SGDRegressor(max_iter=1000, tol=1e-3))\n",
      "     |  >>> reg.fit(X, y)\n",
      "     |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "     |                  ('sgdregressor', SGDRegressor())])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SGDRegressor\n",
      "     |      BaseSGDRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseSGD\n",
      "     |      sklearn.linear_model._base.SparseCoefMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='squared_error', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, warm_start=False, average=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._stochastic_gradient.SGDRegressor, *, coef_init: Union[bool, NoneType, str] = '$UNCHANGED$', intercept_init: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      coef_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``coef_init`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      intercept_init : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``intercept_init`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_partial_fit_request(self: sklearn.linear_model._stochastic_gradient.SGDRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``partial_fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._stochastic_gradient.SGDRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._stochastic_gradient.SGDRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSGDRegressor:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      "     |      Fit linear model with Stochastic Gradient Descent.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      coef_init : ndarray of shape (n_features,), default=None\n",
      "     |          The initial coefficients to warm-start the optimization.\n",
      "     |      \n",
      "     |      intercept_init : ndarray of shape (1,), default=None\n",
      "     |          The initial intercept to warm-start the optimization.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples (1. for unweighted).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted `SGDRegressor` estimator.\n",
      "     |  \n",
      "     |  partial_fit(self, X, y, sample_weight=None)\n",
      "     |      Perform one epoch of stochastic gradient descent on given samples.\n",
      "     |      \n",
      "     |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      "     |      guaranteed that a minimum of the cost function is reached after calling\n",
      "     |      it once. Matters such as objective convergence and early stopping\n",
      "     |      should be handled by the user.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Subset of training data.\n",
      "     |      \n",
      "     |      y : numpy array of shape (n_samples,)\n",
      "     |          Subset of target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape (n_samples,), default=None\n",
      "     |          Weights applied to individual samples.\n",
      "     |          If not provided, uniform weights are assumed.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns an instance of self.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Input data.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      ndarray of shape (n_samples,)\n",
      "     |         Predicted target values per element in X.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from BaseSGDRegressor:\n",
      "     |  \n",
      "     |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      "     |  \n",
      "     |  densify(self)\n",
      "     |      Convert coefficient matrix to dense array format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      "     |      default format of ``coef_`` and is required for fitting, so calling\n",
      "     |      this method is only required on models that have previously been\n",
      "     |      sparsified; otherwise, it is a no-op.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  sparsify(self)\n",
      "     |      Convert coefficient matrix to sparse format.\n",
      "     |      \n",
      "     |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      "     |      L1-regularized models can be much more memory- and storage-efficient\n",
      "     |      than the usual numpy.ndarray representation.\n",
      "     |      \n",
      "     |      The ``intercept_`` member is not converted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |          Fitted estimator.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      "     |      this may actually *increase* memory usage, so use this method with\n",
      "     |      care. A rule of thumb is that the number of zero elements, which can\n",
      "     |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      "     |      to provide significant benefits.\n",
      "     |      \n",
      "     |      After calling this method, further fitting with the partial_fit\n",
      "     |      method (if any) will not work until you call densify.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class TheilSenRegressor(sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      "     |  TheilSenRegressor(*, fit_intercept=True, copy_X='deprecated', max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=None, verbose=False)\n",
      "     |  \n",
      "     |  Theil-Sen Estimator: robust multivariate regression model.\n",
      "     |  \n",
      "     |  The algorithm calculates least square solutions on subsets with size\n",
      "     |  n_subsamples of the samples in X. Any value of n_subsamples between the\n",
      "     |  number of features and samples leads to an estimator with a compromise\n",
      "     |  between robustness and efficiency. Since the number of least square\n",
      "     |  solutions is \"n_samples choose n_subsamples\", it can be extremely large\n",
      "     |  and can therefore be limited with max_subpopulation. If this limit is\n",
      "     |  reached, the subsets are chosen randomly. In a final step, the spatial\n",
      "     |  median (or L1 median) is calculated of all least square solutions.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <theil_sen_regression>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Whether to calculate the intercept for this model. If set\n",
      "     |      to false, no intercept will be used in calculations.\n",
      "     |  \n",
      "     |  copy_X : bool, default=True\n",
      "     |      If True, X will be copied; else, it may be overwritten.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.6\n",
      "     |          `copy_X` was deprecated in 1.6 and will be removed in 1.8.\n",
      "     |          It has no effect as a copy is always made.\n",
      "     |  \n",
      "     |  max_subpopulation : int, default=1e4\n",
      "     |      Instead of computing with a set of cardinality 'n choose k', where n is\n",
      "     |      the number of samples and k is the number of subsamples (at least\n",
      "     |      number of features), consider only a stochastic subpopulation of a\n",
      "     |      given maximal size if 'n choose k' is larger than max_subpopulation.\n",
      "     |      For other than small problem sizes this parameter will determine\n",
      "     |      memory usage and runtime if n_subsamples is not changed. Note that the\n",
      "     |      data type should be int but floats such as 1e4 can be accepted too.\n",
      "     |  \n",
      "     |  n_subsamples : int, default=None\n",
      "     |      Number of samples to calculate the parameters. This is at least the\n",
      "     |      number of features (plus 1 if fit_intercept=True) and the number of\n",
      "     |      samples as a maximum. A lower number leads to a higher breakdown\n",
      "     |      point and a low efficiency while a high number leads to a low\n",
      "     |      breakdown point and a high efficiency. If None, take the\n",
      "     |      minimum number of subsamples leading to maximal robustness.\n",
      "     |      If n_subsamples is set to n_samples, Theil-Sen is identical to least\n",
      "     |      squares.\n",
      "     |  \n",
      "     |  max_iter : int, default=300\n",
      "     |      Maximum number of iterations for the calculation of spatial median.\n",
      "     |  \n",
      "     |  tol : float, default=1e-3\n",
      "     |      Tolerance when calculating spatial median.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      A random number generator instance to define the state of the random\n",
      "     |      permutations generator. Pass an int for reproducible output across\n",
      "     |      multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of CPUs to use during the cross validation.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      Verbose mode when fitting the model.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : ndarray of shape (n_features,)\n",
      "     |      Coefficients of the regression model (median of distribution).\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Estimated intercept of regression model.\n",
      "     |  \n",
      "     |  breakdown_ : float\n",
      "     |      Approximated breakdown point.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Number of iterations needed for the spatial median.\n",
      "     |  \n",
      "     |  n_subpopulation_ : int\n",
      "     |      Number of combinations taken into account from 'n choose k', where n is\n",
      "     |      the number of samples and k is the number of subsamples.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  HuberRegressor : Linear regression model that is robust to outliers.\n",
      "     |  RANSACRegressor : RANSAC (RANdom SAmple Consensus) algorithm.\n",
      "     |  SGDRegressor : Fitted by minimizing a regularized empirical loss with SGD.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  - Theil-Sen Estimators in a Multiple Linear Regression Model, 2009\n",
      "     |    Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang\n",
      "     |    http://home.olemiss.edu/~xdang/papers/MTSE.pdf\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.linear_model import TheilSenRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(\n",
      "     |  ...     n_samples=200, n_features=2, noise=4.0, random_state=0)\n",
      "     |  >>> reg = TheilSenRegressor(random_state=0).fit(X, y)\n",
      "     |  >>> reg.score(X, y)\n",
      "     |  0.9884\n",
      "     |  >>> reg.predict(X[:1,])\n",
      "     |  array([-31.5871])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TheilSenRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.linear_model._base.LinearModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fit_intercept=True, copy_X='deprecated', max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=None, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : ndarray of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : returns an instance of self.\n",
      "     |          Fitted `TheilSenRegressor` estimator.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._theil_sen.TheilSenRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._theil_sen.TheilSenRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using the linear model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      C : array, shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class TweedieRegressor(_GeneralizedLinearRegressor)\n",
      "     |  TweedieRegressor(*, power=0.0, alpha=1.0, fit_intercept=True, link='auto', solver='lbfgs', max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |  \n",
      "     |  Generalized Linear Model with a Tweedie distribution.\n",
      "     |  \n",
      "     |  This estimator can be used to model different GLMs depending on the\n",
      "     |  ``power`` parameter, which determines the underlying distribution.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <Generalized_linear_models>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  power : float, default=0\n",
      "     |          The power determines the underlying target distribution according\n",
      "     |          to the following table:\n",
      "     |  \n",
      "     |          +-------+------------------------+\n",
      "     |          | Power | Distribution           |\n",
      "     |          +=======+========================+\n",
      "     |          | 0     | Normal                 |\n",
      "     |          +-------+------------------------+\n",
      "     |          | 1     | Poisson                |\n",
      "     |          +-------+------------------------+\n",
      "     |          | (1,2) | Compound Poisson Gamma |\n",
      "     |          +-------+------------------------+\n",
      "     |          | 2     | Gamma                  |\n",
      "     |          +-------+------------------------+\n",
      "     |          | 3     | Inverse Gaussian       |\n",
      "     |          +-------+------------------------+\n",
      "     |  \n",
      "     |          For ``0 < power < 1``, no distribution exists.\n",
      "     |  \n",
      "     |  alpha : float, default=1\n",
      "     |      Constant that multiplies the L2 penalty term and determines the\n",
      "     |      regularization strength. ``alpha = 0`` is equivalent to unpenalized\n",
      "     |      GLMs. In this case, the design matrix `X` must have full column rank\n",
      "     |      (no collinearities).\n",
      "     |      Values of `alpha` must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |  fit_intercept : bool, default=True\n",
      "     |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "     |      added to the linear predictor (`X @ coef + intercept`).\n",
      "     |  \n",
      "     |  link : {'auto', 'identity', 'log'}, default='auto'\n",
      "     |      The link function of the GLM, i.e. mapping from linear predictor\n",
      "     |      `X @ coeff + intercept` to prediction `y_pred`. Option 'auto' sets\n",
      "     |      the link depending on the chosen `power` parameter as follows:\n",
      "     |  \n",
      "     |      - 'identity' for ``power <= 0``, e.g. for the Normal distribution\n",
      "     |      - 'log' for ``power > 0``, e.g. for Poisson, Gamma and Inverse Gaussian\n",
      "     |        distributions\n",
      "     |  \n",
      "     |  solver : {'lbfgs', 'newton-cholesky'}, default='lbfgs'\n",
      "     |      Algorithm to use in the optimization problem:\n",
      "     |  \n",
      "     |      'lbfgs'\n",
      "     |          Calls scipy's L-BFGS-B optimizer.\n",
      "     |  \n",
      "     |      'newton-cholesky'\n",
      "     |          Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to\n",
      "     |          iterated reweighted least squares) with an inner Cholesky based solver.\n",
      "     |          This solver is a good choice for `n_samples` >> `n_features`, especially\n",
      "     |          with one-hot encoded categorical features with rare categories. Be aware\n",
      "     |          that the memory usage of this solver has a quadratic dependency on\n",
      "     |          `n_features` because it explicitly computes the Hessian matrix.\n",
      "     |  \n",
      "     |          .. versionadded:: 1.2\n",
      "     |  \n",
      "     |  max_iter : int, default=100\n",
      "     |      The maximal number of iterations for the solver.\n",
      "     |      Values must be in the range `[1, inf)`.\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Stopping criterion. For the lbfgs solver,\n",
      "     |      the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``\n",
      "     |      where ``g_j`` is the j-th component of the gradient (derivative) of\n",
      "     |      the objective function.\n",
      "     |      Values must be in the range `(0.0, inf)`.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      If set to ``True``, reuse the solution of the previous call to ``fit``\n",
      "     |      as initialization for ``coef_`` and ``intercept_`` .\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      For the lbfgs solver set verbose to any positive number for verbosity.\n",
      "     |      Values must be in the range `[0, inf)`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  coef_ : array of shape (n_features,)\n",
      "     |      Estimated coefficients for the linear predictor (`X @ coef_ +\n",
      "     |      intercept_`) in the GLM.\n",
      "     |  \n",
      "     |  intercept_ : float\n",
      "     |      Intercept (a.k.a. bias) added to linear predictor.\n",
      "     |  \n",
      "     |  n_iter_ : int\n",
      "     |      Actual number of iterations used in the solver.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  PoissonRegressor : Generalized Linear Model with a Poisson distribution.\n",
      "     |  GammaRegressor : Generalized Linear Model with a Gamma distribution.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import linear_model\n",
      "     |  >>> clf = linear_model.TweedieRegressor()\n",
      "     |  >>> X = [[1, 2], [2, 3], [3, 4], [4, 3]]\n",
      "     |  >>> y = [2, 3.5, 5, 5.5]\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  TweedieRegressor()\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  np.float64(0.839)\n",
      "     |  >>> clf.coef_\n",
      "     |  array([0.599, 0.299])\n",
      "     |  >>> clf.intercept_\n",
      "     |  np.float64(1.600)\n",
      "     |  >>> clf.predict([[1, 1], [3, 4]])\n",
      "     |  array([2.500, 4.599])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TweedieRegressor\n",
      "     |      _GeneralizedLinearRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, power=0.0, alpha=1.0, fit_intercept=True, link='auto', solver='lbfgs', max_iter=100, tol=0.0001, warm_start=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.linear_model._glm.glm.TweedieRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._glm.glm.TweedieRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.linear_model._glm.glm.TweedieRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._glm.glm.TweedieRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _GeneralizedLinearRegressor:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit a Generalized Linear Model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted model.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict using GLM with feature matrix X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : array of shape (n_samples,)\n",
      "     |          Returns predicted values.\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Compute D^2, the percentage of deviance explained.\n",
      "     |      \n",
      "     |      D^2 is a generalization of the coefficient of determination R^2.\n",
      "     |      R^2 uses squared error and D^2 uses the deviance of this GLM, see the\n",
      "     |      :ref:`User Guide <regression_metrics>`.\n",
      "     |      \n",
      "     |      D^2 is defined as\n",
      "     |      :math:`D^2 = 1-\\frac{D(y_{true},y_{pred})}{D_{null}}`,\n",
      "     |      :math:`D_{null}` is the null deviance, i.e. the deviance of a model\n",
      "     |      with intercept alone, which corresponds to :math:`y_{pred} = \\bar{y}`.\n",
      "     |      The mean :math:`\\bar{y}` is averaged by sample_weight.\n",
      "     |      Best possible score is 1.0 and it can be negative (because the model\n",
      "     |      can be arbitrarily worse).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          True values of target.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          D^2 of self.predict(X) w.r.t. y.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n",
      "FUNCTIONS\n",
      "    enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      "        Compute elastic net path with coordinate descent.\n",
      "        \n",
      "        The elastic net optimization function varies for mono and multi-outputs.\n",
      "        \n",
      "        For mono-output tasks it is::\n",
      "        \n",
      "            1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      "            + alpha * l1_ratio * ||w||_1\n",
      "            + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      "        \n",
      "        For multi-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      "            + alpha * l1_ratio * ||W||_21\n",
      "            + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      "        \n",
      "        Where::\n",
      "        \n",
      "            ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "        \n",
      "        i.e. the sum of norm of each row.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <elastic_net>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "            unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "            can be sparse.\n",
      "        \n",
      "        y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "            Target values.\n",
      "        \n",
      "        l1_ratio : float, default=0.5\n",
      "            Number between 0 and 1 passed to elastic net (scaling between\n",
      "            l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      "        \n",
      "        eps : float, default=1e-3\n",
      "            Length of the path. ``eps=1e-3`` means that\n",
      "            ``alpha_min / alpha_max = 1e-3``.\n",
      "        \n",
      "        n_alphas : int, default=100\n",
      "            Number of alphas along the regularization path.\n",
      "        \n",
      "        alphas : array-like, default=None\n",
      "            List of alphas where to compute the models.\n",
      "            If None alphas are set automatically.\n",
      "        \n",
      "        precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "            Whether to use a precomputed Gram matrix to speed up\n",
      "            calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "            matrix can also be passed as argument.\n",
      "        \n",
      "        Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If ``True``, X will be copied; else, it may be overwritten.\n",
      "        \n",
      "        coef_init : array-like of shape (n_features, ), default=None\n",
      "            The initial values of the coefficients.\n",
      "        \n",
      "        verbose : bool or int, default=False\n",
      "            Amount of verbosity.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether to return the number of iterations or not.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            If set to True, forces coefficients to be positive.\n",
      "            (Only allowed when ``y.ndim == 1``).\n",
      "        \n",
      "        check_input : bool, default=True\n",
      "            If set to False, the input validation checks are skipped (including the\n",
      "            Gram matrix when provided). It is assumed that they are handled\n",
      "            by the caller.\n",
      "        \n",
      "        **params : kwargs\n",
      "            Keyword arguments passed to the coordinate descent solver.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : ndarray of shape (n_alphas,)\n",
      "            The alphas along the path where models are computed.\n",
      "        \n",
      "        coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "            Coefficients along the path.\n",
      "        \n",
      "        dual_gaps : ndarray of shape (n_alphas,)\n",
      "            The dual gaps at the end of the optimization for each alpha.\n",
      "        \n",
      "        n_iters : list of int\n",
      "            The number of iterations taken by the coordinate descent optimizer to\n",
      "            reach the specified tolerance for each alpha.\n",
      "            (Is returned when ``return_n_iter`` is set to True).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      "        MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      "        ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      "        ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see\n",
      "        :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n",
      "        <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.linear_model import enet_path\n",
      "        >>> from sklearn.datasets import make_regression\n",
      "        >>> X, y, true_coef = make_regression(\n",
      "        ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n",
      "        ... )\n",
      "        >>> true_coef\n",
      "        array([ 0.        ,  0.        ,  0.        , 97.9, 45.7])\n",
      "        >>> alphas, estimated_coef, _ = enet_path(X, y, n_alphas=3)\n",
      "        >>> alphas.shape\n",
      "        (3,)\n",
      "        >>> estimated_coef\n",
      "         array([[ 0.,  0.787,  0.568],\n",
      "                [ 0.,  1.120,  0.620],\n",
      "                [-0., -2.129, -1.128],\n",
      "                [ 0., 23.046, 88.939],\n",
      "                [ 0., 10.637, 41.566]])\n",
      "    \n",
      "    lars_path(X, y, Xy=None, *, Gram=None, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=np.float64(2.220446049250313e-16), copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)\n",
      "        Compute Least Angle Regression or Lasso path using the LARS algorithm.\n",
      "        \n",
      "        The optimization objective for the case method='lasso' is::\n",
      "        \n",
      "        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        in the case of method='lar', the objective function is only known in\n",
      "        the form of an implicit equation (see discussion in [1]_).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : None or ndarray of shape (n_samples, n_features)\n",
      "            Input data. If X is `None`, Gram must also be `None`.\n",
      "            If only the Gram matrix is available, use `lars_path_gram` instead.\n",
      "        \n",
      "        y : None or ndarray of shape (n_samples,)\n",
      "            Input targets.\n",
      "        \n",
      "        Xy : array-like of shape (n_features,), default=None\n",
      "            `Xy = X.T @ y` that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        Gram : None, 'auto', bool, ndarray of shape (n_features, n_features),             default=None\n",
      "            Precomputed Gram matrix `X.T @ X`, if `'auto'`, the Gram\n",
      "            matrix is precomputed from the given X, if there are more samples\n",
      "            than features.\n",
      "        \n",
      "        max_iter : int, default=500\n",
      "            Maximum number of iterations to perform, set to infinity for no limit.\n",
      "        \n",
      "        alpha_min : float, default=0\n",
      "            Minimum correlation along the path. It corresponds to the\n",
      "            regularization parameter `alpha` in the Lasso.\n",
      "        \n",
      "        method : {'lar', 'lasso'}, default='lar'\n",
      "            Specifies the returned model. Select `'lar'` for Least Angle\n",
      "            Regression, `'lasso'` for the Lasso.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If `False`, `X` is overwritten.\n",
      "        \n",
      "        eps : float, default=np.finfo(float).eps\n",
      "            The machine-precision regularization in the computation of the\n",
      "            Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "            systems. Unlike the `tol` parameter in some iterative\n",
      "            optimization-based algorithms, this parameter does not control\n",
      "            the tolerance of the optimization.\n",
      "        \n",
      "        copy_Gram : bool, default=True\n",
      "            If `False`, `Gram` is overwritten.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Controls output verbosity.\n",
      "        \n",
      "        return_path : bool, default=True\n",
      "            If `True`, returns the entire path, else returns only the\n",
      "            last point of the path.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether to return the number of iterations.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            Restrict coefficients to be >= 0.\n",
      "            This option is only allowed with method 'lasso'. Note that the model\n",
      "            coefficients will not converge to the ordinary-least-squares solution\n",
      "            for small values of alpha. Only coefficients up to the smallest alpha\n",
      "            value (`alphas_[alphas_ > 0.].min()` when fit_path=True) reached by\n",
      "            the stepwise Lars-Lasso algorithm are typically in congruence with the\n",
      "            solution of the coordinate descent `lasso_path` function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : ndarray of shape (n_alphas + 1,)\n",
      "            Maximum of covariances (in absolute value) at each iteration.\n",
      "            `n_alphas` is either `max_iter`, `n_features`, or the\n",
      "            number of nodes in the path with `alpha >= alpha_min`, whichever\n",
      "            is smaller.\n",
      "        \n",
      "        active : ndarray of shape (n_alphas,)\n",
      "            Indices of active variables at the end of the path.\n",
      "        \n",
      "        coefs : ndarray of shape (n_features, n_alphas + 1)\n",
      "            Coefficients along the path.\n",
      "        \n",
      "        n_iter : int\n",
      "            Number of iterations run. Returned only if `return_n_iter` is set\n",
      "            to True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lars_path_gram : Compute LARS path in the sufficient stats mode.\n",
      "        lasso_path : Compute Lasso path with coordinate descent.\n",
      "        LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "        Lars : Least Angle Regression model a.k.a. LAR.\n",
      "        LassoLarsCV : Cross-validated Lasso, using the LARS algorithm.\n",
      "        LarsCV : Cross-validated Least Angle Regression model.\n",
      "        sklearn.decomposition.sparse_encode : Sparse coding.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Least Angle Regression\", Efron et al.\n",
      "               http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Least-angle regression\n",
      "               <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n",
      "        \n",
      "        .. [3] `Wikipedia entry on the Lasso\n",
      "               <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.linear_model import lars_path\n",
      "        >>> from sklearn.datasets import make_regression\n",
      "        >>> X, y, true_coef = make_regression(\n",
      "        ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n",
      "        ... )\n",
      "        >>> true_coef\n",
      "        array([ 0.        ,  0.        ,  0.        , 97.9, 45.7])\n",
      "        >>> alphas, _, estimated_coef = lars_path(X, y)\n",
      "        >>> alphas.shape\n",
      "        (3,)\n",
      "        >>> estimated_coef\n",
      "        array([[ 0.     ,  0.     ,  0.     ],\n",
      "               [ 0.     ,  0.     ,  0.     ],\n",
      "               [ 0.     ,  0.     ,  0.     ],\n",
      "               [ 0.     , 46.96, 97.99],\n",
      "               [ 0.     ,  0.     , 45.70]])\n",
      "    \n",
      "    lars_path_gram(Xy, Gram, *, n_samples, max_iter=500, alpha_min=0, method='lar', copy_X=True, eps=np.float64(2.220446049250313e-16), copy_Gram=True, verbose=0, return_path=True, return_n_iter=False, positive=False)\n",
      "        The lars_path in the sufficient stats mode.\n",
      "        \n",
      "        The optimization objective for the case method='lasso' is::\n",
      "        \n",
      "        (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        in the case of method='lar', the objective function is only known in\n",
      "        the form of an implicit equation (see discussion in [1]_).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <least_angle_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        Xy : ndarray of shape (n_features,)\n",
      "            `Xy = X.T @ y`.\n",
      "        \n",
      "        Gram : ndarray of shape (n_features, n_features)\n",
      "            `Gram = X.T @ X`.\n",
      "        \n",
      "        n_samples : int\n",
      "            Equivalent size of sample.\n",
      "        \n",
      "        max_iter : int, default=500\n",
      "            Maximum number of iterations to perform, set to infinity for no limit.\n",
      "        \n",
      "        alpha_min : float, default=0\n",
      "            Minimum correlation along the path. It corresponds to the\n",
      "            regularization parameter alpha parameter in the Lasso.\n",
      "        \n",
      "        method : {'lar', 'lasso'}, default='lar'\n",
      "            Specifies the returned model. Select `'lar'` for Least Angle\n",
      "            Regression, ``'lasso'`` for the Lasso.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If `False`, `X` is overwritten.\n",
      "        \n",
      "        eps : float, default=np.finfo(float).eps\n",
      "            The machine-precision regularization in the computation of the\n",
      "            Cholesky diagonal factors. Increase this for very ill-conditioned\n",
      "            systems. Unlike the `tol` parameter in some iterative\n",
      "            optimization-based algorithms, this parameter does not control\n",
      "            the tolerance of the optimization.\n",
      "        \n",
      "        copy_Gram : bool, default=True\n",
      "            If `False`, `Gram` is overwritten.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Controls output verbosity.\n",
      "        \n",
      "        return_path : bool, default=True\n",
      "            If `return_path==True` returns the entire path, else returns only the\n",
      "            last point of the path.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether to return the number of iterations.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            Restrict coefficients to be >= 0.\n",
      "            This option is only allowed with method 'lasso'. Note that the model\n",
      "            coefficients will not converge to the ordinary-least-squares solution\n",
      "            for small values of alpha. Only coefficients up to the smallest alpha\n",
      "            value (`alphas_[alphas_ > 0.].min()` when `fit_path=True`) reached by\n",
      "            the stepwise Lars-Lasso algorithm are typically in congruence with the\n",
      "            solution of the coordinate descent lasso_path function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : ndarray of shape (n_alphas + 1,)\n",
      "            Maximum of covariances (in absolute value) at each iteration.\n",
      "            `n_alphas` is either `max_iter`, `n_features` or the\n",
      "            number of nodes in the path with `alpha >= alpha_min`, whichever\n",
      "            is smaller.\n",
      "        \n",
      "        active : ndarray of shape (n_alphas,)\n",
      "            Indices of active variables at the end of the path.\n",
      "        \n",
      "        coefs : ndarray of shape (n_features, n_alphas + 1)\n",
      "            Coefficients along the path.\n",
      "        \n",
      "        n_iter : int\n",
      "            Number of iterations run. Returned only if `return_n_iter` is set\n",
      "            to True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lars_path_gram : Compute LARS path.\n",
      "        lasso_path : Compute Lasso path with coordinate descent.\n",
      "        LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "        Lars : Least Angle Regression model a.k.a. LAR.\n",
      "        LassoLarsCV : Cross-validated Lasso, using the LARS algorithm.\n",
      "        LarsCV : Cross-validated Least Angle Regression model.\n",
      "        sklearn.decomposition.sparse_encode : Sparse coding.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Least Angle Regression\", Efron et al.\n",
      "               http://statweb.stanford.edu/~tibs/ftp/lars.pdf\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Least-angle regression\n",
      "               <https://en.wikipedia.org/wiki/Least-angle_regression>`_\n",
      "        \n",
      "        .. [3] `Wikipedia entry on the Lasso\n",
      "               <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.linear_model import lars_path_gram\n",
      "        >>> from sklearn.datasets import make_regression\n",
      "        >>> X, y, true_coef = make_regression(\n",
      "        ...    n_samples=100, n_features=5, n_informative=2, coef=True, random_state=0\n",
      "        ... )\n",
      "        >>> true_coef\n",
      "        array([ 0.        ,  0.        ,  0.        , 97.9, 45.7])\n",
      "        >>> alphas, _, estimated_coef = lars_path_gram(X.T @ y, X.T @ X, n_samples=100)\n",
      "        >>> alphas.shape\n",
      "        (3,)\n",
      "        >>> estimated_coef\n",
      "        array([[ 0.     ,  0.     ,  0.     ],\n",
      "               [ 0.     ,  0.     ,  0.     ],\n",
      "               [ 0.     ,  0.     ,  0.     ],\n",
      "               [ 0.     , 46.96, 97.99],\n",
      "               [ 0.     ,  0.     , 45.70]])\n",
      "    \n",
      "    lasso_path(X, y, *, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, **params)\n",
      "        Compute Lasso path with coordinate descent.\n",
      "        \n",
      "        The Lasso optimization function varies for mono and multi-outputs.\n",
      "        \n",
      "        For mono-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n",
      "        \n",
      "        For multi-output tasks it is::\n",
      "        \n",
      "            (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n",
      "        \n",
      "        Where::\n",
      "        \n",
      "            ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      "        \n",
      "        i.e. the sum of norm of each row.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <lasso>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Training data. Pass directly as Fortran-contiguous data to avoid\n",
      "            unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      "            can be sparse.\n",
      "        \n",
      "        y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      "            Target values.\n",
      "        \n",
      "        eps : float, default=1e-3\n",
      "            Length of the path. ``eps=1e-3`` means that\n",
      "            ``alpha_min / alpha_max = 1e-3``.\n",
      "        \n",
      "        n_alphas : int, default=100\n",
      "            Number of alphas along the regularization path.\n",
      "        \n",
      "        alphas : array-like, default=None\n",
      "            List of alphas where to compute the models.\n",
      "            If ``None`` alphas are set automatically.\n",
      "        \n",
      "        precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      "            Whether to use a precomputed Gram matrix to speed up\n",
      "            calculations. If set to ``'auto'`` let us decide. The Gram\n",
      "            matrix can also be passed as argument.\n",
      "        \n",
      "        Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      "            Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      "            only when the Gram matrix is precomputed.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            If ``True``, X will be copied; else, it may be overwritten.\n",
      "        \n",
      "        coef_init : array-like of shape (n_features, ), default=None\n",
      "            The initial values of the coefficients.\n",
      "        \n",
      "        verbose : bool or int, default=False\n",
      "            Amount of verbosity.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether to return the number of iterations or not.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            If set to True, forces coefficients to be positive.\n",
      "            (Only allowed when ``y.ndim == 1``).\n",
      "        \n",
      "        **params : kwargs\n",
      "            Keyword arguments passed to the coordinate descent solver.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alphas : ndarray of shape (n_alphas,)\n",
      "            The alphas along the path where models are computed.\n",
      "        \n",
      "        coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      "            Coefficients along the path.\n",
      "        \n",
      "        dual_gaps : ndarray of shape (n_alphas,)\n",
      "            The dual gaps at the end of the optimization for each alpha.\n",
      "        \n",
      "        n_iters : list of int\n",
      "            The number of iterations taken by the coordinate descent optimizer to\n",
      "            reach the specified tolerance for each alpha.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lars_path : Compute Least Angle Regression or Lasso path using LARS\n",
      "            algorithm.\n",
      "        Lasso : The Lasso is a linear model that estimates sparse coefficients.\n",
      "        LassoLars : Lasso model fit with Least Angle Regression a.k.a. Lars.\n",
      "        LassoCV : Lasso linear model with iterative fitting along a regularization\n",
      "            path.\n",
      "        LassoLarsCV : Cross-validated Lasso using the LARS algorithm.\n",
      "        sklearn.decomposition.sparse_encode : Estimator that can be used to\n",
      "            transform signals into sparse linear combination of atoms from a fixed.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For an example, see\n",
      "        :ref:`examples/linear_model/plot_lasso_lasso_lars_elasticnet_path.py\n",
      "        <sphx_glr_auto_examples_linear_model_plot_lasso_lasso_lars_elasticnet_path.py>`.\n",
      "        \n",
      "        To avoid unnecessary memory duplication the X argument of the fit method\n",
      "        should be directly passed as a Fortran-contiguous numpy array.\n",
      "        \n",
      "        Note that in certain cases, the Lars solver may be significantly\n",
      "        faster to implement this functionality. In particular, linear\n",
      "        interpolation can be used to retrieve model coefficients between the\n",
      "        values output by lars_path\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Comparing lasso_path and lars_path with interpolation:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.linear_model import lasso_path\n",
      "        >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T\n",
      "        >>> y = np.array([1, 2, 3.1])\n",
      "        >>> # Use lasso_path to compute a coefficient path\n",
      "        >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5])\n",
      "        >>> print(coef_path)\n",
      "        [[0.         0.         0.46874778]\n",
      "         [0.2159048  0.4425765  0.23689075]]\n",
      "        \n",
      "        >>> # Now use lars_path and 1D linear interpolation to compute the\n",
      "        >>> # same path\n",
      "        >>> from sklearn.linear_model import lars_path\n",
      "        >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso')\n",
      "        >>> from scipy import interpolate\n",
      "        >>> coef_path_continuous = interpolate.interp1d(alphas[::-1],\n",
      "        ...                                             coef_path_lars[:, ::-1])\n",
      "        >>> print(coef_path_continuous([5., 1., .5]))\n",
      "        [[0.         0.         0.46915237]\n",
      "         [0.2159048  0.4425765  0.23668876]]\n",
      "    \n",
      "    orthogonal_mp(X, y, *, n_nonzero_coefs=None, tol=None, precompute=False, copy_X=True, return_path=False, return_n_iter=False)\n",
      "        Orthogonal Matching Pursuit (OMP).\n",
      "        \n",
      "        Solves n_targets Orthogonal Matching Pursuit problems.\n",
      "        An instance of the problem has the form:\n",
      "        \n",
      "        When parametrized by the number of non-zero coefficients using\n",
      "        `n_nonzero_coefs`:\n",
      "        argmin ||y - X\\gamma||^2 subject to ||\\gamma||_0 <= n_{nonzero coefs}\n",
      "        \n",
      "        When parametrized by error using the parameter `tol`:\n",
      "        argmin ||\\gamma||_0 subject to ||y - X\\gamma||^2 <= tol\n",
      "        \n",
      "        Read more in the :ref:`User Guide <omp>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Input data. Columns are assumed to have unit norm.\n",
      "        \n",
      "        y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "            Input targets.\n",
      "        \n",
      "        n_nonzero_coefs : int, default=None\n",
      "            Desired number of non-zero entries in the solution. If None (by\n",
      "            default) this value is set to 10% of n_features.\n",
      "        \n",
      "        tol : float, default=None\n",
      "            Maximum squared norm of the residual. If not None, overrides n_nonzero_coefs.\n",
      "        \n",
      "        precompute : 'auto' or bool, default=False\n",
      "            Whether to perform precomputations. Improves performance when n_targets\n",
      "            or n_samples is very large.\n",
      "        \n",
      "        copy_X : bool, default=True\n",
      "            Whether the design matrix X must be copied by the algorithm. A false\n",
      "            value is only helpful if X is already Fortran-ordered, otherwise a\n",
      "            copy is made anyway.\n",
      "        \n",
      "        return_path : bool, default=False\n",
      "            Whether to return every value of the nonzero coefficients along the\n",
      "            forward path. Useful for cross-validation.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : ndarray of shape (n_features,) or (n_features, n_targets)\n",
      "            Coefficients of the OMP solution. If `return_path=True`, this contains\n",
      "            the whole coefficient path. In this case its shape is\n",
      "            (n_features, n_features) or (n_features, n_targets, n_features) and\n",
      "            iterating over the last axis generates coefficients in increasing order\n",
      "            of active features.\n",
      "        \n",
      "        n_iters : array-like or int\n",
      "            Number of active features across every target. Returned only if\n",
      "            `return_n_iter` is set to True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model.\n",
      "        orthogonal_mp_gram : Solve OMP problems using Gram matrix and the product X.T * y.\n",
      "        lars_path : Compute Least Angle Regression or Lasso path using LARS algorithm.\n",
      "        sklearn.decomposition.sparse_encode : Sparse coding.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang,\n",
      "        Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "        Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "        (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "        \n",
      "        This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "        M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "        Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "        https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets import make_regression\n",
      "        >>> from sklearn.linear_model import orthogonal_mp\n",
      "        >>> X, y = make_regression(noise=4, random_state=0)\n",
      "        >>> coef = orthogonal_mp(X, y)\n",
      "        >>> coef.shape\n",
      "        (100,)\n",
      "        >>> X[:1,] @ coef\n",
      "        array([-78.68])\n",
      "    \n",
      "    orthogonal_mp_gram(Gram, Xy, *, n_nonzero_coefs=None, tol=None, norms_squared=None, copy_Gram=True, copy_Xy=True, return_path=False, return_n_iter=False)\n",
      "        Gram Orthogonal Matching Pursuit (OMP).\n",
      "        \n",
      "        Solves n_targets Orthogonal Matching Pursuit problems using only\n",
      "        the Gram matrix X.T * X and the product X.T * y.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <omp>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        Gram : array-like of shape (n_features, n_features)\n",
      "            Gram matrix of the input data: `X.T * X`.\n",
      "        \n",
      "        Xy : array-like of shape (n_features,) or (n_features, n_targets)\n",
      "            Input targets multiplied by `X`: `X.T * y`.\n",
      "        \n",
      "        n_nonzero_coefs : int, default=None\n",
      "            Desired number of non-zero entries in the solution. If `None` (by\n",
      "            default) this value is set to 10% of n_features.\n",
      "        \n",
      "        tol : float, default=None\n",
      "            Maximum squared norm of the residual. If not `None`,\n",
      "            overrides `n_nonzero_coefs`.\n",
      "        \n",
      "        norms_squared : array-like of shape (n_targets,), default=None\n",
      "            Squared L2 norms of the lines of `y`. Required if `tol` is not None.\n",
      "        \n",
      "        copy_Gram : bool, default=True\n",
      "            Whether the gram matrix must be copied by the algorithm. A `False`\n",
      "            value is only helpful if it is already Fortran-ordered, otherwise a\n",
      "            copy is made anyway.\n",
      "        \n",
      "        copy_Xy : bool, default=True\n",
      "            Whether the covariance vector `Xy` must be copied by the algorithm.\n",
      "            If `False`, it may be overwritten.\n",
      "        \n",
      "        return_path : bool, default=False\n",
      "            Whether to return every value of the nonzero coefficients along the\n",
      "            forward path. Useful for cross-validation.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            Whether or not to return the number of iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : ndarray of shape (n_features,) or (n_features, n_targets)\n",
      "            Coefficients of the OMP solution. If `return_path=True`, this contains\n",
      "            the whole coefficient path. In this case its shape is\n",
      "            `(n_features, n_features)` or `(n_features, n_targets, n_features)` and\n",
      "            iterating over the last axis yields coefficients in increasing order\n",
      "            of active features.\n",
      "        \n",
      "        n_iters : list or int\n",
      "            Number of active features across every target. Returned only if\n",
      "            `return_n_iter` is set to True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        OrthogonalMatchingPursuit : Orthogonal Matching Pursuit model (OMP).\n",
      "        orthogonal_mp : Solves n_targets Orthogonal Matching Pursuit problems.\n",
      "        lars_path : Compute Least Angle Regression or Lasso path using\n",
      "            LARS algorithm.\n",
      "        sklearn.decomposition.sparse_encode : Generic sparse coding.\n",
      "            Each column of the result is the solution to a Lasso problem.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,\n",
      "        Matching pursuits with time-frequency dictionaries, IEEE Transactions on\n",
      "        Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.\n",
      "        (https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf)\n",
      "        \n",
      "        This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,\n",
      "        M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal\n",
      "        Matching Pursuit Technical Report - CS Technion, April 2008.\n",
      "        https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets import make_regression\n",
      "        >>> from sklearn.linear_model import orthogonal_mp_gram\n",
      "        >>> X, y = make_regression(noise=4, random_state=0)\n",
      "        >>> coef = orthogonal_mp_gram(X.T @ X, X.T @ y)\n",
      "        >>> coef.shape\n",
      "        (100,)\n",
      "        >>> X[:1,] @ coef\n",
      "        array([-78.68])\n",
      "    \n",
      "    ridge_regression(X, y, alpha, *, sample_weight=None, solver='auto', max_iter=None, tol=0.0001, verbose=0, positive=False, random_state=None, return_n_iter=False, return_intercept=False, check_input=True)\n",
      "        Solve the ridge equation by the method of normal equations.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <ridge_regression>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix, LinearOperator} of shape         (n_samples, n_features)\n",
      "            Training data.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      "            Target values.\n",
      "        \n",
      "        alpha : float or array-like of shape (n_targets,)\n",
      "            Constant that multiplies the L2 term, controlling regularization\n",
      "            strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.\n",
      "        \n",
      "            When `alpha = 0`, the objective is equivalent to ordinary least\n",
      "            squares, solved by the :class:`LinearRegression` object. For numerical\n",
      "            reasons, using `alpha = 0` with the `Ridge` object is not advised.\n",
      "            Instead, you should use the :class:`LinearRegression` object.\n",
      "        \n",
      "            If an array is passed, penalties are assumed to be specific to the\n",
      "            targets. Hence they must correspond in number.\n",
      "        \n",
      "        sample_weight : float or array-like of shape (n_samples,), default=None\n",
      "            Individual weights for each sample. If given a float, every sample\n",
      "            will have the same weight. If sample_weight is not None and\n",
      "            solver='auto', the solver will be set to 'cholesky'.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg',             'sag', 'saga', 'lbfgs'}, default='auto'\n",
      "            Solver to use in the computational routines:\n",
      "        \n",
      "            - 'auto' chooses the solver automatically based on the type of data.\n",
      "        \n",
      "            - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n",
      "              coefficients. It is the most stable solver, in particular more stable\n",
      "              for singular matrices than 'cholesky' at the cost of being slower.\n",
      "        \n",
      "            - 'cholesky' uses the standard scipy.linalg.solve function to\n",
      "              obtain a closed-form solution via a Cholesky decomposition of\n",
      "              dot(X.T, X)\n",
      "        \n",
      "            - 'sparse_cg' uses the conjugate gradient solver as found in\n",
      "              scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n",
      "              more appropriate than 'cholesky' for large-scale data\n",
      "              (possibility to set `tol` and `max_iter`).\n",
      "        \n",
      "            - 'lsqr' uses the dedicated regularized least-squares routine\n",
      "              scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n",
      "              procedure.\n",
      "        \n",
      "            - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n",
      "              its improved, unbiased version named SAGA. Both methods also use an\n",
      "              iterative procedure, and are often faster than other solvers when\n",
      "              both n_samples and n_features are large. Note that 'sag' and\n",
      "              'saga' fast convergence is only guaranteed on features with\n",
      "              approximately the same scale. You can preprocess the data with a\n",
      "              scaler from sklearn.preprocessing.\n",
      "        \n",
      "            - 'lbfgs' uses L-BFGS-B algorithm implemented in\n",
      "              `scipy.optimize.minimize`. It can be used only when `positive`\n",
      "              is True.\n",
      "        \n",
      "            All solvers except 'svd' support both dense and sparse data. However, only\n",
      "            'lsqr', 'sag', 'sparse_cg', and 'lbfgs' support sparse input when\n",
      "            `fit_intercept` is True.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               Stochastic Average Gradient descent solver.\n",
      "            .. versionadded:: 0.19\n",
      "               SAGA solver.\n",
      "        \n",
      "        max_iter : int, default=None\n",
      "            Maximum number of iterations for conjugate gradient solver.\n",
      "            For the 'sparse_cg' and 'lsqr' solvers, the default value is determined\n",
      "            by scipy.sparse.linalg. For 'sag' and saga solver, the default value is\n",
      "            1000. For 'lbfgs' solver, the default value is 15000.\n",
      "        \n",
      "        tol : float, default=1e-4\n",
      "            Precision of the solution. Note that `tol` has no effect for solvers 'svd' and\n",
      "            'cholesky'.\n",
      "        \n",
      "            .. versionchanged:: 1.2\n",
      "               Default value changed from 1e-3 to 1e-4 for consistency with other linear\n",
      "               models.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Verbosity level. Setting verbose > 0 will display additional\n",
      "            information depending on the solver used.\n",
      "        \n",
      "        positive : bool, default=False\n",
      "            When set to ``True``, forces the coefficients to be positive.\n",
      "            Only 'lbfgs' solver is supported in this case.\n",
      "        \n",
      "        random_state : int, RandomState instance, default=None\n",
      "            Used when ``solver`` == 'sag' or 'saga' to shuffle the data.\n",
      "            See :term:`Glossary <random_state>` for details.\n",
      "        \n",
      "        return_n_iter : bool, default=False\n",
      "            If True, the method also returns `n_iter`, the actual number of\n",
      "            iteration performed by the solver.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        return_intercept : bool, default=False\n",
      "            If True and if X is sparse, the method also returns the intercept,\n",
      "            and the solver is automatically changed to 'sag'. This is only a\n",
      "            temporary fix for fitting the intercept with sparse data. For dense\n",
      "            data, use sklearn.linear_model._preprocess_data before your regression.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        \n",
      "        check_input : bool, default=True\n",
      "            If False, the input arrays X and y will not be checked.\n",
      "        \n",
      "            .. versionadded:: 0.21\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coef : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      "            Weight vector(s).\n",
      "        \n",
      "        n_iter : int, optional\n",
      "            The actual number of iteration performed by the solver.\n",
      "            Only returned if `return_n_iter` is True.\n",
      "        \n",
      "        intercept : float or ndarray of shape (n_targets,)\n",
      "            The intercept of the model. Only returned if `return_intercept`\n",
      "            is True and if X is a scipy sparse array.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function won't compute the intercept.\n",
      "        \n",
      "        Regularization improves the conditioning of the problem and\n",
      "        reduces the variance of the estimates. Larger values specify stronger\n",
      "        regularization. Alpha corresponds to ``1 / (2C)`` in other linear\n",
      "        models such as :class:`~sklearn.linear_model.LogisticRegression` or\n",
      "        :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are\n",
      "        assumed to be specific to the targets. Hence they must correspond in\n",
      "        number.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.datasets import make_regression\n",
      "        >>> from sklearn.linear_model import ridge_regression\n",
      "        >>> rng = np.random.RandomState(0)\n",
      "        >>> X = rng.randn(100, 4)\n",
      "        >>> y = 2.0 * X[:, 0] - 1.0 * X[:, 1] + 0.1 * rng.standard_normal(100)\n",
      "        >>> coef, intercept = ridge_regression(X, y, alpha=1.0, return_intercept=True,\n",
      "        ...                                    random_state=0)\n",
      "        >>> coef\n",
      "        array([ 1.97, -1., -2.69e-3, -9.27e-4 ])\n",
      "        >>> intercept\n",
      "        np.float64(-.0012)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['ARDRegression', 'BayesianRidge', 'ElasticNet', 'ElasticNet...\n",
      "\n",
      "FILE\n",
      "    c:\\project\\bigdata_cert\\.venv\\lib\\site-packages\\sklearn\\linear_model\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "help(linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a5317f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_rmse = 6637.045965650744\n",
      "elastic net rmse = 12928.914036912636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "lr_rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f'lr_rmse = {lr_rmse}')\n",
    "\n",
    "elastic = ElasticNet()\n",
    "elastic.fit(X_train, y_train)\n",
    "y_pred = elastic.predict(X_test)\n",
    "elastic_rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f'elastic net rmse = {elastic_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "102027b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.ensemble in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.ensemble - Ensemble-based methods for classification, regression and anomaly detection.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _bagging\n",
      "    _base\n",
      "    _forest\n",
      "    _gb\n",
      "    _gradient_boosting\n",
      "    _hist_gradient_boosting (package)\n",
      "    _iforest\n",
      "    _stacking\n",
      "    _voting\n",
      "    _weight_boosting\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(sklearn.utils._repr_html.base.ReprHTMLMixin, sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin, sklearn.utils._metadata_requests._MetadataRequester)\n",
      "        sklearn.ensemble._base.BaseEnsemble(sklearn.base.MetaEstimatorMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.ClassifierMixin(builtins.object)\n",
      "        sklearn.ensemble._bagging.BaggingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._bagging.BaseBagging)\n",
      "        sklearn.ensemble._gb.GradientBoostingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._gb.BaseGradientBoosting)\n",
      "        sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting)\n",
      "        sklearn.ensemble._stacking.StackingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._stacking._BaseStacking)\n",
      "        sklearn.ensemble._voting.VotingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._voting._BaseVoting)\n",
      "        sklearn.ensemble._weight_boosting.AdaBoostClassifier(sklearn.utils._metadata_requests._RoutingNotSupportedMixin, sklearn.base.ClassifierMixin, sklearn.ensemble._weight_boosting.BaseWeightBoosting)\n",
      "    sklearn.base.MetaEstimatorMixin(builtins.object)\n",
      "        sklearn.ensemble._base.BaseEnsemble(sklearn.base.MetaEstimatorMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.OutlierMixin(builtins.object)\n",
      "        sklearn.ensemble._iforest.IsolationForest(sklearn.base.OutlierMixin, sklearn.ensemble._bagging.BaseBagging)\n",
      "    sklearn.base.RegressorMixin(builtins.object)\n",
      "        sklearn.ensemble._bagging.BaggingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._bagging.BaseBagging)\n",
      "        sklearn.ensemble._gb.GradientBoostingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._gb.BaseGradientBoosting)\n",
      "        sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting)\n",
      "        sklearn.ensemble._stacking.StackingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._stacking._BaseStacking)\n",
      "        sklearn.ensemble._voting.VotingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._voting._BaseVoting)\n",
      "        sklearn.ensemble._weight_boosting.AdaBoostRegressor(sklearn.utils._metadata_requests._RoutingNotSupportedMixin, sklearn.base.RegressorMixin, sklearn.ensemble._weight_boosting.BaseWeightBoosting)\n",
      "    sklearn.base.TransformerMixin(sklearn.utils._set_output._SetOutputMixin)\n",
      "        sklearn.ensemble._forest.RandomTreesEmbedding(sklearn.base.TransformerMixin, sklearn.ensemble._forest.BaseForest)\n",
      "    sklearn.ensemble._bagging.BaseBagging(sklearn.ensemble._base.BaseEnsemble)\n",
      "        sklearn.ensemble._bagging.BaggingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._bagging.BaseBagging)\n",
      "        sklearn.ensemble._bagging.BaggingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._bagging.BaseBagging)\n",
      "        sklearn.ensemble._iforest.IsolationForest(sklearn.base.OutlierMixin, sklearn.ensemble._bagging.BaseBagging)\n",
      "    sklearn.ensemble._forest.BaseForest(sklearn.base.MultiOutputMixin, sklearn.ensemble._base.BaseEnsemble)\n",
      "        sklearn.ensemble._forest.RandomTreesEmbedding(sklearn.base.TransformerMixin, sklearn.ensemble._forest.BaseForest)\n",
      "    sklearn.ensemble._forest.ForestClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._forest.BaseForest)\n",
      "        sklearn.ensemble._forest.ExtraTreesClassifier\n",
      "        sklearn.ensemble._forest.RandomForestClassifier\n",
      "    sklearn.ensemble._forest.ForestRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._forest.BaseForest)\n",
      "        sklearn.ensemble._forest.ExtraTreesRegressor\n",
      "        sklearn.ensemble._forest.RandomForestRegressor\n",
      "    sklearn.ensemble._gb.BaseGradientBoosting(sklearn.ensemble._base.BaseEnsemble)\n",
      "        sklearn.ensemble._gb.GradientBoostingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._gb.BaseGradientBoosting)\n",
      "        sklearn.ensemble._gb.GradientBoostingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._gb.BaseGradientBoosting)\n",
      "    sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting(sklearn.base.BaseEstimator, abc.ABC)\n",
      "        sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting)\n",
      "        sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting)\n",
      "    sklearn.ensemble._stacking._BaseStacking(sklearn.base.TransformerMixin, sklearn.ensemble._base._BaseHeterogeneousEnsemble)\n",
      "        sklearn.ensemble._stacking.StackingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._stacking._BaseStacking)\n",
      "        sklearn.ensemble._stacking.StackingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._stacking._BaseStacking)\n",
      "    sklearn.ensemble._voting._BaseVoting(sklearn.base.TransformerMixin, sklearn.ensemble._base._BaseHeterogeneousEnsemble)\n",
      "        sklearn.ensemble._voting.VotingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._voting._BaseVoting)\n",
      "        sklearn.ensemble._voting.VotingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._voting._BaseVoting)\n",
      "    sklearn.ensemble._weight_boosting.BaseWeightBoosting(sklearn.ensemble._base.BaseEnsemble)\n",
      "        sklearn.ensemble._weight_boosting.AdaBoostClassifier(sklearn.utils._metadata_requests._RoutingNotSupportedMixin, sklearn.base.ClassifierMixin, sklearn.ensemble._weight_boosting.BaseWeightBoosting)\n",
      "        sklearn.ensemble._weight_boosting.AdaBoostRegressor(sklearn.utils._metadata_requests._RoutingNotSupportedMixin, sklearn.base.RegressorMixin, sklearn.ensemble._weight_boosting.BaseWeightBoosting)\n",
      "    sklearn.utils._metadata_requests._RoutingNotSupportedMixin(builtins.object)\n",
      "        sklearn.ensemble._weight_boosting.AdaBoostClassifier(sklearn.utils._metadata_requests._RoutingNotSupportedMixin, sklearn.base.ClassifierMixin, sklearn.ensemble._weight_boosting.BaseWeightBoosting)\n",
      "        sklearn.ensemble._weight_boosting.AdaBoostRegressor(sklearn.utils._metadata_requests._RoutingNotSupportedMixin, sklearn.base.RegressorMixin, sklearn.ensemble._weight_boosting.BaseWeightBoosting)\n",
      "    \n",
      "    class AdaBoostClassifier(sklearn.utils._metadata_requests._RoutingNotSupportedMixin, sklearn.base.ClassifierMixin, BaseWeightBoosting)\n",
      "     |  AdaBoostClassifier(estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='deprecated', random_state=None)\n",
      "     |  \n",
      "     |  An AdaBoost classifier.\n",
      "     |  \n",
      "     |  An AdaBoost [1]_ classifier is a meta-estimator that begins by fitting a\n",
      "     |  classifier on the original dataset and then fits additional copies of the\n",
      "     |  classifier on the same dataset but where the weights of incorrectly\n",
      "     |  classified instances are adjusted such that subsequent classifiers focus\n",
      "     |  more on difficult cases.\n",
      "     |  \n",
      "     |  This class implements the algorithm based on [2]_.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <adaboost>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.14\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : object, default=None\n",
      "     |      The base estimator from which the boosted ensemble is built.\n",
      "     |      Support for sample weighting is required, as well as proper\n",
      "     |      ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n",
      "     |      the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n",
      "     |      initialized with `max_depth=1`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |         `base_estimator` was renamed to `estimator`.\n",
      "     |  \n",
      "     |  n_estimators : int, default=50\n",
      "     |      The maximum number of estimators at which boosting is terminated.\n",
      "     |      In case of perfect fit, the learning procedure is stopped early.\n",
      "     |      Values must be in the range `[1, inf)`.\n",
      "     |  \n",
      "     |  learning_rate : float, default=1.0\n",
      "     |      Weight applied to each classifier at each boosting iteration. A higher\n",
      "     |      learning rate increases the contribution of each classifier. There is\n",
      "     |      a trade-off between the `learning_rate` and `n_estimators` parameters.\n",
      "     |      Values must be in the range `(0.0, inf)`.\n",
      "     |  \n",
      "     |  algorithm : {'SAMME'}, default='SAMME'\n",
      "     |      Use the SAMME discrete boosting algorithm.\n",
      "     |  \n",
      "     |      .. deprecated:: 1.6\n",
      "     |          `algorithm` is deprecated and will be removed in version 1.8. This\n",
      "     |          estimator only implements the 'SAMME' algorithm.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the random seed given at each `estimator` at each\n",
      "     |      boosting iteration.\n",
      "     |      Thus, it is only used when `estimator` exposes a `random_state`.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : estimator\n",
      "     |      The base estimator from which the ensemble is grown.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |         `base_estimator_` was renamed to `estimator_`.\n",
      "     |  \n",
      "     |  estimators_ : list of classifiers\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  n_classes_ : int\n",
      "     |      The number of classes.\n",
      "     |  \n",
      "     |  estimator_weights_ : ndarray of floats\n",
      "     |      Weights for each estimator in the boosted ensemble.\n",
      "     |  \n",
      "     |  estimator_errors_ : ndarray of floats\n",
      "     |      Classification error for each estimator in the boosted\n",
      "     |      ensemble.\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances if supported by the\n",
      "     |      ``estimator`` (when based on decision trees).\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  AdaBoostRegressor : An AdaBoost regressor that begins by fitting a\n",
      "     |      regressor on the original dataset and then fits additional copies of\n",
      "     |      the regressor on the same dataset but where the weights of instances\n",
      "     |      are adjusted according to the error of the current prediction.\n",
      "     |  \n",
      "     |  GradientBoostingClassifier : GB builds an additive model in a forward\n",
      "     |      stage-wise fashion. Regression trees are fit on the negative gradient\n",
      "     |      of the binomial or multinomial deviance loss function. Binary\n",
      "     |      classification is a special case where only a single regression tree is\n",
      "     |      induced.\n",
      "     |  \n",
      "     |  sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning\n",
      "     |      method used for classification.\n",
      "     |      Creates a model that predicts the value of a target variable by\n",
      "     |      learning simple decision rules inferred from the data features.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      "     |         on-Line Learning and an Application to Boosting\", 1995.\n",
      "     |  \n",
      "     |  .. [2] :doi:`J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class adaboost.\"\n",
      "     |         Statistics and its Interface 2.3 (2009): 349-360.\n",
      "     |         <10.4310/SII.2009.v2.n3.a8>`\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import AdaBoostClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      "     |  ...                            n_informative=2, n_redundant=0,\n",
      "     |  ...                            random_state=0, shuffle=False)\n",
      "     |  >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  AdaBoostClassifier(n_estimators=100, random_state=0)\n",
      "     |  >>> clf.predict([[0, 0, 0, 0]])\n",
      "     |  array([1])\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.96\n",
      "     |  \n",
      "     |  For a detailed example of using AdaBoost to fit a sequence of DecisionTrees\n",
      "     |  as weaklearners, please refer to\n",
      "     |  :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_multiclass.py`.\n",
      "     |  \n",
      "     |  For a detailed example of using AdaBoost to fit a non-linearly separable\n",
      "     |  classification dataset composed of two Gaussian quantiles clusters, please\n",
      "     |  refer to :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_twoclass.py`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AdaBoostClassifier\n",
      "     |      sklearn.utils._metadata_requests._RoutingNotSupportedMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseWeightBoosting\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='deprecated', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Compute the decision function of ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : ndarray of shape of (n_samples, k)\n",
      "     |          The decision function of the input samples. The order of\n",
      "     |          outputs is the same as that of the :term:`classes_` attribute.\n",
      "     |          Binary classification is a special cases with ``k == 1``,\n",
      "     |          otherwise ``k==n_classes``. For binary classification,\n",
      "     |          values closer to -1 or 1 mean more like the first or second\n",
      "     |          class in ``classes_``, respectively.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict classes for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is computed as the weighted mean\n",
      "     |      prediction of the classifiers in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the weighted mean predicted class log-probabilities of the classifiers\n",
      "     |      in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes)\n",
      "     |          The class probabilities of the input samples. The order of\n",
      "     |          outputs is the same of that of the :term:`classes_` attribute.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample is computed as\n",
      "     |      the weighted mean predicted class probabilities of the classifiers\n",
      "     |      in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes)\n",
      "     |          The class probabilities of the input samples. The order of\n",
      "     |          outputs is the same of that of the :term:`classes_` attribute.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.ensemble._weight_boosting.AdaBoostClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._weight_boosting.AdaBoostClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._weight_boosting.AdaBoostClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._weight_boosting.AdaBoostClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  staged_decision_function(self, X)\n",
      "     |      Compute decision function of ``X`` for each boosting iteration.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each boosting iteration.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      score : generator of ndarray of shape (n_samples, k)\n",
      "     |          The decision function of the input samples. The order of\n",
      "     |          outputs is the same of that of the :term:`classes_` attribute.\n",
      "     |          Binary classification is a special cases with ``k == 1``,\n",
      "     |          otherwise ``k==n_classes``. For binary classification,\n",
      "     |          values closer to -1 or 1 mean more like the first or second\n",
      "     |          class in ``classes_``, respectively.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Return staged predictions for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is computed as the weighted mean\n",
      "     |      prediction of the classifiers in the ensemble.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble prediction after each\n",
      "     |      iteration of boosting and therefore allows monitoring, such as to\n",
      "     |      determine the prediction on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      y : generator of ndarray of shape (n_samples,)\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  staged_predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample is computed as\n",
      "     |      the weighted mean predicted class probabilities of the classifiers\n",
      "     |      in the ensemble.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble predicted class probabilities\n",
      "     |      after each iteration of boosting and therefore allows monitoring, such\n",
      "     |      as to determine the predicted class probabilities on a test set after\n",
      "     |      each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      p : generator of ndarray of shape (n_samples,)\n",
      "     |          The class probabilities of the input samples. The order of\n",
      "     |          outputs is the same of that of the :term:`classes_` attribute.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._RoutingNotSupportedMixin:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Raise `NotImplementedError`.\n",
      "     |      \n",
      "     |      This estimator does not support metadata routing yet.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._metadata_requests._RoutingNotSupportedMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a boosted classifier/regressor from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, the sample weights are initialized to\n",
      "     |          1 / n_samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  staged_score(self, X, y, sample_weight=None)\n",
      "     |      Return staged scores for X, y.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble score after each iteration of\n",
      "     |      boosting and therefore allows monitoring, such as to determine the\n",
      "     |      score on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      z : float\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The feature importances.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class AdaBoostRegressor(sklearn.utils._metadata_requests._RoutingNotSupportedMixin, sklearn.base.RegressorMixin, BaseWeightBoosting)\n",
      "     |  AdaBoostRegressor(estimator=None, *, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
      "     |  \n",
      "     |  An AdaBoost regressor.\n",
      "     |  \n",
      "     |  An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n",
      "     |  regressor on the original dataset and then fits additional copies of the\n",
      "     |  regressor on the same dataset but where the weights of instances are\n",
      "     |  adjusted according to the error of the current prediction. As such,\n",
      "     |  subsequent regressors focus more on difficult cases.\n",
      "     |  \n",
      "     |  This class implements the algorithm known as AdaBoost.R2 [2].\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <adaboost>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.14\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : object, default=None\n",
      "     |      The base estimator from which the boosted ensemble is built.\n",
      "     |      If ``None``, then the base estimator is\n",
      "     |      :class:`~sklearn.tree.DecisionTreeRegressor` initialized with\n",
      "     |      `max_depth=3`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |         `base_estimator` was renamed to `estimator`.\n",
      "     |  \n",
      "     |  n_estimators : int, default=50\n",
      "     |      The maximum number of estimators at which boosting is terminated.\n",
      "     |      In case of perfect fit, the learning procedure is stopped early.\n",
      "     |      Values must be in the range `[1, inf)`.\n",
      "     |  \n",
      "     |  learning_rate : float, default=1.0\n",
      "     |      Weight applied to each regressor at each boosting iteration. A higher\n",
      "     |      learning rate increases the contribution of each regressor. There is\n",
      "     |      a trade-off between the `learning_rate` and `n_estimators` parameters.\n",
      "     |      Values must be in the range `(0.0, inf)`.\n",
      "     |  \n",
      "     |  loss : {'linear', 'square', 'exponential'}, default='linear'\n",
      "     |      The loss function to use when updating the weights after each\n",
      "     |      boosting iteration.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the random seed given at each `estimator` at each\n",
      "     |      boosting iteration.\n",
      "     |      Thus, it is only used when `estimator` exposes a `random_state`.\n",
      "     |      In addition, it controls the bootstrap of the weights used to train the\n",
      "     |      `estimator` at each boosting iteration.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : estimator\n",
      "     |      The base estimator from which the ensemble is grown.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |         `base_estimator_` was renamed to `estimator_`.\n",
      "     |  \n",
      "     |  estimators_ : list of regressors\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  estimator_weights_ : ndarray of floats\n",
      "     |      Weights for each estimator in the boosted ensemble.\n",
      "     |  \n",
      "     |  estimator_errors_ : ndarray of floats\n",
      "     |      Regression error for each estimator in the boosted ensemble.\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances if supported by the\n",
      "     |      ``estimator`` (when based on decision trees).\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  AdaBoostClassifier : An AdaBoost classifier.\n",
      "     |  GradientBoostingRegressor : Gradient Boosting Classification Tree.\n",
      "     |  sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      "     |         on-Line Learning and an Application to Boosting\", 1995.\n",
      "     |  \n",
      "     |  .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import AdaBoostRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      "     |  ...                        random_state=0, shuffle=False)\n",
      "     |  >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  AdaBoostRegressor(n_estimators=100, random_state=0)\n",
      "     |  >>> regr.predict([[0, 0, 0, 0]])\n",
      "     |  array([4.7972])\n",
      "     |  >>> regr.score(X, y)\n",
      "     |  0.9771\n",
      "     |  \n",
      "     |  For a detailed example of utilizing :class:`~sklearn.ensemble.AdaBoostRegressor`\n",
      "     |  to fit a sequence of decision trees as weak learners, please refer to\n",
      "     |  :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_regression.py`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AdaBoostRegressor\n",
      "     |      sklearn.utils._metadata_requests._RoutingNotSupportedMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseWeightBoosting\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator=None, *, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression value for X.\n",
      "     |      \n",
      "     |      The predicted regression value of an input sample is computed\n",
      "     |      as the weighted median prediction of the regressors in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          The predicted regression values.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.ensemble._weight_boosting.AdaBoostRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._weight_boosting.AdaBoostRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._weight_boosting.AdaBoostRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._weight_boosting.AdaBoostRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Return staged predictions for X.\n",
      "     |      \n",
      "     |      The predicted regression value of an input sample is computed\n",
      "     |      as the weighted median prediction of the regressors in the ensemble.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble prediction after each\n",
      "     |      iteration of boosting and therefore allows monitoring, such as to\n",
      "     |      determine the prediction on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      y : generator of ndarray of shape (n_samples,)\n",
      "     |          The predicted regression values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._RoutingNotSupportedMixin:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Raise `NotImplementedError`.\n",
      "     |      \n",
      "     |      This estimator does not support metadata routing yet.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._metadata_requests._RoutingNotSupportedMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a boosted classifier/regressor from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, the sample weights are initialized to\n",
      "     |          1 / n_samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  staged_score(self, X, y, sample_weight=None)\n",
      "     |      Return staged scores for X, y.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble score after each iteration of\n",
      "     |      boosting and therefore allows monitoring, such as to determine the\n",
      "     |      score on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      z : float\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The feature importances.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class BaggingClassifier(sklearn.base.ClassifierMixin, BaseBagging)\n",
      "     |  BaggingClassifier(estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      "     |  \n",
      "     |  A Bagging classifier.\n",
      "     |  \n",
      "     |  A Bagging classifier is an ensemble meta-estimator that fits base\n",
      "     |  classifiers each on random subsets of the original dataset and then\n",
      "     |  aggregate their individual predictions (either by voting or by averaging)\n",
      "     |  to form a final prediction. Such a meta-estimator can typically be used as\n",
      "     |  a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      "     |  tree), by introducing randomization into its construction procedure and\n",
      "     |  then making an ensemble out of it.\n",
      "     |  \n",
      "     |  This algorithm encompasses several works from the literature. When random\n",
      "     |  subsets of the dataset are drawn as random subsets of the samples, then\n",
      "     |  this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      "     |  replacement, then the method is known as Bagging [2]_. When random subsets\n",
      "     |  of the dataset are drawn as random subsets of the features, then the method\n",
      "     |  is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      "     |  on subsets of both samples and features, then the method is known as\n",
      "     |  Random Patches [4]_.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bagging>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.15\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : object, default=None\n",
      "     |      The base estimator to fit on random subsets of the dataset.\n",
      "     |      If None, then the base estimator is a\n",
      "     |      :class:`~sklearn.tree.DecisionTreeClassifier`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |         `base_estimator` was renamed to `estimator`.\n",
      "     |  \n",
      "     |  n_estimators : int, default=10\n",
      "     |      The number of base estimators in the ensemble.\n",
      "     |  \n",
      "     |  max_samples : int or float, default=1.0\n",
      "     |      The number of samples to draw from X to train each base estimator (with\n",
      "     |      replacement by default, see `bootstrap` for more details).\n",
      "     |  \n",
      "     |      - If int, then draw `max_samples` samples.\n",
      "     |      - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "     |  \n",
      "     |  max_features : int or float, default=1.0\n",
      "     |      The number of features to draw from X to train each base estimator (\n",
      "     |      without replacement by default, see `bootstrap_features` for more\n",
      "     |      details).\n",
      "     |  \n",
      "     |      - If int, then draw `max_features` features.\n",
      "     |      - If float, then draw `max(1, int(max_features * n_features_in_))` features.\n",
      "     |  \n",
      "     |  bootstrap : bool, default=True\n",
      "     |      Whether samples are drawn with replacement. If False, sampling\n",
      "     |      without replacement is performed.\n",
      "     |  \n",
      "     |  bootstrap_features : bool, default=False\n",
      "     |      Whether features are drawn with replacement.\n",
      "     |  \n",
      "     |  oob_score : bool, default=False\n",
      "     |      Whether to use out-of-bag samples to estimate\n",
      "     |      the generalization error. Only available if bootstrap=True.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit\n",
      "     |      a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *warm_start* constructor parameter.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel for both :meth:`fit` and\n",
      "     |      :meth:`predict`. ``None`` means 1 unless in a\n",
      "     |      :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      "     |      processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the random resampling of the original dataset\n",
      "     |      (sample wise and feature wise).\n",
      "     |      If the base estimator accepts a `random_state` attribute, a different\n",
      "     |      seed is generated for each instance in the ensemble.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : estimator\n",
      "     |      The base estimator from which the ensemble is grown.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |         `base_estimator_` was renamed to `estimator_`.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  estimators_ : list of estimators\n",
      "     |      The collection of fitted base estimators.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of arrays\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator. Each subset is defined by an array of the indices selected.\n",
      "     |  \n",
      "     |  estimators_features_ : list of arrays\n",
      "     |      The subset of drawn features for each base estimator.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  n_classes_ : int or list\n",
      "     |      The number of classes.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n",
      "     |      Decision function computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      "     |      only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  BaggingRegressor : A Bagging regressor.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      "     |         databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      "     |  \n",
      "     |  .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      "     |         1996.\n",
      "     |  \n",
      "     |  .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      "     |         forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      "     |         1998.\n",
      "     |  \n",
      "     |  .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      "     |         Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> from sklearn.ensemble import BaggingClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> X, y = make_classification(n_samples=100, n_features=4,\n",
      "     |  ...                            n_informative=2, n_redundant=0,\n",
      "     |  ...                            random_state=0, shuffle=False)\n",
      "     |  >>> clf = BaggingClassifier(estimator=SVC(),\n",
      "     |  ...                         n_estimators=10, random_state=0).fit(X, y)\n",
      "     |  >>> clf.predict([[0, 0, 0, 0]])\n",
      "     |  array([1])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BaggingClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseBagging\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X, **params)\n",
      "     |      Average of the decision functions of the base classifiers.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      **params : dict\n",
      "     |          Parameters routed to the `decision_function` method of the sub-estimators\n",
      "     |          via the metadata routing API.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |              Only available if\n",
      "     |              `sklearn.set_config(enable_metadata_routing=True)` is set. See\n",
      "     |              :ref:`Metadata Routing User Guide <metadata_routing>` for more\n",
      "     |              details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : ndarray of shape (n_samples, k)\n",
      "     |          The decision function of the input samples. The columns correspond\n",
      "     |          to the classes in sorted order, as they appear in the attribute\n",
      "     |          ``classes_``. Regression and binary classification are special\n",
      "     |          cases with ``k == 1``, otherwise ``k==n_classes``.\n",
      "     |  \n",
      "     |  predict(self, X, **params)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is computed as the class with\n",
      "     |      the highest mean predicted probability. If base estimators do not\n",
      "     |      implement a ``predict_proba`` method, then it resorts to voting.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      **params : dict\n",
      "     |          Parameters routed to the `predict_proba` (if available) or the `predict`\n",
      "     |          method (otherwise) of the sub-estimators via the metadata routing API.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |              Only available if\n",
      "     |              `sklearn.set_config(enable_metadata_routing=True)` is set. See\n",
      "     |              :ref:`Metadata Routing User Guide <metadata_routing>` for more\n",
      "     |              details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X, **params)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the log of the mean predicted class probabilities of the base\n",
      "     |      estimators in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      **params : dict\n",
      "     |          Parameters routed to the `predict_log_proba`, the `predict_proba` or the\n",
      "     |          `proba` method of the sub-estimators via the metadata routing API. The\n",
      "     |          routing is tried in the mentioned order depending on whether this method is\n",
      "     |          available on the sub-estimator.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |              Only available if\n",
      "     |              `sklearn.set_config(enable_metadata_routing=True)` is set. See\n",
      "     |              :ref:`Metadata Routing User Guide <metadata_routing>` for more\n",
      "     |              details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes)\n",
      "     |          The class log-probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X, **params)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample is computed as\n",
      "     |      the mean predicted class probabilities of the base estimators in the\n",
      "     |      ensemble. If base estimators do not implement a ``predict_proba``\n",
      "     |      method, then it resorts to voting and the predicted class probabilities\n",
      "     |      of an input sample represents the proportion of estimators predicting\n",
      "     |      each class.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      **params : dict\n",
      "     |          Parameters routed to the `predict_proba` (if available) or the `predict`\n",
      "     |          method (otherwise) of the sub-estimators via the metadata routing API.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |              Only available if\n",
      "     |              `sklearn.set_config(enable_metadata_routing=True)` is set. See\n",
      "     |              :ref:`Metadata Routing User Guide <metadata_routing>` for more\n",
      "     |              details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes)\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.ensemble._bagging.BaggingClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._bagging.BaggingClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._bagging.BaggingClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._bagging.BaggingClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseBagging:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, **fit_params)\n",
      "     |      Build a Bagging ensemble of estimators from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |          Note that this is supported only if the base estimator supports\n",
      "     |          sample weighting.\n",
      "     |      **fit_params : dict\n",
      "     |          Parameters to pass to the underlying estimators.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.5\n",
      "     |      \n",
      "     |              Only available if `enable_metadata_routing=True`,\n",
      "     |              which can be set by using\n",
      "     |              ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseBagging:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of indices identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class BaggingRegressor(sklearn.base.RegressorMixin, BaseBagging)\n",
      "     |  BaggingRegressor(estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      "     |  \n",
      "     |  A Bagging regressor.\n",
      "     |  \n",
      "     |  A Bagging regressor is an ensemble meta-estimator that fits base\n",
      "     |  regressors each on random subsets of the original dataset and then\n",
      "     |  aggregate their individual predictions (either by voting or by averaging)\n",
      "     |  to form a final prediction. Such a meta-estimator can typically be used as\n",
      "     |  a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      "     |  tree), by introducing randomization into its construction procedure and\n",
      "     |  then making an ensemble out of it.\n",
      "     |  \n",
      "     |  This algorithm encompasses several works from the literature. When random\n",
      "     |  subsets of the dataset are drawn as random subsets of the samples, then\n",
      "     |  this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      "     |  replacement, then the method is known as Bagging [2]_. When random subsets\n",
      "     |  of the dataset are drawn as random subsets of the features, then the method\n",
      "     |  is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      "     |  on subsets of both samples and features, then the method is known as\n",
      "     |  Random Patches [4]_.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bagging>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.15\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : object, default=None\n",
      "     |      The base estimator to fit on random subsets of the dataset.\n",
      "     |      If None, then the base estimator is a\n",
      "     |      :class:`~sklearn.tree.DecisionTreeRegressor`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |         `base_estimator` was renamed to `estimator`.\n",
      "     |  \n",
      "     |  n_estimators : int, default=10\n",
      "     |      The number of base estimators in the ensemble.\n",
      "     |  \n",
      "     |  max_samples : int or float, default=1.0\n",
      "     |      The number of samples to draw from X to train each base estimator (with\n",
      "     |      replacement by default, see `bootstrap` for more details).\n",
      "     |  \n",
      "     |      - If int, then draw `max_samples` samples.\n",
      "     |      - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "     |  \n",
      "     |  max_features : int or float, default=1.0\n",
      "     |      The number of features to draw from X to train each base estimator (\n",
      "     |      without replacement by default, see `bootstrap_features` for more\n",
      "     |      details).\n",
      "     |  \n",
      "     |      - If int, then draw `max_features` features.\n",
      "     |      - If float, then draw `max(1, int(max_features * n_features_in_))` features.\n",
      "     |  \n",
      "     |  bootstrap : bool, default=True\n",
      "     |      Whether samples are drawn with replacement. If False, sampling\n",
      "     |      without replacement is performed.\n",
      "     |  \n",
      "     |  bootstrap_features : bool, default=False\n",
      "     |      Whether features are drawn with replacement.\n",
      "     |  \n",
      "     |  oob_score : bool, default=False\n",
      "     |      Whether to use out-of-bag samples to estimate\n",
      "     |      the generalization error. Only available if bootstrap=True.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit\n",
      "     |      a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel for both :meth:`fit` and\n",
      "     |      :meth:`predict`. ``None`` means 1 unless in a\n",
      "     |      :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      "     |      processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the random resampling of the original dataset\n",
      "     |      (sample wise and feature wise).\n",
      "     |      If the base estimator accepts a `random_state` attribute, a different\n",
      "     |      seed is generated for each instance in the ensemble.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : estimator\n",
      "     |      The base estimator from which the ensemble is grown.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |         `base_estimator_` was renamed to `estimator_`.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  estimators_ : list of estimators\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of arrays\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator. Each subset is defined by an array of the indices selected.\n",
      "     |  \n",
      "     |  estimators_features_ : list of arrays\n",
      "     |      The subset of drawn features for each base estimator.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  oob_prediction_ : ndarray of shape (n_samples,)\n",
      "     |      Prediction computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_prediction_` might contain NaN. This attribute exists only\n",
      "     |      when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  BaggingClassifier : A Bagging classifier.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      "     |         databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      "     |  \n",
      "     |  .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      "     |         1996.\n",
      "     |  \n",
      "     |  .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      "     |         forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      "     |         1998.\n",
      "     |  \n",
      "     |  .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      "     |         Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.svm import SVR\n",
      "     |  >>> from sklearn.ensemble import BaggingRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_samples=100, n_features=4,\n",
      "     |  ...                        n_informative=2, n_targets=1,\n",
      "     |  ...                        random_state=0, shuffle=False)\n",
      "     |  >>> regr = BaggingRegressor(estimator=SVR(),\n",
      "     |  ...                         n_estimators=10, random_state=0).fit(X, y)\n",
      "     |  >>> regr.predict([[0, 0, 0, 0]])\n",
      "     |  array([-2.8720])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BaggingRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseBagging\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  predict(self, X, **params)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      The predicted regression target of an input sample is computed as the\n",
      "     |      mean predicted regression targets of the estimators in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      **params : dict\n",
      "     |          Parameters routed to the `predict` method of the sub-estimators via the\n",
      "     |          metadata routing API.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |              Only available if\n",
      "     |              `sklearn.set_config(enable_metadata_routing=True)` is set. See\n",
      "     |              :ref:`Metadata Routing User Guide <metadata_routing>` for more\n",
      "     |              details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.ensemble._bagging.BaggingRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._bagging.BaggingRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._bagging.BaggingRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._bagging.BaggingRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseBagging:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, **fit_params)\n",
      "     |      Build a Bagging ensemble of estimators from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |          Note that this is supported only if the base estimator supports\n",
      "     |          sample weighting.\n",
      "     |      **fit_params : dict\n",
      "     |          Parameters to pass to the underlying estimators.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.5\n",
      "     |      \n",
      "     |              Only available if `enable_metadata_routing=True`,\n",
      "     |              which can be set by using\n",
      "     |              ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseBagging:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of indices identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class BaseEnsemble(sklearn.base.MetaEstimatorMixin, sklearn.base.BaseEstimator)\n",
      "     |  BaseEnsemble(estimator=None, *, n_estimators=10, estimator_params=())\n",
      "     |  \n",
      "     |  Base class for all ensemble classes.\n",
      "     |  \n",
      "     |  Warning: This class should not be used directly. Use derived classes\n",
      "     |  instead.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : object\n",
      "     |      The base estimator from which the ensemble is built.\n",
      "     |  \n",
      "     |  n_estimators : int, default=10\n",
      "     |      The number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  estimator_params : list of str, default=tuple()\n",
      "     |      The list of attributes to use as parameters when instantiating a\n",
      "     |      new base estimator. If none are given, default parameters are used.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : estimator\n",
      "     |      The base estimator from which the ensemble is grown.\n",
      "     |  \n",
      "     |  estimators_ : list of estimators\n",
      "     |      The collection of fitted base estimators.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __init__(self, estimator=None, *, n_estimators=10, estimator_params=())\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__init__'})\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class ExtraTreesClassifier(ForestClassifier)\n",
      "     |  ExtraTreesClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
      "     |  \n",
      "     |  An extra-trees classifier.\n",
      "     |  \n",
      "     |  This class implements a meta estimator that fits a number of\n",
      "     |  randomized decision trees (a.k.a. extra-trees) on various sub-samples\n",
      "     |  of the dataset and uses averaging to improve the predictive accuracy\n",
      "     |  and control over-fitting.\n",
      "     |  \n",
      "     |  This estimator has native support for missing values (NaNs) for\n",
      "     |  random splits. During training, a random threshold will be chosen\n",
      "     |  to split the non-missing values on. Then the non-missing values will be sent\n",
      "     |  to the left and right child based on the randomly selected threshold, while\n",
      "     |  the missing values will also be randomly sent to the left or right child.\n",
      "     |  This is repeated for every feature considered at each split. The best split\n",
      "     |  among these is chosen.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : int, default=100\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``n_estimators`` changed from 10 to 100\n",
      "     |         in 0.22.\n",
      "     |  \n",
      "     |  criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
      "     |      The function to measure the quality of a split. Supported criteria are\n",
      "     |      \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
      "     |      Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
      "     |      Note: This parameter is tree-specific.\n",
      "     |  \n",
      "     |  max_depth : int, default=None\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `max(1, int(max_features * n_features_in_))` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.1\n",
      "     |          The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  bootstrap : bool, default=False\n",
      "     |      Whether bootstrap samples are used when building trees. If False, the\n",
      "     |      whole dataset is used to build each tree.\n",
      "     |  \n",
      "     |  oob_score : bool or callable, default=False\n",
      "     |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      "     |      By default, :func:`~sklearn.metrics.accuracy_score` is used.\n",
      "     |      Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
      "     |      custom metric. Only available if `bootstrap=True`.\n",
      "     |  \n",
      "     |      For an illustration of out-of-bag (OOB) error estimation, see the example\n",
      "     |      :ref:`sphx_glr_auto_examples_ensemble_plot_ensemble_oob.py`.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      "     |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      "     |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "     |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      "     |      <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls 3 sources of randomness:\n",
      "     |  \n",
      "     |      - the bootstrapping of the samples used when building trees\n",
      "     |        (if ``bootstrap=True``)\n",
      "     |      - the sampling of the features to consider when looking for the best\n",
      "     |        split at each node (if ``max_features < n_features``)\n",
      "     |      - the draw of the splits for each of the `max_features`\n",
      "     |  \n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`Glossary <warm_start>` and\n",
      "     |      :ref:`tree_ensemble_warm_start` for details.\n",
      "     |  \n",
      "     |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one. For\n",
      "     |      multi-output problems, a list of dicts can be provided in the same\n",
      "     |      order as the columns of y.\n",
      "     |  \n",
      "     |      Note that for multioutput (including multilabel) weights should be\n",
      "     |      defined for each class of every column in its own dict. For example,\n",
      "     |      for four-class multilabel classification weights should be\n",
      "     |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "     |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      "     |      weights are computed based on the bootstrap sample for every tree\n",
      "     |      grown.\n",
      "     |  \n",
      "     |      For multi-output, the weights of each column of y will be multiplied.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "     |      :ref:`minimal_cost_complexity_pruning` for details. See\n",
      "     |      :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n",
      "     |      for an example of such pruning.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  max_samples : int or float, default=None\n",
      "     |      If bootstrap is True, the number of samples to draw from X\n",
      "     |      to train each base estimator.\n",
      "     |  \n",
      "     |      - If None (default), then draw `X.shape[0]` samples.\n",
      "     |      - If int, then draw `max_samples` samples.\n",
      "     |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      "     |        `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  monotonic_cst : array-like of int of shape (n_features), default=None\n",
      "     |      Indicates the monotonicity constraint to enforce on each feature.\n",
      "     |        - 1: monotonically increasing\n",
      "     |        - 0: no constraint\n",
      "     |        - -1: monotonically decreasing\n",
      "     |  \n",
      "     |      If monotonic_cst is None, no constraints are applied.\n",
      "     |  \n",
      "     |      Monotonicity constraints are not supported for:\n",
      "     |        - multiclass classifications (i.e. when `n_classes > 2`),\n",
      "     |        - multioutput classifications (i.e. when `n_outputs_ > 1`),\n",
      "     |        - classifications trained on data with missing values.\n",
      "     |  \n",
      "     |      The constraints hold over the probability of the positive class.\n",
      "     |  \n",
      "     |      Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : :class:`~sklearn.tree.ExtraTreeClassifier`\n",
      "     |      The child estimator template used to create the collection of fitted\n",
      "     |      sub-estimators.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |         `base_estimator_` was renamed to `estimator_`.\n",
      "     |  \n",
      "     |  estimators_ : list of DecisionTreeClassifier\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      "     |      The classes labels (single output problem), or a list of arrays of\n",
      "     |      class labels (multi-output problem).\n",
      "     |  \n",
      "     |  n_classes_ : int or list\n",
      "     |      The number of classes (single output problem), or a list containing the\n",
      "     |      number of classes for each output (multi-output problem).\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
      "     |      Decision function computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      "     |      only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of arrays\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator. Each subset is defined by an array of the indices selected.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  ExtraTreesRegressor : An extra-trees regressor with random splits.\n",
      "     |  RandomForestClassifier : A random forest classifier with optimal splits.\n",
      "     |  RandomForestRegressor : Ensemble regressor using trees with optimal splits.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      "     |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import ExtraTreesClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
      "     |  >>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  ExtraTreesClassifier(random_state=0)\n",
      "     |  >>> clf.predict([[0, 0, 0, 0]])\n",
      "     |  array([1])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExtraTreesClassifier\n",
      "     |      ForestClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseForest\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.ensemble._forest.ExtraTreesClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.ExtraTreesClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._forest.ExtraTreesClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.ExtraTreesClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestClassifier:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is a vote by the trees in\n",
      "     |      the forest, weighted by their probability estimates. That is,\n",
      "     |      the predicted class is the one with highest mean probability\n",
      "     |      estimate across the trees.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the log of the mean predicted class probabilities of the trees in the\n",
      "     |      forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample are computed as\n",
      "     |      the mean predicted class probabilities of the trees in the forest.\n",
      "     |      The class probability of a single tree is the fraction of samples of\n",
      "     |      the same class in a leaf.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      "     |          Return a node indicator matrix where non zero elements indicates\n",
      "     |          that the samples goes through the nodes. The matrix is of CSR\n",
      "     |          format.\n",
      "     |      \n",
      "     |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, its dtype will be converted\n",
      "     |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseForest:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of indices identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class ExtraTreesRegressor(ForestRegressor)\n",
      "     |  ExtraTreesRegressor(n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
      "     |  \n",
      "     |  An extra-trees regressor.\n",
      "     |  \n",
      "     |  This class implements a meta estimator that fits a number of\n",
      "     |  randomized decision trees (a.k.a. extra-trees) on various sub-samples\n",
      "     |  of the dataset and uses averaging to improve the predictive accuracy\n",
      "     |  and control over-fitting.\n",
      "     |  \n",
      "     |  This estimator has native support for missing values (NaNs) for\n",
      "     |  random splits. During training, a random threshold will be chosen\n",
      "     |  to split the non-missing values on. Then the non-missing values will be sent\n",
      "     |  to the left and right child based on the randomly selected threshold, while\n",
      "     |  the missing values will also be randomly sent to the left or right child.\n",
      "     |  This is repeated for every feature considered at each split. The best split\n",
      "     |  among these is chosen.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : int, default=100\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``n_estimators`` changed from 10 to 100\n",
      "     |         in 0.22.\n",
      "     |  \n",
      "     |  criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"},             default=\"squared_error\"\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"squared_error\" for the mean squared error, which is equal to\n",
      "     |      variance reduction as feature selection criterion and minimizes the L2\n",
      "     |      loss using the mean of each terminal node, \"friedman_mse\", which uses\n",
      "     |      mean squared error with Friedman's improvement score for potential\n",
      "     |      splits, \"absolute_error\" for the mean absolute error, which minimizes\n",
      "     |      the L1 loss using the median of each terminal node, and \"poisson\" which\n",
      "     |      uses reduction in Poisson deviance to find splits.\n",
      "     |      Training using \"absolute_error\" is significantly slower\n",
      "     |      than when using \"squared_error\".\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Mean Absolute Error (MAE) criterion.\n",
      "     |  \n",
      "     |  max_depth : int, default=None\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `max(1, int(max_features * n_features_in_))` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None or 1.0, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      .. note::\n",
      "     |          The default of 1.0 is equivalent to bagged trees and more\n",
      "     |          randomness can be achieved by setting smaller values, e.g. 0.3.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.1\n",
      "     |          The default of `max_features` changed from `\"auto\"` to 1.0.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  bootstrap : bool, default=False\n",
      "     |      Whether bootstrap samples are used when building trees. If False, the\n",
      "     |      whole dataset is used to build each tree.\n",
      "     |  \n",
      "     |  oob_score : bool or callable, default=False\n",
      "     |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      "     |      By default, :func:`~sklearn.metrics.r2_score` is used.\n",
      "     |      Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
      "     |      custom metric. Only available if `bootstrap=True`.\n",
      "     |  \n",
      "     |      For an illustration of out-of-bag (OOB) error estimation, see the example\n",
      "     |      :ref:`sphx_glr_auto_examples_ensemble_plot_ensemble_oob.py`.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      "     |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      "     |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "     |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      "     |      <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls 3 sources of randomness:\n",
      "     |  \n",
      "     |      - the bootstrapping of the samples used when building trees\n",
      "     |        (if ``bootstrap=True``)\n",
      "     |      - the sampling of the features to consider when looking for the best\n",
      "     |        split at each node (if ``max_features < n_features``)\n",
      "     |      - the draw of the splits for each of the `max_features`\n",
      "     |  \n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`Glossary <warm_start>` and\n",
      "     |      :ref:`tree_ensemble_warm_start` for details.\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "     |      :ref:`minimal_cost_complexity_pruning` for details. See\n",
      "     |      :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n",
      "     |      for an example of such pruning.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  max_samples : int or float, default=None\n",
      "     |      If bootstrap is True, the number of samples to draw from X\n",
      "     |      to train each base estimator.\n",
      "     |  \n",
      "     |      - If None (default), then draw `X.shape[0]` samples.\n",
      "     |      - If int, then draw `max_samples` samples.\n",
      "     |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      "     |        `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  monotonic_cst : array-like of int of shape (n_features), default=None\n",
      "     |      Indicates the monotonicity constraint to enforce on each feature.\n",
      "     |        - 1: monotonically increasing\n",
      "     |        - 0: no constraint\n",
      "     |        - -1: monotonically decreasing\n",
      "     |  \n",
      "     |      If monotonic_cst is None, no constraints are applied.\n",
      "     |  \n",
      "     |      Monotonicity constraints are not supported for:\n",
      "     |        - multioutput regressions (i.e. when `n_outputs_ > 1`),\n",
      "     |        - regressions trained on data with missing values.\n",
      "     |  \n",
      "     |      Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor`\n",
      "     |      The child estimator template used to create the collection of fitted\n",
      "     |      sub-estimators.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |         `base_estimator_` was renamed to `estimator_`.\n",
      "     |  \n",
      "     |  estimators_ : list of DecisionTreeRegressor\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |      Prediction computed with out-of-bag estimate on the training set.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of arrays\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator. Each subset is defined by an array of the indices selected.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  ExtraTreesClassifier : An extra-trees classifier with random splits.\n",
      "     |  RandomForestClassifier : A random forest classifier with optimal splits.\n",
      "     |  RandomForestRegressor : Ensemble regressor using trees with optimal splits.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "     |         Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_diabetes\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.ensemble import ExtraTreesRegressor\n",
      "     |  >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...     X, y, random_state=0)\n",
      "     |  >>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(\n",
      "     |  ...    X_train, y_train)\n",
      "     |  >>> reg.score(X_test, y_test)\n",
      "     |  0.2727...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExtraTreesRegressor\n",
      "     |      ForestRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseForest\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.ensemble._forest.ExtraTreesRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.ExtraTreesRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._forest.ExtraTreesRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.ExtraTreesRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestRegressor:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      The predicted regression target of an input sample is computed as the\n",
      "     |      mean predicted regression targets of the trees in the forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      "     |          Return a node indicator matrix where non zero elements indicates\n",
      "     |          that the samples goes through the nodes. The matrix is of CSR\n",
      "     |          format.\n",
      "     |      \n",
      "     |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, its dtype will be converted\n",
      "     |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseForest:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of indices identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class GradientBoostingClassifier(sklearn.base.ClassifierMixin, BaseGradientBoosting)\n",
      "     |  GradientBoostingClassifier(*, loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      "     |  \n",
      "     |  Gradient Boosting for classification.\n",
      "     |  \n",
      "     |  This algorithm builds an additive model in a forward stage-wise fashion; it\n",
      "     |  allows for the optimization of arbitrary differentiable loss functions. In\n",
      "     |  each stage ``n_classes_`` regression trees are fit on the negative gradient\n",
      "     |  of the loss function, e.g. binary or multiclass log loss. Binary\n",
      "     |  classification is a special case where only a single regression tree is\n",
      "     |  induced.\n",
      "     |  \n",
      "     |  :class:`~sklearn.ensemble.HistGradientBoostingClassifier` is a much faster variant\n",
      "     |  of this algorithm for intermediate and large datasets (`n_samples >= 10_000`) and\n",
      "     |  supports monotonic constraints.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : {'log_loss', 'exponential'}, default='log_loss'\n",
      "     |      The loss function to be optimized. 'log_loss' refers to binomial and\n",
      "     |      multinomial deviance, the same as used in logistic regression.\n",
      "     |      It is a good choice for classification with probabilistic outputs.\n",
      "     |      For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.\n",
      "     |  \n",
      "     |  learning_rate : float, default=0.1\n",
      "     |      Learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      "     |      There is a trade-off between learning_rate and n_estimators.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |      For an example of the effects of this parameter and its interaction with\n",
      "     |      ``subsample``, see\n",
      "     |      :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regularization.py`.\n",
      "     |  \n",
      "     |  n_estimators : int, default=100\n",
      "     |      The number of boosting stages to perform. Gradient boosting\n",
      "     |      is fairly robust to over-fitting so a large number usually\n",
      "     |      results in better performance.\n",
      "     |      Values must be in the range `[1, inf)`.\n",
      "     |  \n",
      "     |  subsample : float, default=1.0\n",
      "     |      The fraction of samples to be used for fitting the individual base\n",
      "     |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      "     |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      "     |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |      Values must be in the range `(0.0, 1.0]`.\n",
      "     |  \n",
      "     |  criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'\n",
      "     |      The function to measure the quality of a split. Supported criteria are\n",
      "     |      'friedman_mse' for the mean squared error with improvement score by\n",
      "     |      Friedman, 'squared_error' for mean squared error. The default value of\n",
      "     |      'friedman_mse' is generally the best as it can provide a better\n",
      "     |      approximation in some cases.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, values must be in the range `[2, inf)`.\n",
      "     |      - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\n",
      "     |        will be `ceil(min_samples_split * n_samples)`.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, values must be in the range `[1, inf)`.\n",
      "     |      - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`\n",
      "     |        will be `ceil(min_samples_leaf * n_samples)`.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |      Values must be in the range `[0.0, 0.5]`.\n",
      "     |  \n",
      "     |  max_depth : int or None, default=3\n",
      "     |      Maximum depth of the individual regression estimators. The maximum\n",
      "     |      depth limits the number of nodes in the tree. Tune this parameter\n",
      "     |      for best performance; the best value depends on the interaction\n",
      "     |      of the input variables. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |      If int, values must be in the range `[1, inf)`.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  init : estimator or 'zero', default=None\n",
      "     |      An estimator object that is used to compute the initial predictions.\n",
      "     |      ``init`` has to provide :term:`fit` and :term:`predict_proba`. If\n",
      "     |      'zero', the initial raw predictions are set to zero. By default, a\n",
      "     |      ``DummyEstimator`` predicting the classes priors is used.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the random seed given to each Tree estimator at each\n",
      "     |      boosting iteration.\n",
      "     |      In addition, it controls the random permutation of the features at\n",
      "     |      each split (see Notes for more details).\n",
      "     |      It also controls the random splitting of the training data to obtain a\n",
      "     |      validation set if `n_iter_no_change` is not None.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  max_features : {'sqrt', 'log2'}, int or float, default=None\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, values must be in the range `[1, inf)`.\n",
      "     |      - If float, values must be in the range `(0.0, 1.0]` and the features\n",
      "     |        considered at each split will be `max(1, int(max_features * n_features_in_))`.\n",
      "     |      - If 'sqrt', then `max_features=sqrt(n_features)`.\n",
      "     |      - If 'log2', then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Enable verbose output. If 1 then it prints progress and performance\n",
      "     |      once in a while (the more trees the lower the frequency). If greater\n",
      "     |      than 1 then it prints progress and performance for every tree.\n",
      "     |      Values must be in the range `[0, inf)`.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      Values must be in the range `[2, inf)`.\n",
      "     |      If `None`, then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just erase the\n",
      "     |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Values must be in the range `(0.0, 1.0)`.\n",
      "     |      Only used if ``n_iter_no_change`` is set to an integer.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=None\n",
      "     |      ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      "     |      to terminate training when validation score is not improving. By\n",
      "     |      default it is set to None to disable early stopping. If set to a\n",
      "     |      number, it will set aside ``validation_fraction`` size of the training\n",
      "     |      data as validation and terminate training when validation score is not\n",
      "     |      improving in all of the previous ``n_iter_no_change`` numbers of\n",
      "     |      iterations. The split is stratified.\n",
      "     |      Values must be in the range `[1, inf)`.\n",
      "     |      See\n",
      "     |      :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Tolerance for the early stopping. When the loss is not improving\n",
      "     |      by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      "     |      number), the training stops.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |      See :ref:`minimal_cost_complexity_pruning` for details. See\n",
      "     |      :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n",
      "     |      for an example of such pruning.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_estimators_ : int\n",
      "     |      The number of estimators as selected by early stopping (if\n",
      "     |      ``n_iter_no_change`` is specified). Otherwise it is set to\n",
      "     |      ``n_estimators``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_trees_per_iteration_ : int\n",
      "     |      The number of trees that are built at each iteration. For binary classifiers,\n",
      "     |      this is always 1.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  oob_improvement_ : ndarray of shape (n_estimators,)\n",
      "     |      The improvement in loss on the out-of-bag samples\n",
      "     |      relative to the previous iteration.\n",
      "     |      ``oob_improvement_[0]`` is the improvement in\n",
      "     |      loss of the first stage over the ``init`` estimator.\n",
      "     |      Only available if ``subsample < 1.0``.\n",
      "     |  \n",
      "     |  oob_scores_ : ndarray of shape (n_estimators,)\n",
      "     |      The full history of the loss values on the out-of-bag\n",
      "     |      samples. Only available if `subsample < 1.0`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      The last value of the loss on the out-of-bag samples. It is\n",
      "     |      the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  train_score_ : ndarray of shape (n_estimators,)\n",
      "     |      The i-th score ``train_score_[i]`` is the loss of the\n",
      "     |      model at iteration ``i`` on the in-bag sample.\n",
      "     |      If ``subsample == 1`` this is the loss on the training data.\n",
      "     |  \n",
      "     |  init_ : estimator\n",
      "     |      The estimator that provides the initial predictions. Set via the ``init``\n",
      "     |      argument.\n",
      "     |  \n",
      "     |  estimators_ : ndarray of DecisionTreeRegressor of             shape (n_estimators, ``n_trees_per_iteration_``)\n",
      "     |      The collection of fitted sub-estimators. ``n_trees_per_iteration_`` is 1 for\n",
      "     |      binary classification, otherwise ``n_classes``.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_classes_ : int\n",
      "     |      The number of classes.\n",
      "     |  \n",
      "     |  max_features_ : int\n",
      "     |      The inferred value of max_features.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  HistGradientBoostingClassifier : Histogram-based Gradient Boosting\n",
      "     |      Classification Tree.\n",
      "     |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      "     |  RandomForestClassifier : A meta-estimator that fits a number of decision\n",
      "     |      tree classifiers on various sub-samples of the dataset and uses\n",
      "     |      averaging to improve the predictive accuracy and control over-fitting.\n",
      "     |  AdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n",
      "     |      on the original dataset and then fits additional copies of the\n",
      "     |      classifier on the same dataset where the weights of incorrectly\n",
      "     |      classified instances are adjusted such that subsequent classifiers\n",
      "     |      focus more on difficult cases.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data and\n",
      "     |  ``max_features=n_features``, if the improvement of the criterion is\n",
      "     |  identical for several splits enumerated during the search of the best\n",
      "     |  split. To obtain a deterministic behaviour during fitting,\n",
      "     |  ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      "     |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      "     |  \n",
      "     |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      "     |  \n",
      "     |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      "     |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  The following example shows how to fit a gradient boosting classifier with\n",
      "     |  100 decision stumps as weak learners.\n",
      "     |  \n",
      "     |  >>> from sklearn.datasets import make_hastie_10_2\n",
      "     |  >>> from sklearn.ensemble import GradientBoostingClassifier\n",
      "     |  \n",
      "     |  >>> X, y = make_hastie_10_2(random_state=0)\n",
      "     |  >>> X_train, X_test = X[:2000], X[2000:]\n",
      "     |  >>> y_train, y_test = y[:2000], y[2000:]\n",
      "     |  \n",
      "     |  >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
      "     |  ...     max_depth=1, random_state=0).fit(X_train, y_train)\n",
      "     |  >>> clf.score(X_test, y_test)\n",
      "     |  0.913\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GradientBoostingClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseGradientBoosting\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Compute the decision function of ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : ndarray of shape (n_samples, n_classes) or (n_samples,)\n",
      "     |          The decision function of the input samples, which corresponds to\n",
      "     |          the raw values predicted from the trees of the ensemble . The\n",
      "     |          order of the classes corresponds to that in the attribute\n",
      "     |          :term:`classes_`. Regression and binary classification produce an\n",
      "     |          array of shape (n_samples,).\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes)\n",
      "     |          The class log-probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          If the ``loss`` does not support probabilities.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes)\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          If the ``loss`` does not support probabilities.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.ensemble._gb.GradientBoostingClassifier, *, monitor: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._gb.GradientBoostingClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      monitor : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``monitor`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._gb.GradientBoostingClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._gb.GradientBoostingClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  staged_decision_function(self, X)\n",
      "     |      Compute decision function of ``X`` for each iteration.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      score : generator of ndarray of shape (n_samples, k)\n",
      "     |          The decision function of the input samples, which corresponds to\n",
      "     |          the raw values predicted from the trees of the ensemble . The\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |          Regression and binary classification are special cases with\n",
      "     |          ``k == 1``, otherwise ``k==n_classes``.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Predict class at each stage for X.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      y : generator of ndarray of shape (n_samples,)\n",
      "     |          The predicted value of the input samples.\n",
      "     |  \n",
      "     |  staged_predict_proba(self, X)\n",
      "     |      Predict class probabilities at each stage for X.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      y : generator of ndarray of shape (n_samples,)\n",
      "     |          The predicted value of the input samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the ensemble to X, return leaf indices.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      "     |          be converted to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array-like of shape (n_samples, n_estimators, n_classes)\n",
      "     |          For each datapoint x in X and for each tree in the ensemble,\n",
      "     |          return the index of the leaf x ends up in each estimator.\n",
      "     |          In the case of binary classification n_classes is 1.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      "     |      Fit the gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values (strings or integers in classification, real numbers\n",
      "     |          in regression)\n",
      "     |          For classification, labels must correspond to classes.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      monitor : callable, default=None\n",
      "     |          The monitor is called after each iteration with the current\n",
      "     |          iteration, a reference to the estimator and the local variables of\n",
      "     |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      "     |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      "     |          is stopped. The monitor can be used for various things such as\n",
      "     |          computing held-out estimates, early stopping, model introspect, and\n",
      "     |          snapshotting.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class GradientBoostingRegressor(sklearn.base.RegressorMixin, BaseGradientBoosting)\n",
      "     |  GradientBoostingRegressor(*, loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      "     |  \n",
      "     |  Gradient Boosting for regression.\n",
      "     |  \n",
      "     |  This estimator builds an additive model in a forward stage-wise fashion; it\n",
      "     |  allows for the optimization of arbitrary differentiable loss functions. In\n",
      "     |  each stage a regression tree is fit on the negative gradient of the given\n",
      "     |  loss function.\n",
      "     |  \n",
      "     |  :class:`~sklearn.ensemble.HistGradientBoostingRegressor` is a much faster variant\n",
      "     |  of this algorithm for intermediate and large datasets (`n_samples >= 10_000`) and\n",
      "     |  supports monotonic constraints.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : {'squared_error', 'absolute_error', 'huber', 'quantile'},             default='squared_error'\n",
      "     |      Loss function to be optimized. 'squared_error' refers to the squared\n",
      "     |      error for regression. 'absolute_error' refers to the absolute error of\n",
      "     |      regression and is a robust loss function. 'huber' is a\n",
      "     |      combination of the two. 'quantile' allows quantile regression (use\n",
      "     |      `alpha` to specify the quantile).\n",
      "     |      See\n",
      "     |      :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_quantile.py`\n",
      "     |      for an example that demonstrates quantile regression for creating\n",
      "     |      prediction intervals with `loss='quantile'`.\n",
      "     |  \n",
      "     |  learning_rate : float, default=0.1\n",
      "     |      Learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      "     |      There is a trade-off between learning_rate and n_estimators.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |  n_estimators : int, default=100\n",
      "     |      The number of boosting stages to perform. Gradient boosting\n",
      "     |      is fairly robust to over-fitting so a large number usually\n",
      "     |      results in better performance.\n",
      "     |      Values must be in the range `[1, inf)`.\n",
      "     |  \n",
      "     |  subsample : float, default=1.0\n",
      "     |      The fraction of samples to be used for fitting the individual base\n",
      "     |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      "     |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      "     |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |      Values must be in the range `(0.0, 1.0]`.\n",
      "     |  \n",
      "     |  criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse'\n",
      "     |      The function to measure the quality of a split. Supported criteria are\n",
      "     |      \"friedman_mse\" for the mean squared error with improvement score by\n",
      "     |      Friedman, \"squared_error\" for mean squared error. The default value of\n",
      "     |      \"friedman_mse\" is generally the best as it can provide a better\n",
      "     |      approximation in some cases.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, values must be in the range `[2, inf)`.\n",
      "     |      - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split`\n",
      "     |        will be `ceil(min_samples_split * n_samples)`.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, values must be in the range `[1, inf)`.\n",
      "     |      - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf`\n",
      "     |        will be `ceil(min_samples_leaf * n_samples)`.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |      Values must be in the range `[0.0, 0.5]`.\n",
      "     |  \n",
      "     |  max_depth : int or None, default=3\n",
      "     |      Maximum depth of the individual regression estimators. The maximum\n",
      "     |      depth limits the number of nodes in the tree. Tune this parameter\n",
      "     |      for best performance; the best value depends on the interaction\n",
      "     |      of the input variables. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |      If int, values must be in the range `[1, inf)`.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  init : estimator or 'zero', default=None\n",
      "     |      An estimator object that is used to compute the initial predictions.\n",
      "     |      ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n",
      "     |      initial raw predictions are set to zero. By default a\n",
      "     |      ``DummyEstimator`` is used, predicting either the average target value\n",
      "     |      (for loss='squared_error'), or a quantile for the other losses.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the random seed given to each Tree estimator at each\n",
      "     |      boosting iteration.\n",
      "     |      In addition, it controls the random permutation of the features at\n",
      "     |      each split (see Notes for more details).\n",
      "     |      It also controls the random splitting of the training data to obtain a\n",
      "     |      validation set if `n_iter_no_change` is not None.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  max_features : {'sqrt', 'log2'}, int or float, default=None\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, values must be in the range `[1, inf)`.\n",
      "     |      - If float, values must be in the range `(0.0, 1.0]` and the features\n",
      "     |        considered at each split will be `max(1, int(max_features * n_features_in_))`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  alpha : float, default=0.9\n",
      "     |      The alpha-quantile of the huber loss function and the quantile\n",
      "     |      loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n",
      "     |      Values must be in the range `(0.0, 1.0)`.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Enable verbose output. If 1 then it prints progress and performance\n",
      "     |      once in a while (the more trees the lower the frequency). If greater\n",
      "     |      than 1 then it prints progress and performance for every tree.\n",
      "     |      Values must be in the range `[0, inf)`.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      Values must be in the range `[2, inf)`.\n",
      "     |      If None, then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just erase the\n",
      "     |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Values must be in the range `(0.0, 1.0)`.\n",
      "     |      Only used if ``n_iter_no_change`` is set to an integer.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=None\n",
      "     |      ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      "     |      to terminate training when validation score is not improving. By\n",
      "     |      default it is set to None to disable early stopping. If set to a\n",
      "     |      number, it will set aside ``validation_fraction`` size of the training\n",
      "     |      data as validation and terminate training when validation score is not\n",
      "     |      improving in all of the previous ``n_iter_no_change`` numbers of\n",
      "     |      iterations.\n",
      "     |      Values must be in the range `[1, inf)`.\n",
      "     |      See\n",
      "     |      :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Tolerance for the early stopping. When the loss is not improving\n",
      "     |      by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      "     |      number), the training stops.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n",
      "     |      Values must be in the range `[0.0, inf)`.\n",
      "     |      See :ref:`minimal_cost_complexity_pruning` for details. See\n",
      "     |      :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n",
      "     |      for an example of such pruning.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_estimators_ : int\n",
      "     |      The number of estimators as selected by early stopping (if\n",
      "     |      ``n_iter_no_change`` is specified). Otherwise it is set to\n",
      "     |      ``n_estimators``.\n",
      "     |  \n",
      "     |  n_trees_per_iteration_ : int\n",
      "     |      The number of trees that are built at each iteration. For regressors, this is\n",
      "     |      always 1.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.4.0\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  oob_improvement_ : ndarray of shape (n_estimators,)\n",
      "     |      The improvement in loss on the out-of-bag samples\n",
      "     |      relative to the previous iteration.\n",
      "     |      ``oob_improvement_[0]`` is the improvement in\n",
      "     |      loss of the first stage over the ``init`` estimator.\n",
      "     |      Only available if ``subsample < 1.0``.\n",
      "     |  \n",
      "     |  oob_scores_ : ndarray of shape (n_estimators,)\n",
      "     |      The full history of the loss values on the out-of-bag\n",
      "     |      samples. Only available if `subsample < 1.0`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      The last value of the loss on the out-of-bag samples. It is\n",
      "     |      the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.3\n",
      "     |  \n",
      "     |  train_score_ : ndarray of shape (n_estimators,)\n",
      "     |      The i-th score ``train_score_[i]`` is the loss of the\n",
      "     |      model at iteration ``i`` on the in-bag sample.\n",
      "     |      If ``subsample == 1`` this is the loss on the training data.\n",
      "     |  \n",
      "     |  init_ : estimator\n",
      "     |      The estimator that provides the initial predictions. Set via the ``init``\n",
      "     |      argument.\n",
      "     |  \n",
      "     |  estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  max_features_ : int\n",
      "     |      The inferred value of max_features.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  HistGradientBoostingRegressor : Histogram-based Gradient Boosting\n",
      "     |      Classification Tree.\n",
      "     |  sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
      "     |  sklearn.ensemble.RandomForestRegressor : A random forest regressor.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data and\n",
      "     |  ``max_features=n_features``, if the improvement of the criterion is\n",
      "     |  identical for several splits enumerated during the search of the best\n",
      "     |  split. To obtain a deterministic behaviour during fitting,\n",
      "     |  ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      "     |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      "     |  \n",
      "     |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      "     |  \n",
      "     |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      "     |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> from sklearn.ensemble import GradientBoostingRegressor\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> X, y = make_regression(random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...     X, y, random_state=0)\n",
      "     |  >>> reg = GradientBoostingRegressor(random_state=0)\n",
      "     |  >>> reg.fit(X_train, y_train)\n",
      "     |  GradientBoostingRegressor(random_state=0)\n",
      "     |  >>> reg.predict(X_test[1:2])\n",
      "     |  array([-61.1])\n",
      "     |  >>> reg.score(X_test, y_test)\n",
      "     |  0.4...\n",
      "     |  \n",
      "     |  For a detailed example of utilizing\n",
      "     |  :class:`~sklearn.ensemble.GradientBoostingRegressor`\n",
      "     |  to fit an ensemble of weak predictive models, please refer to\n",
      "     |  :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regression.py`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GradientBoostingRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseGradientBoosting\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the ensemble to X, return leaf indices.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      "     |          be converted to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array-like of shape (n_samples, n_estimators)\n",
      "     |          For each datapoint x in X and for each tree in the ensemble,\n",
      "     |          return the index of the leaf x ends up in each estimator.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.ensemble._gb.GradientBoostingRegressor, *, monitor: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._gb.GradientBoostingRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      monitor : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``monitor`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._gb.GradientBoostingRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._gb.GradientBoostingRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Predict regression target at each stage for X.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      y : generator of ndarray of shape (n_samples,)\n",
      "     |          The predicted value of the input samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      "     |      Fit the gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values (strings or integers in classification, real numbers\n",
      "     |          in regression)\n",
      "     |          For classification, labels must correspond to classes.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      monitor : callable, default=None\n",
      "     |          The monitor is called after each iteration with the current\n",
      "     |          iteration, a reference to the estimator and the local variables of\n",
      "     |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      "     |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      "     |          is stopped. The monitor can be used for various things such as\n",
      "     |          computing held-out estimates, early stopping, model introspect, and\n",
      "     |          snapshotting.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class HistGradientBoostingClassifier(sklearn.base.ClassifierMixin, BaseHistGradientBoosting)\n",
      "     |  HistGradientBoostingClassifier(loss='log_loss', *, learning_rate=0.1, max_iter=100, max_leaf_nodes=31, max_depth=None, min_samples_leaf=20, l2_regularization=0.0, max_features=1.0, max_bins=255, categorical_features='from_dtype', monotonic_cst=None, interaction_cst=None, warm_start=False, early_stopping='auto', scoring='loss', validation_fraction=0.1, n_iter_no_change=10, tol=1e-07, verbose=0, random_state=None, class_weight=None)\n",
      "     |  \n",
      "     |  Histogram-based Gradient Boosting Classification Tree.\n",
      "     |  \n",
      "     |  This estimator is much faster than\n",
      "     |  :class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`\n",
      "     |  for big datasets (n_samples >= 10 000).\n",
      "     |  \n",
      "     |  This estimator has native support for missing values (NaNs). During\n",
      "     |  training, the tree grower learns at each split point whether samples\n",
      "     |  with missing values should go to the left or right child, based on the\n",
      "     |  potential gain. When predicting, samples with missing values are\n",
      "     |  assigned to the left or right child consequently. If no missing values\n",
      "     |  were encountered for a given feature during training, then samples with\n",
      "     |  missing values are mapped to whichever child has the most samples.\n",
      "     |  \n",
      "     |  This implementation is inspired by\n",
      "     |  `LightGBM <https://github.com/Microsoft/LightGBM>`_.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.21\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : {'log_loss'}, default='log_loss'\n",
      "     |      The loss function to use in the boosting process.\n",
      "     |  \n",
      "     |      For binary classification problems, 'log_loss' is also known as logistic loss,\n",
      "     |      binomial deviance or binary crossentropy. Internally, the model fits one tree\n",
      "     |      per boosting iteration and uses the logistic sigmoid function (expit) as\n",
      "     |      inverse link function to compute the predicted positive class probability.\n",
      "     |  \n",
      "     |      For multiclass classification problems, 'log_loss' is also known as multinomial\n",
      "     |      deviance or categorical crossentropy. Internally, the model fits one tree per\n",
      "     |      boosting iteration and per class and uses the softmax function as inverse link\n",
      "     |      function to compute the predicted probabilities of the classes.\n",
      "     |  \n",
      "     |  learning_rate : float, default=0.1\n",
      "     |      The learning rate, also known as *shrinkage*. This is used as a\n",
      "     |      multiplicative factor for the leaves values. Use ``1`` for no\n",
      "     |      shrinkage.\n",
      "     |  max_iter : int, default=100\n",
      "     |      The maximum number of iterations of the boosting process, i.e. the\n",
      "     |      maximum number of trees for binary classification. For multiclass\n",
      "     |      classification, `n_classes` trees per iteration are built.\n",
      "     |  max_leaf_nodes : int or None, default=31\n",
      "     |      The maximum number of leaves for each tree. Must be strictly greater\n",
      "     |      than 1. If None, there is no maximum limit.\n",
      "     |  max_depth : int or None, default=None\n",
      "     |      The maximum depth of each tree. The depth of a tree is the number of\n",
      "     |      edges to go from the root to the deepest leaf.\n",
      "     |      Depth isn't constrained by default.\n",
      "     |  min_samples_leaf : int, default=20\n",
      "     |      The minimum number of samples per leaf. For small datasets with less\n",
      "     |      than a few hundred samples, it is recommended to lower this value\n",
      "     |      since only very shallow trees would be built.\n",
      "     |  l2_regularization : float, default=0\n",
      "     |      The L2 regularization parameter penalizing leaves with small hessians.\n",
      "     |      Use ``0`` for no regularization (default).\n",
      "     |  max_features : float, default=1.0\n",
      "     |      Proportion of randomly chosen features in each and every node split.\n",
      "     |      This is a form of regularization, smaller values make the trees weaker\n",
      "     |      learners and might prevent overfitting.\n",
      "     |      If interaction constraints from `interaction_cst` are present, only allowed\n",
      "     |      features are taken into account for the subsampling.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  max_bins : int, default=255\n",
      "     |      The maximum number of bins to use for non-missing values. Before\n",
      "     |      training, each feature of the input array `X` is binned into\n",
      "     |      integer-valued bins, which allows for a much faster training stage.\n",
      "     |      Features with a small number of unique values may use less than\n",
      "     |      ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n",
      "     |      is always reserved for missing values. Must be no larger than 255.\n",
      "     |  categorical_features : array-like of {bool, int, str} of shape (n_features)             or shape (n_categorical_features,), default='from_dtype'\n",
      "     |      Indicates the categorical features.\n",
      "     |  \n",
      "     |      - None : no feature will be considered categorical.\n",
      "     |      - boolean array-like : boolean mask indicating categorical features.\n",
      "     |      - integer array-like : integer indices indicating categorical\n",
      "     |        features.\n",
      "     |      - str array-like: names of categorical features (assuming the training\n",
      "     |        data has feature names).\n",
      "     |      - `\"from_dtype\"`: dataframe columns with dtype \"category\" are\n",
      "     |        considered to be categorical features. The input must be an object\n",
      "     |        exposing a ``__dataframe__`` method such as pandas or polars\n",
      "     |        DataFrames to use this feature.\n",
      "     |  \n",
      "     |      For each categorical feature, there must be at most `max_bins` unique\n",
      "     |      categories. Negative values for categorical features encoded as numeric\n",
      "     |      dtypes are treated as missing values. All categorical values are\n",
      "     |      converted to floating point numbers. This means that categorical values\n",
      "     |      of 1.0 and 1 are treated as the same category.\n",
      "     |  \n",
      "     |      Read more in the :ref:`User Guide <categorical_support_gbdt>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.2\n",
      "     |         Added support for feature names.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.4\n",
      "     |         Added `\"from_dtype\"` option.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.6\n",
      "     |         The default value changed from `None` to `\"from_dtype\"`.\n",
      "     |  \n",
      "     |  monotonic_cst : array-like of int of shape (n_features) or dict, default=None\n",
      "     |      Monotonic constraint to enforce on each feature are specified using the\n",
      "     |      following integer values:\n",
      "     |  \n",
      "     |      - 1: monotonic increase\n",
      "     |      - 0: no constraint\n",
      "     |      - -1: monotonic decrease\n",
      "     |  \n",
      "     |      If a dict with str keys, map feature to monotonic constraints by name.\n",
      "     |      If an array, the features are mapped to constraints by position. See\n",
      "     |      :ref:`monotonic_cst_features_names` for a usage example.\n",
      "     |  \n",
      "     |      The constraints are only valid for binary classifications and hold\n",
      "     |      over the probability of the positive class.\n",
      "     |      Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.2\n",
      "     |         Accept dict of constraints with feature names as keys.\n",
      "     |  \n",
      "     |  interaction_cst : {\"pairwise\", \"no_interactions\"} or sequence of lists/tuples/sets             of int, default=None\n",
      "     |      Specify interaction constraints, the sets of features which can\n",
      "     |      interact with each other in child node splits.\n",
      "     |  \n",
      "     |      Each item specifies the set of feature indices that are allowed\n",
      "     |      to interact with each other. If there are more features than\n",
      "     |      specified in these constraints, they are treated as if they were\n",
      "     |      specified as an additional set.\n",
      "     |  \n",
      "     |      The strings \"pairwise\" and \"no_interactions\" are shorthands for\n",
      "     |      allowing only pairwise or no interactions, respectively.\n",
      "     |  \n",
      "     |      For instance, with 5 features in total, `interaction_cst=[{0, 1}]`\n",
      "     |      is equivalent to `interaction_cst=[{0, 1}, {2, 3, 4}]`,\n",
      "     |      and specifies that each branch of a tree will either only split\n",
      "     |      on features 0 and 1 or only split on features 2, 3 and 4.\n",
      "     |  \n",
      "     |      See :ref:`this example<ice-vs-pdp>` on how to use `interaction_cst`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble. For results to be valid, the\n",
      "     |      estimator should be re-trained on the same data only.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  early_stopping : 'auto' or bool, default='auto'\n",
      "     |      If 'auto', early stopping is enabled if the sample size is larger than\n",
      "     |      10000 or if `X_val` and `y_val` are passed to `fit`. If True, early stopping\n",
      "     |      is enabled, otherwise early stopping is disabled.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  scoring : str or callable or None, default='loss'\n",
      "     |      Scoring method to use for early stopping. Only used if `early_stopping`\n",
      "     |      is enabled. Options:\n",
      "     |  \n",
      "     |      - str: see :ref:`scoring_string_names` for options.\n",
      "     |      - callable: a scorer callable object (e.g., function) with signature\n",
      "     |        ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n",
      "     |      - `None`: :ref:`accuracy <accuracy_score>` is used.\n",
      "     |      - 'loss': early stopping is checked w.r.t the loss value.\n",
      "     |  \n",
      "     |  validation_fraction : int or float or None, default=0.1\n",
      "     |      Proportion (or absolute size) of training data to set aside as\n",
      "     |      validation data for early stopping. If None, early stopping is done on\n",
      "     |      the training data.\n",
      "     |      The value is ignored if either early stopping is not performed, e.g.\n",
      "     |      `early_stopping=False`, or if `X_val` and `y_val` are passed to fit.\n",
      "     |  n_iter_no_change : int, default=10\n",
      "     |      Used to determine when to \"early stop\". The fitting process is\n",
      "     |      stopped when none of the last ``n_iter_no_change`` scores are better\n",
      "     |      than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n",
      "     |      tolerance. Only used if early stopping is performed.\n",
      "     |  tol : float, default=1e-7\n",
      "     |      The absolute tolerance to use when comparing scores. The higher the\n",
      "     |      tolerance, the more likely we are to early stop: higher tolerance\n",
      "     |      means that it will be harder for subsequent iterations to be\n",
      "     |      considered an improvement upon the reference score.\n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level. If not zero, print some information about the\n",
      "     |      fitting process. ``1`` prints only summary info, ``2`` prints info per\n",
      "     |      iteration.\n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Pseudo-random number generator to control the subsampling in the\n",
      "     |      binning process, and the train/validation data split if early stopping\n",
      "     |      is enabled.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  class_weight : dict or 'balanced', default=None\n",
      "     |      Weights associated with classes in the form `{class_label: weight}`.\n",
      "     |      If not given, all classes are supposed to have weight one.\n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as `n_samples / (n_classes * np.bincount(y))`.\n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if `sample_weight` is specified.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : array, shape = (n_classes,)\n",
      "     |      Class labels.\n",
      "     |  do_early_stopping_ : bool\n",
      "     |      Indicates whether early stopping is used during training.\n",
      "     |  n_iter_ : int\n",
      "     |      The number of iterations as selected by early stopping, depending on\n",
      "     |      the `early_stopping` parameter. Otherwise it corresponds to max_iter.\n",
      "     |  n_trees_per_iteration_ : int\n",
      "     |      The number of tree that are built at each iteration. This is equal to 1\n",
      "     |      for binary classification, and to ``n_classes`` for multiclass\n",
      "     |      classification.\n",
      "     |  train_score_ : ndarray, shape (n_iter_+1,)\n",
      "     |      The scores at each iteration on the training data. The first entry\n",
      "     |      is the score of the ensemble before the first iteration. Scores are\n",
      "     |      computed according to the ``scoring`` parameter. If ``scoring`` is\n",
      "     |      not 'loss', scores are computed on a subset of at most 10 000\n",
      "     |      samples. Empty if no early stopping.\n",
      "     |  validation_score_ : ndarray, shape (n_iter_+1,)\n",
      "     |      The scores at each iteration on the held-out validation data. The\n",
      "     |      first entry is the score of the ensemble before the first iteration.\n",
      "     |      Scores are computed according to the ``scoring`` parameter. Empty if\n",
      "     |      no early stopping or if ``validation_fraction`` is None.\n",
      "     |  is_categorical_ : ndarray, shape (n_features, ) or None\n",
      "     |      Boolean mask for the categorical features. ``None`` if there are no\n",
      "     |      categorical features.\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  GradientBoostingClassifier : Exact gradient boosting method that does not\n",
      "     |      scale as good on datasets with a large number of samples.\n",
      "     |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      "     |  RandomForestClassifier : A meta-estimator that fits a number of decision\n",
      "     |      tree classifiers on various sub-samples of the dataset and uses\n",
      "     |      averaging to improve the predictive accuracy and control over-fitting.\n",
      "     |  AdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n",
      "     |      on the original dataset and then fits additional copies of the\n",
      "     |      classifier on the same dataset where the weights of incorrectly\n",
      "     |      classified instances are adjusted such that subsequent classifiers\n",
      "     |      focus more on difficult cases.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import HistGradientBoostingClassifier\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> clf = HistGradientBoostingClassifier().fit(X, y)\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  1.0\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HistGradientBoostingClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseHistGradientBoosting\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='log_loss', *, learning_rate=0.1, max_iter=100, max_leaf_nodes=31, max_depth=None, min_samples_leaf=20, l2_regularization=0.0, max_features=1.0, max_bins=255, categorical_features='from_dtype', monotonic_cst=None, interaction_cst=None, warm_start=False, early_stopping='auto', scoring='loss', validation_fraction=0.1, n_iter_no_change=10, tol=1e-07, verbose=0, random_state=None, class_weight=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Compute the decision function of ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      decision : ndarray, shape (n_samples,) or                 (n_samples, n_trees_per_iteration)\n",
      "     |          The raw predicted values (i.e. the sum of the trees leaves) for\n",
      "     |          each sample. n_trees_per_iteration is equal to the number of\n",
      "     |          classes in multiclass classification.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict classes for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray, shape (n_samples,)\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray, shape (n_samples, n_classes)\n",
      "     |          The class probabilities of the input samples.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier, *, X_val: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight_val: Union[bool, NoneType, str] = '$UNCHANGED$', y_val: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X_val : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``X_val`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      sample_weight_val : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight_val`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      y_val : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``y_val`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  staged_decision_function(self, X)\n",
      "     |      Compute decision function of ``X`` for each iteration.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      decision : generator of ndarray of shape (n_samples,) or                 (n_samples, n_trees_per_iteration)\n",
      "     |          The decision function of the input samples, which corresponds to\n",
      "     |          the raw values predicted from the trees of the ensemble . The\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Predict classes at each iteration.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.24\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      y : generator of ndarray of shape (n_samples,)\n",
      "     |          The predicted classes of the input samples, for each iteration.\n",
      "     |  \n",
      "     |  staged_predict_proba(self, X)\n",
      "     |      Predict class probabilities at each iteration.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      y : generator of ndarray of shape (n_samples,)\n",
      "     |          The predicted class probabilities of the input samples,\n",
      "     |          for each iteration.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseHistGradientBoosting:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, *, X_val=None, y_val=None, sample_weight_val=None)\n",
      "     |      Fit the gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,) default=None\n",
      "     |          Weights of training data.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.23\n",
      "     |      \n",
      "     |      X_val : array-like of shape (n_val, n_features)\n",
      "     |          Additional sample of features for validation used in early stopping.\n",
      "     |          In a `Pipeline`, `X_val` can be transformed the same way as `X` with\n",
      "     |          `Pipeline(..., transform_input=[\"X_val\"])`.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |      y_val : array-like of shape (n_samples,)\n",
      "     |          Additional sample of target values for validation used in early stopping.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |      sample_weight_val : array-like of shape (n_samples,) default=None\n",
      "     |          Additional weights for validation used in early stopping.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseHistGradientBoosting:\n",
      "     |  \n",
      "     |  n_iter_\n",
      "     |      Number of iterations of the boosting process.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class HistGradientBoostingRegressor(sklearn.base.RegressorMixin, BaseHistGradientBoosting)\n",
      "     |  HistGradientBoostingRegressor(loss='squared_error', *, quantile=None, learning_rate=0.1, max_iter=100, max_leaf_nodes=31, max_depth=None, min_samples_leaf=20, l2_regularization=0.0, max_features=1.0, max_bins=255, categorical_features='from_dtype', monotonic_cst=None, interaction_cst=None, warm_start=False, early_stopping='auto', scoring='loss', validation_fraction=0.1, n_iter_no_change=10, tol=1e-07, verbose=0, random_state=None)\n",
      "     |  \n",
      "     |  Histogram-based Gradient Boosting Regression Tree.\n",
      "     |  \n",
      "     |  This estimator is much faster than\n",
      "     |  :class:`GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>`\n",
      "     |  for big datasets (n_samples >= 10 000).\n",
      "     |  \n",
      "     |  This estimator has native support for missing values (NaNs). During\n",
      "     |  training, the tree grower learns at each split point whether samples\n",
      "     |  with missing values should go to the left or right child, based on the\n",
      "     |  potential gain. When predicting, samples with missing values are\n",
      "     |  assigned to the left or right child consequently. If no missing values\n",
      "     |  were encountered for a given feature during training, then samples with\n",
      "     |  missing values are mapped to whichever child has the most samples.\n",
      "     |  See :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for a\n",
      "     |  usecase example of this feature.\n",
      "     |  \n",
      "     |  This implementation is inspired by\n",
      "     |  `LightGBM <https://github.com/Microsoft/LightGBM>`_.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.21\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : {'squared_error', 'absolute_error', 'gamma', 'poisson', 'quantile'},             default='squared_error'\n",
      "     |      The loss function to use in the boosting process. Note that the\n",
      "     |      \"squared error\", \"gamma\" and \"poisson\" losses actually implement\n",
      "     |      \"half least squares loss\", \"half gamma deviance\" and \"half poisson\n",
      "     |      deviance\" to simplify the computation of the gradient. Furthermore,\n",
      "     |      \"gamma\" and \"poisson\" losses internally use a log-link, \"gamma\"\n",
      "     |      requires ``y > 0`` and \"poisson\" requires ``y >= 0``.\n",
      "     |      \"quantile\" uses the pinball loss.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.23\n",
      "     |         Added option 'poisson'.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.1\n",
      "     |         Added option 'quantile'.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.3\n",
      "     |         Added option 'gamma'.\n",
      "     |  \n",
      "     |  quantile : float, default=None\n",
      "     |      If loss is \"quantile\", this parameter specifies which quantile to be estimated\n",
      "     |      and must be between 0 and 1.\n",
      "     |  learning_rate : float, default=0.1\n",
      "     |      The learning rate, also known as *shrinkage*. This is used as a\n",
      "     |      multiplicative factor for the leaves values. Use ``1`` for no\n",
      "     |      shrinkage.\n",
      "     |  max_iter : int, default=100\n",
      "     |      The maximum number of iterations of the boosting process, i.e. the\n",
      "     |      maximum number of trees.\n",
      "     |  max_leaf_nodes : int or None, default=31\n",
      "     |      The maximum number of leaves for each tree. Must be strictly greater\n",
      "     |      than 1. If None, there is no maximum limit.\n",
      "     |  max_depth : int or None, default=None\n",
      "     |      The maximum depth of each tree. The depth of a tree is the number of\n",
      "     |      edges to go from the root to the deepest leaf.\n",
      "     |      Depth isn't constrained by default.\n",
      "     |  min_samples_leaf : int, default=20\n",
      "     |      The minimum number of samples per leaf. For small datasets with less\n",
      "     |      than a few hundred samples, it is recommended to lower this value\n",
      "     |      since only very shallow trees would be built.\n",
      "     |  l2_regularization : float, default=0\n",
      "     |      The L2 regularization parameter penalizing leaves with small hessians.\n",
      "     |      Use ``0`` for no regularization (default).\n",
      "     |  max_features : float, default=1.0\n",
      "     |      Proportion of randomly chosen features in each and every node split.\n",
      "     |      This is a form of regularization, smaller values make the trees weaker\n",
      "     |      learners and might prevent overfitting.\n",
      "     |      If interaction constraints from `interaction_cst` are present, only allowed\n",
      "     |      features are taken into account for the subsampling.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  max_bins : int, default=255\n",
      "     |      The maximum number of bins to use for non-missing values. Before\n",
      "     |      training, each feature of the input array `X` is binned into\n",
      "     |      integer-valued bins, which allows for a much faster training stage.\n",
      "     |      Features with a small number of unique values may use less than\n",
      "     |      ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin\n",
      "     |      is always reserved for missing values. Must be no larger than 255.\n",
      "     |  categorical_features : array-like of {bool, int, str} of shape (n_features)             or shape (n_categorical_features,), default='from_dtype'\n",
      "     |      Indicates the categorical features.\n",
      "     |  \n",
      "     |      - None : no feature will be considered categorical.\n",
      "     |      - boolean array-like : boolean mask indicating categorical features.\n",
      "     |      - integer array-like : integer indices indicating categorical\n",
      "     |        features.\n",
      "     |      - str array-like: names of categorical features (assuming the training\n",
      "     |        data has feature names).\n",
      "     |      - `\"from_dtype\"`: dataframe columns with dtype \"category\" are\n",
      "     |        considered to be categorical features. The input must be an object\n",
      "     |        exposing a ``__dataframe__`` method such as pandas or polars\n",
      "     |        DataFrames to use this feature.\n",
      "     |  \n",
      "     |      For each categorical feature, there must be at most `max_bins` unique\n",
      "     |      categories. Negative values for categorical features encoded as numeric\n",
      "     |      dtypes are treated as missing values. All categorical values are\n",
      "     |      converted to floating point numbers. This means that categorical values\n",
      "     |      of 1.0 and 1 are treated as the same category.\n",
      "     |  \n",
      "     |      Read more in the :ref:`User Guide <categorical_support_gbdt>` and\n",
      "     |      :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_categorical.py`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.2\n",
      "     |         Added support for feature names.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.4\n",
      "     |         Added `\"from_dtype\"` option.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.6\n",
      "     |         The default value changed from `None` to `\"from_dtype\"`.\n",
      "     |  \n",
      "     |  monotonic_cst : array-like of int of shape (n_features) or dict, default=None\n",
      "     |      Monotonic constraint to enforce on each feature are specified using the\n",
      "     |      following integer values:\n",
      "     |  \n",
      "     |      - 1: monotonic increase\n",
      "     |      - 0: no constraint\n",
      "     |      - -1: monotonic decrease\n",
      "     |  \n",
      "     |      If a dict with str keys, map feature to monotonic constraints by name.\n",
      "     |      If an array, the features are mapped to constraints by position. See\n",
      "     |      :ref:`monotonic_cst_features_names` for a usage example.\n",
      "     |  \n",
      "     |      Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.2\n",
      "     |         Accept dict of constraints with feature names as keys.\n",
      "     |  \n",
      "     |  interaction_cst : {\"pairwise\", \"no_interactions\"} or sequence of lists/tuples/sets             of int, default=None\n",
      "     |      Specify interaction constraints, the sets of features which can\n",
      "     |      interact with each other in child node splits.\n",
      "     |  \n",
      "     |      Each item specifies the set of feature indices that are allowed\n",
      "     |      to interact with each other. If there are more features than\n",
      "     |      specified in these constraints, they are treated as if they were\n",
      "     |      specified as an additional set.\n",
      "     |  \n",
      "     |      The strings \"pairwise\" and \"no_interactions\" are shorthands for\n",
      "     |      allowing only pairwise or no interactions, respectively.\n",
      "     |  \n",
      "     |      For instance, with 5 features in total, `interaction_cst=[{0, 1}]`\n",
      "     |      is equivalent to `interaction_cst=[{0, 1}, {2, 3, 4}]`,\n",
      "     |      and specifies that each branch of a tree will either only split\n",
      "     |      on features 0 and 1 or only split on features 2, 3 and 4.\n",
      "     |  \n",
      "     |      See :ref:`this example<ice-vs-pdp>` on how to use `interaction_cst`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble. For results to be valid, the\n",
      "     |      estimator should be re-trained on the same data only.\n",
      "     |      See :term:`the Glossary <warm_start>`.\n",
      "     |  early_stopping : 'auto' or bool, default='auto'\n",
      "     |      If 'auto', early stopping is enabled if the sample size is larger than\n",
      "     |      10000 or if `X_val` and `y_val` are passed to `fit`. If True, early stopping\n",
      "     |      is enabled, otherwise early stopping is disabled.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  scoring : str or callable or None, default='loss'\n",
      "     |      Scoring method to use for early stopping. Only used if `early_stopping`\n",
      "     |      is enabled. Options:\n",
      "     |  \n",
      "     |      - str: see :ref:`scoring_string_names` for options.\n",
      "     |      - callable: a scorer callable object (e.g., function) with signature\n",
      "     |        ``scorer(estimator, X, y)``. See :ref:`scoring_callable` for details.\n",
      "     |      - `None`: the :ref:`coefficient of determination <r2_score>`\n",
      "     |        (:math:`R^2`) is used.\n",
      "     |      - 'loss': early stopping is checked w.r.t the loss value.\n",
      "     |  \n",
      "     |  validation_fraction : int or float or None, default=0.1\n",
      "     |      Proportion (or absolute size) of training data to set aside as\n",
      "     |      validation data for early stopping. If None, early stopping is done on\n",
      "     |      the training data.\n",
      "     |      The value is ignored if either early stopping is not performed, e.g.\n",
      "     |      `early_stopping=False`, or if `X_val` and `y_val` are passed to fit.\n",
      "     |  n_iter_no_change : int, default=10\n",
      "     |      Used to determine when to \"early stop\". The fitting process is\n",
      "     |      stopped when none of the last ``n_iter_no_change`` scores are better\n",
      "     |      than the ``n_iter_no_change - 1`` -th-to-last one, up to some\n",
      "     |      tolerance. Only used if early stopping is performed.\n",
      "     |  tol : float, default=1e-7\n",
      "     |      The absolute tolerance to use when comparing scores during early\n",
      "     |      stopping. The higher the tolerance, the more likely we are to early\n",
      "     |      stop: higher tolerance means that it will be harder for subsequent\n",
      "     |      iterations to be considered an improvement upon the reference score.\n",
      "     |  verbose : int, default=0\n",
      "     |      The verbosity level. If not zero, print some information about the\n",
      "     |      fitting process. ``1`` prints only summary info, ``2`` prints info per\n",
      "     |      iteration.\n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Pseudo-random number generator to control the subsampling in the\n",
      "     |      binning process, and the train/validation data split if early stopping\n",
      "     |      is enabled.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  do_early_stopping_ : bool\n",
      "     |      Indicates whether early stopping is used during training.\n",
      "     |  n_iter_ : int\n",
      "     |      The number of iterations as selected by early stopping, depending on\n",
      "     |      the `early_stopping` parameter. Otherwise it corresponds to max_iter.\n",
      "     |  n_trees_per_iteration_ : int\n",
      "     |      The number of tree that are built at each iteration. For regressors,\n",
      "     |      this is always 1.\n",
      "     |  train_score_ : ndarray, shape (n_iter_+1,)\n",
      "     |      The scores at each iteration on the training data. The first entry\n",
      "     |      is the score of the ensemble before the first iteration. Scores are\n",
      "     |      computed according to the ``scoring`` parameter. If ``scoring`` is\n",
      "     |      not 'loss', scores are computed on a subset of at most 10 000\n",
      "     |      samples. Empty if no early stopping.\n",
      "     |  validation_score_ : ndarray, shape (n_iter_+1,)\n",
      "     |      The scores at each iteration on the held-out validation data. The\n",
      "     |      first entry is the score of the ensemble before the first iteration.\n",
      "     |      Scores are computed according to the ``scoring`` parameter. Empty if\n",
      "     |      no early stopping or if ``validation_fraction`` is None.\n",
      "     |  is_categorical_ : ndarray, shape (n_features, ) or None\n",
      "     |      Boolean mask for the categorical features. ``None`` if there are no\n",
      "     |      categorical features.\n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  GradientBoostingRegressor : Exact gradient boosting method that does not\n",
      "     |      scale as good on datasets with a large number of samples.\n",
      "     |  sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
      "     |  RandomForestRegressor : A meta-estimator that fits a number of decision\n",
      "     |      tree regressors on various sub-samples of the dataset and uses\n",
      "     |      averaging to improve the statistical performance and control\n",
      "     |      over-fitting.\n",
      "     |  AdaBoostRegressor : A meta-estimator that begins by fitting a regressor\n",
      "     |      on the original dataset and then fits additional copies of the\n",
      "     |      regressor on the same dataset but where the weights of instances are\n",
      "     |      adjusted according to the error of the current prediction. As such,\n",
      "     |      subsequent regressors focus more on difficult cases.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import HistGradientBoostingRegressor\n",
      "     |  >>> from sklearn.datasets import load_diabetes\n",
      "     |  >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |  >>> est = HistGradientBoostingRegressor().fit(X, y)\n",
      "     |  >>> est.score(X, y)\n",
      "     |  0.92...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HistGradientBoostingRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseHistGradientBoosting\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loss='squared_error', *, quantile=None, learning_rate=0.1, max_iter=100, max_leaf_nodes=31, max_depth=None, min_samples_leaf=20, l2_regularization=0.0, max_features=1.0, max_bins=255, categorical_features='from_dtype', monotonic_cst=None, interaction_cst=None, warm_start=False, early_stopping='auto', scoring='loss', validation_fraction=0.1, n_iter_no_change=10, tol=1e-07, verbose=0, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict values for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray, shape (n_samples,)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor, *, X_val: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight_val: Union[bool, NoneType, str] = '$UNCHANGED$', y_val: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X_val : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``X_val`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      sample_weight_val : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight_val`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      y_val : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``y_val`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Predict regression target for each iteration.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.24\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      y : generator of ndarray of shape (n_samples,)\n",
      "     |          The predicted values of the input samples, for each iteration.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseHistGradientBoosting:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, *, X_val=None, y_val=None, sample_weight_val=None)\n",
      "     |      Fit the gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,) default=None\n",
      "     |          Weights of training data.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.23\n",
      "     |      \n",
      "     |      X_val : array-like of shape (n_val, n_features)\n",
      "     |          Additional sample of features for validation used in early stopping.\n",
      "     |          In a `Pipeline`, `X_val` can be transformed the same way as `X` with\n",
      "     |          `Pipeline(..., transform_input=[\"X_val\"])`.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |      y_val : array-like of shape (n_samples,)\n",
      "     |          Additional sample of target values for validation used in early stopping.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |      sample_weight_val : array-like of shape (n_samples,) default=None\n",
      "     |          Additional weights for validation used in early stopping.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.7\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseHistGradientBoosting:\n",
      "     |  \n",
      "     |  n_iter_\n",
      "     |      Number of iterations of the boosting process.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class IsolationForest(sklearn.base.OutlierMixin, sklearn.ensemble._bagging.BaseBagging)\n",
      "     |  IsolationForest(*, n_estimators=100, max_samples='auto', contamination='auto', max_features=1.0, bootstrap=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      "     |  \n",
      "     |  Isolation Forest Algorithm.\n",
      "     |  \n",
      "     |  Return the anomaly score of each sample using the IsolationForest algorithm\n",
      "     |  \n",
      "     |  The IsolationForest 'isolates' observations by randomly selecting a feature\n",
      "     |  and then randomly selecting a split value between the maximum and minimum\n",
      "     |  values of the selected feature.\n",
      "     |  \n",
      "     |  Since recursive partitioning can be represented by a tree structure, the\n",
      "     |  number of splittings required to isolate a sample is equivalent to the path\n",
      "     |  length from the root node to the terminating node.\n",
      "     |  \n",
      "     |  This path length, averaged over a forest of such random trees, is a\n",
      "     |  measure of normality and our decision function.\n",
      "     |  \n",
      "     |  Random partitioning produces noticeably shorter paths for anomalies.\n",
      "     |  Hence, when a forest of random trees collectively produce shorter path\n",
      "     |  lengths for particular samples, they are highly likely to be anomalies.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <isolation_forest>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : int, default=100\n",
      "     |      The number of base estimators in the ensemble.\n",
      "     |  \n",
      "     |  max_samples : \"auto\", int or float, default=\"auto\"\n",
      "     |      The number of samples to draw from X to train each base estimator.\n",
      "     |  \n",
      "     |      - If int, then draw `max_samples` samples.\n",
      "     |      - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "     |      - If \"auto\", then `max_samples=min(256, n_samples)`.\n",
      "     |  \n",
      "     |      If max_samples is larger than the number of samples provided,\n",
      "     |      all samples will be used for all trees (no sampling).\n",
      "     |  \n",
      "     |  contamination : 'auto' or float, default='auto'\n",
      "     |      The amount of contamination of the data set, i.e. the proportion\n",
      "     |      of outliers in the data set. Used when fitting to define the threshold\n",
      "     |      on the scores of the samples.\n",
      "     |  \n",
      "     |      - If 'auto', the threshold is determined as in the\n",
      "     |        original paper.\n",
      "     |      - If float, the contamination should be in the range (0, 0.5].\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``contamination`` changed from 0.1\n",
      "     |         to ``'auto'``.\n",
      "     |  \n",
      "     |  max_features : int or float, default=1.0\n",
      "     |      The number of features to draw from X to train each base estimator.\n",
      "     |  \n",
      "     |      - If int, then draw `max_features` features.\n",
      "     |      - If float, then draw `max(1, int(max_features * n_features_in_))` features.\n",
      "     |  \n",
      "     |      Note: using a float number less than 1.0 or integer less than number of\n",
      "     |      features will enable feature subsampling and leads to a longer runtime.\n",
      "     |  \n",
      "     |  bootstrap : bool, default=False\n",
      "     |      If True, individual trees are fit on random subsets of the training\n",
      "     |      data sampled with replacement. If False, sampling without replacement\n",
      "     |      is performed.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel for :meth:`fit`. ``None`` means 1\n",
      "     |      unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using\n",
      "     |      all processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the pseudo-randomness of the selection of the feature\n",
      "     |      and split values for each branching step and each tree in the forest.\n",
      "     |  \n",
      "     |      Pass an int for reproducible results across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity of the tree building process.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.21\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance\n",
      "     |      The child estimator template used to create the collection of\n",
      "     |      fitted sub-estimators.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |         `base_estimator_` was renamed to `estimator_`.\n",
      "     |  \n",
      "     |  estimators_ : list of ExtraTreeRegressor instances\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  estimators_features_ : list of ndarray\n",
      "     |      The subset of drawn features for each base estimator.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of ndarray\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator.\n",
      "     |  \n",
      "     |  max_samples_ : int\n",
      "     |      The actual number of samples.\n",
      "     |  \n",
      "     |  offset_ : float\n",
      "     |      Offset used to define the decision function from the raw scores. We\n",
      "     |      have the relation: ``decision_function = score_samples - offset_``.\n",
      "     |      ``offset_`` is defined as follows. When the contamination parameter is\n",
      "     |      set to \"auto\", the offset is equal to -0.5 as the scores of inliers are\n",
      "     |      close to 0 and the scores of outliers are close to -1. When a\n",
      "     |      contamination parameter different than \"auto\" is provided, the offset\n",
      "     |      is defined in such a way we obtain the expected number of outliers\n",
      "     |      (samples with decision function < 0) in training.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.covariance.EllipticEnvelope : An object for detecting outliers in a\n",
      "     |      Gaussian distributed dataset.\n",
      "     |  sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\n",
      "     |      Estimate the support of a high-dimensional distribution.\n",
      "     |      The implementation is based on libsvm.\n",
      "     |  sklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection\n",
      "     |      using Local Outlier Factor (LOF).\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The implementation is based on an ensemble of ExtraTreeRegressor. The\n",
      "     |  maximum depth of each tree is set to ``ceil(log_2(n))`` where\n",
      "     |  :math:`n` is the number of samples used to build the tree\n",
      "     |  (see (Liu et al., 2008) for more details).\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n",
      "     |         Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n",
      "     |  .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n",
      "     |         anomaly detection.\" ACM Transactions on Knowledge Discovery from\n",
      "     |         Data (TKDD) 6.1 (2012): 3.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import IsolationForest\n",
      "     |  >>> X = [[-1.1], [0.3], [0.5], [100]]\n",
      "     |  >>> clf = IsolationForest(random_state=0).fit(X)\n",
      "     |  >>> clf.predict([[0.1], [0], [90]])\n",
      "     |  array([ 1,  1, -1])\n",
      "     |  \n",
      "     |  For an example of using isolation forest for anomaly detection see\n",
      "     |  :ref:`sphx_glr_auto_examples_ensemble_plot_isolation_forest.py`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IsolationForest\n",
      "     |      sklearn.base.OutlierMixin\n",
      "     |      sklearn.ensemble._bagging.BaseBagging\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_estimators=100, max_samples='auto', contamination='auto', max_features=1.0, bootstrap=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Average anomaly score of X of the base classifiers.\n",
      "     |      \n",
      "     |      The anomaly score of an input sample is computed as\n",
      "     |      the mean anomaly score of the trees in the forest.\n",
      "     |      \n",
      "     |      The measure of normality of an observation given a tree is the depth\n",
      "     |      of the leaf containing this observation, which is equivalent to\n",
      "     |      the number of splittings required to isolate this point. In case of\n",
      "     |      several observations n_left in the leaf, the average path length of\n",
      "     |      a n_left samples isolation tree is added.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,)\n",
      "     |          The anomaly score of the input samples.\n",
      "     |          The lower, the more abnormal. Negative scores represent outliers,\n",
      "     |          positive scores represent inliers.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The decision_function method can be parallelized by setting a joblib context.\n",
      "     |      This inherently does NOT use the ``n_jobs`` parameter initialized in the class,\n",
      "     |      which is used during ``fit``. This is because, calculating the score may\n",
      "     |      actually be faster without parallelization for a small number of samples,\n",
      "     |      such as for 1000 samples or less.\n",
      "     |      The user can set the number of jobs in the joblib context to control the\n",
      "     |      number of parallel jobs.\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          from joblib import parallel_backend\n",
      "     |      \n",
      "     |          # Note, we use threading here as the decision_function method is\n",
      "     |          # not CPU bound.\n",
      "     |          with parallel_backend(\"threading\", n_jobs=4):\n",
      "     |              model.decision_function(X)\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Fit estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Use ``dtype=np.float32`` for maximum\n",
      "     |          efficiency. Sparse matrices are also supported, use sparse\n",
      "     |          ``csc_matrix`` for maximum efficiency.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict if a particular sample is an outlier or not.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      is_inlier : ndarray of shape (n_samples,)\n",
      "     |          For each observation, tells whether or not (+1 or -1) it should\n",
      "     |          be considered as an inlier according to the fitted model.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The predict method can be parallelized by setting a joblib context. This\n",
      "     |      inherently does NOT use the ``n_jobs`` parameter initialized in the class,\n",
      "     |      which is used during ``fit``. This is because, predict may actually be faster\n",
      "     |      without parallelization for a small number of samples,\n",
      "     |      such as for 1000 samples or less. The user can set the\n",
      "     |      number of jobs in the joblib context to control the number of parallel jobs.\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          from joblib import parallel_backend\n",
      "     |      \n",
      "     |          # Note, we use threading here as the predict method is not CPU bound.\n",
      "     |          with parallel_backend(\"threading\", n_jobs=4):\n",
      "     |              model.predict(X)\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Opposite of the anomaly score defined in the original paper.\n",
      "     |      \n",
      "     |      The anomaly score of an input sample is computed as\n",
      "     |      the mean anomaly score of the trees in the forest.\n",
      "     |      \n",
      "     |      The measure of normality of an observation given a tree is the depth\n",
      "     |      of the leaf containing this observation, which is equivalent to\n",
      "     |      the number of splittings required to isolate this point. In case of\n",
      "     |      several observations n_left in the leaf, the average path length of\n",
      "     |      a n_left samples isolation tree is added.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,)\n",
      "     |          The anomaly score of the input samples.\n",
      "     |          The lower, the more abnormal.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The score function method can be parallelized by setting a joblib context. This\n",
      "     |      inherently does NOT use the ``n_jobs`` parameter initialized in the class,\n",
      "     |      which is used during ``fit``. This is because, calculating the score may\n",
      "     |      actually be faster without parallelization for a small number of samples,\n",
      "     |      such as for 1000 samples or less.\n",
      "     |      The user can set the number of jobs in the joblib context to control the\n",
      "     |      number of parallel jobs.\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          from joblib import parallel_backend\n",
      "     |      \n",
      "     |          # Note, we use threading here as the score_samples method is not CPU bound.\n",
      "     |          with parallel_backend(\"threading\", n_jobs=4):\n",
      "     |              model.score(X)\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.ensemble._iforest.IsolationForest, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._iforest.IsolationForest from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.OutlierMixin:\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None, **kwargs)\n",
      "     |      Perform fit on X and returns labels for X.\n",
      "     |      \n",
      "     |      Returns -1 for outliers and 1 for inliers.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Arguments to be passed to ``fit``.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          1 for inliers, -1 for outliers.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.OutlierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._bagging.BaseBagging:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.ensemble._bagging.BaseBagging:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of indices identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class RandomForestClassifier(ForestClassifier)\n",
      "     |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
      "     |  \n",
      "     |  A random forest classifier.\n",
      "     |  \n",
      "     |  A random forest is a meta estimator that fits a number of decision tree\n",
      "     |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      "     |  improve the predictive accuracy and control over-fitting.\n",
      "     |  Trees in the forest use the best split strategy, i.e. equivalent to passing\n",
      "     |  `splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeClassifier`.\n",
      "     |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      "     |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      "     |  each tree.\n",
      "     |  \n",
      "     |  For a comparison between tree-based ensemble models see the example\n",
      "     |  :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n",
      "     |  \n",
      "     |  This estimator has native support for missing values (NaNs). During training,\n",
      "     |  the tree grower learns at each split point whether samples with missing values\n",
      "     |  should go to the left or right child, based on the potential gain. When predicting,\n",
      "     |  samples with missing values are assigned to the left or right child consequently.\n",
      "     |  If no missing values were encountered for a given feature during training, then\n",
      "     |  samples with missing values are mapped to whichever child has the most samples.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : int, default=100\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``n_estimators`` changed from 10 to 100\n",
      "     |         in 0.22.\n",
      "     |  \n",
      "     |  criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
      "     |      The function to measure the quality of a split. Supported criteria are\n",
      "     |      \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
      "     |      Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
      "     |      Note: This parameter is tree-specific.\n",
      "     |  \n",
      "     |  max_depth : int, default=None\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `max(1, int(max_features * n_features_in_))` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.1\n",
      "     |          The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  bootstrap : bool, default=True\n",
      "     |      Whether bootstrap samples are used when building trees. If False, the\n",
      "     |      whole dataset is used to build each tree.\n",
      "     |  \n",
      "     |  oob_score : bool or callable, default=False\n",
      "     |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      "     |      By default, :func:`~sklearn.metrics.accuracy_score` is used.\n",
      "     |      Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
      "     |      custom metric. Only available if `bootstrap=True`.\n",
      "     |  \n",
      "     |      For an illustration of out-of-bag (OOB) error estimation, see the example\n",
      "     |      :ref:`sphx_glr_auto_examples_ensemble_plot_ensemble_oob.py`.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      "     |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      "     |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "     |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      "     |      <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls both the randomness of the bootstrapping of the samples used\n",
      "     |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      "     |      features to consider when looking for the best split at each node\n",
      "     |      (if ``max_features < n_features``).\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`Glossary <warm_start>` and\n",
      "     |      :ref:`tree_ensemble_warm_start` for details.\n",
      "     |  \n",
      "     |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one. For\n",
      "     |      multi-output problems, a list of dicts can be provided in the same\n",
      "     |      order as the columns of y.\n",
      "     |  \n",
      "     |      Note that for multioutput (including multilabel) weights should be\n",
      "     |      defined for each class of every column in its own dict. For example,\n",
      "     |      for four-class multilabel classification weights should be\n",
      "     |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "     |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      "     |      weights are computed based on the bootstrap sample for every tree\n",
      "     |      grown.\n",
      "     |  \n",
      "     |      For multi-output, the weights of each column of y will be multiplied.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "     |      :ref:`minimal_cost_complexity_pruning` for details. See\n",
      "     |      :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n",
      "     |      for an example of such pruning.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  max_samples : int or float, default=None\n",
      "     |      If bootstrap is True, the number of samples to draw from X\n",
      "     |      to train each base estimator.\n",
      "     |  \n",
      "     |      - If None (default), then draw `X.shape[0]` samples.\n",
      "     |      - If int, then draw `max_samples` samples.\n",
      "     |      - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n",
      "     |        `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  monotonic_cst : array-like of int of shape (n_features), default=None\n",
      "     |      Indicates the monotonicity constraint to enforce on each feature.\n",
      "     |        - 1: monotonic increase\n",
      "     |        - 0: no constraint\n",
      "     |        - -1: monotonic decrease\n",
      "     |  \n",
      "     |      If monotonic_cst is None, no constraints are applied.\n",
      "     |  \n",
      "     |      Monotonicity constraints are not supported for:\n",
      "     |        - multiclass classifications (i.e. when `n_classes > 2`),\n",
      "     |        - multioutput classifications (i.e. when `n_outputs_ > 1`),\n",
      "     |        - classifications trained on data with missing values.\n",
      "     |  \n",
      "     |      The constraints hold over the probability of the positive class.\n",
      "     |  \n",
      "     |      Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n",
      "     |      The child estimator template used to create the collection of fitted\n",
      "     |      sub-estimators.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |         `base_estimator_` was renamed to `estimator_`.\n",
      "     |  \n",
      "     |  estimators_ : list of DecisionTreeClassifier\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      "     |      The classes labels (single output problem), or a list of arrays of\n",
      "     |      class labels (multi-output problem).\n",
      "     |  \n",
      "     |  n_classes_ : int or list\n",
      "     |      The number of classes (single output problem), or a list containing the\n",
      "     |      number of classes for each output (multi-output problem).\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
      "     |      Decision function computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      "     |      only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of arrays\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator. Each subset is defined by an array of the indices selected.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      "     |  sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
      "     |      tree classifiers.\n",
      "     |  sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n",
      "     |      Boosting Classification Tree, very fast for big datasets (n_samples >=\n",
      "     |      10_000).\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data,\n",
      "     |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "     |  of the criterion is identical for several splits enumerated during the\n",
      "     |  search of the best split. To obtain a deterministic behaviour during\n",
      "     |  fitting, ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      "     |  ...                            n_informative=2, n_redundant=0,\n",
      "     |  ...                            random_state=0, shuffle=False)\n",
      "     |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  RandomForestClassifier(...)\n",
      "     |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomForestClassifier\n",
      "     |      ForestClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseForest\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.ensemble._forest.RandomForestClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.RandomForestClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._forest.RandomForestClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.RandomForestClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestClassifier:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is a vote by the trees in\n",
      "     |      the forest, weighted by their probability estimates. That is,\n",
      "     |      the predicted class is the one with highest mean probability\n",
      "     |      estimate across the trees.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the log of the mean predicted class probabilities of the trees in the\n",
      "     |      forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample are computed as\n",
      "     |      the mean predicted class probabilities of the trees in the forest.\n",
      "     |      The class probability of a single tree is the fraction of samples of\n",
      "     |      the same class in a leaf.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      "     |          Return a node indicator matrix where non zero elements indicates\n",
      "     |          that the samples goes through the nodes. The matrix is of CSR\n",
      "     |          format.\n",
      "     |      \n",
      "     |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, its dtype will be converted\n",
      "     |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseForest:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of indices identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class RandomForestRegressor(ForestRegressor)\n",
      "     |  RandomForestRegressor(n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
      "     |  \n",
      "     |  A random forest regressor.\n",
      "     |  \n",
      "     |  A random forest is a meta estimator that fits a number of decision tree\n",
      "     |  regressors on various sub-samples of the dataset and uses averaging to\n",
      "     |  improve the predictive accuracy and control over-fitting.\n",
      "     |  Trees in the forest use the best split strategy, i.e. equivalent to passing\n",
      "     |  `splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`.\n",
      "     |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      "     |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      "     |  each tree.\n",
      "     |  \n",
      "     |  This estimator has native support for missing values (NaNs). During training,\n",
      "     |  the tree grower learns at each split point whether samples with missing values\n",
      "     |  should go to the left or right child, based on the potential gain. When predicting,\n",
      "     |  samples with missing values are assigned to the left or right child consequently.\n",
      "     |  If no missing values were encountered for a given feature during training, then\n",
      "     |  samples with missing values are mapped to whichever child has the most samples.\n",
      "     |  \n",
      "     |  For a comparison between tree-based ensemble models see the example\n",
      "     |  :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : int, default=100\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``n_estimators`` changed from 10 to 100\n",
      "     |         in 0.22.\n",
      "     |  \n",
      "     |  criterion : {\"squared_error\", \"absolute_error\", \"friedman_mse\", \"poisson\"},             default=\"squared_error\"\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"squared_error\" for the mean squared error, which is equal to\n",
      "     |      variance reduction as feature selection criterion and minimizes the L2\n",
      "     |      loss using the mean of each terminal node, \"friedman_mse\", which uses\n",
      "     |      mean squared error with Friedman's improvement score for potential\n",
      "     |      splits, \"absolute_error\" for the mean absolute error, which minimizes\n",
      "     |      the L1 loss using the median of each terminal node, and \"poisson\" which\n",
      "     |      uses reduction in Poisson deviance to find splits.\n",
      "     |      Training using \"absolute_error\" is significantly slower\n",
      "     |      than when using \"squared_error\".\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Mean Absolute Error (MAE) criterion.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |         Poisson criterion.\n",
      "     |  \n",
      "     |  max_depth : int, default=None\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `max(1, int(max_features * n_features_in_))` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None or 1.0, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      .. note::\n",
      "     |          The default of 1.0 is equivalent to bagged trees and more\n",
      "     |          randomness can be achieved by setting smaller values, e.g. 0.3.\n",
      "     |  \n",
      "     |      .. versionchanged:: 1.1\n",
      "     |          The default of `max_features` changed from `\"auto\"` to 1.0.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  bootstrap : bool, default=True\n",
      "     |      Whether bootstrap samples are used when building trees. If False, the\n",
      "     |      whole dataset is used to build each tree.\n",
      "     |  \n",
      "     |  oob_score : bool or callable, default=False\n",
      "     |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      "     |      By default, :func:`~sklearn.metrics.r2_score` is used.\n",
      "     |      Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
      "     |      custom metric. Only available if `bootstrap=True`.\n",
      "     |  \n",
      "     |      For an illustration of out-of-bag (OOB) error estimation, see the example\n",
      "     |      :ref:`sphx_glr_auto_examples_ensemble_plot_ensemble_oob.py`.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      "     |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      "     |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "     |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      "     |      <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls both the randomness of the bootstrapping of the samples used\n",
      "     |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      "     |      features to consider when looking for the best split at each node\n",
      "     |      (if ``max_features < n_features``).\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`Glossary <warm_start>` and\n",
      "     |      :ref:`tree_ensemble_warm_start` for details.\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "     |      :ref:`minimal_cost_complexity_pruning` for details. See\n",
      "     |      :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n",
      "     |      for an example of such pruning.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  max_samples : int or float, default=None\n",
      "     |      If bootstrap is True, the number of samples to draw from X\n",
      "     |      to train each base estimator.\n",
      "     |  \n",
      "     |      - If None (default), then draw `X.shape[0]` samples.\n",
      "     |      - If int, then draw `max_samples` samples.\n",
      "     |      - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n",
      "     |        `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  monotonic_cst : array-like of int of shape (n_features), default=None\n",
      "     |      Indicates the monotonicity constraint to enforce on each feature.\n",
      "     |        - 1: monotonically increasing\n",
      "     |        - 0: no constraint\n",
      "     |        - -1: monotonically decreasing\n",
      "     |  \n",
      "     |      If monotonic_cst is None, no constraints are applied.\n",
      "     |  \n",
      "     |      Monotonicity constraints are not supported for:\n",
      "     |        - multioutput regressions (i.e. when `n_outputs_ > 1`),\n",
      "     |        - regressions trained on data with missing values.\n",
      "     |  \n",
      "     |      Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : :class:`~sklearn.tree.DecisionTreeRegressor`\n",
      "     |      The child estimator template used to create the collection of fitted\n",
      "     |      sub-estimators.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |         `base_estimator_` was renamed to `estimator_`.\n",
      "     |  \n",
      "     |  estimators_ : list of DecisionTreeRegressor\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |      Prediction computed with out-of-bag estimate on the training set.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of arrays\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator. Each subset is defined by an array of the indices selected.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
      "     |  sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n",
      "     |      tree regressors.\n",
      "     |  sklearn.ensemble.HistGradientBoostingRegressor : A Histogram-based Gradient\n",
      "     |      Boosting Regression Tree, very fast for big datasets (n_samples >=\n",
      "     |      10_000).\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data,\n",
      "     |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "     |  of the criterion is identical for several splits enumerated during the\n",
      "     |  search of the best split. To obtain a deterministic behaviour during\n",
      "     |  fitting, ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  The default value ``max_features=1.0`` uses ``n_features``\n",
      "     |  rather than ``n_features / 3``. The latter was originally suggested in\n",
      "     |  [1], whereas the former was more recently justified empirically in [2].\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "     |  \n",
      "     |  .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      "     |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      "     |  ...                        random_state=0, shuffle=False)\n",
      "     |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  RandomForestRegressor(...)\n",
      "     |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "     |  [-8.32987858]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomForestRegressor\n",
      "     |      ForestRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseForest\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.ensemble._forest.RandomForestRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.RandomForestRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._forest.RandomForestRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.RandomForestRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestRegressor:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      The predicted regression target of an input sample is computed as the\n",
      "     |      mean predicted regression targets of the trees in the forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      "     |          Return a node indicator matrix where non zero elements indicates\n",
      "     |          that the samples goes through the nodes. The matrix is of CSR\n",
      "     |          format.\n",
      "     |      \n",
      "     |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, its dtype will be converted\n",
      "     |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseForest:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of indices identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  __init_subclass__(**kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "    \n",
      "    class RandomTreesEmbedding(sklearn.base.TransformerMixin, BaseForest)\n",
      "     |  RandomTreesEmbedding(n_estimators=100, *, max_depth=5, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_leaf_nodes=None, min_impurity_decrease=0.0, sparse_output=True, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      "     |  \n",
      "     |  An ensemble of totally random trees.\n",
      "     |  \n",
      "     |  An unsupervised transformation of a dataset to a high-dimensional\n",
      "     |  sparse representation. A datapoint is coded according to which leaf of\n",
      "     |  each tree it is sorted into. Using a one-hot encoding of the leaves,\n",
      "     |  this leads to a binary coding with as many ones as there are trees in\n",
      "     |  the forest.\n",
      "     |  \n",
      "     |  The dimensionality of the resulting representation is\n",
      "     |  ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,\n",
      "     |  the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.\n",
      "     |  \n",
      "     |  For an example of applying Random Trees Embedding to non-linear\n",
      "     |  classification, see\n",
      "     |  :ref:`sphx_glr_auto_examples_ensemble_plot_random_forest_embedding.py`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <random_trees_embedding>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : int, default=100\n",
      "     |      Number of trees in the forest.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``n_estimators`` changed from 10 to 100\n",
      "     |         in 0.22.\n",
      "     |  \n",
      "     |  max_depth : int, default=5\n",
      "     |      The maximum depth of each tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` is the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` is the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  sparse_output : bool, default=True\n",
      "     |      Whether or not to return a sparse CSR matrix, as default behavior,\n",
      "     |      or to return a dense array compatible with dense pipeline operators.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`,\n",
      "     |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      "     |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "     |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      "     |      <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the generation of the random `y` used to fit the trees\n",
      "     |      and the draw of the splits for each feature at the trees' nodes.\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`Glossary <warm_start>` and\n",
      "     |      :ref:`tree_ensemble_warm_start` for details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : :class:`~sklearn.tree.ExtraTreeRegressor` instance\n",
      "     |      The child estimator template used to create the collection of fitted\n",
      "     |      sub-estimators.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2\n",
      "     |         `base_estimator_` was renamed to `estimator_`.\n",
      "     |  \n",
      "     |  estimators_ : list of :class:`~sklearn.tree.ExtraTreeRegressor` instances\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The feature importances (the higher, the more important the feature).\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      "     |      has feature names that are all strings.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  one_hot_encoder_ : OneHotEncoder instance\n",
      "     |      One-hot encoder used to create the sparse embedding.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of arrays\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator. Each subset is defined by an array of the indices selected.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.4\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  ExtraTreesClassifier : An extra-trees classifier.\n",
      "     |  ExtraTreesRegressor : An extra-trees regressor.\n",
      "     |  RandomForestClassifier : A random forest classifier.\n",
      "     |  RandomForestRegressor : A random forest regressor.\n",
      "     |  sklearn.tree.ExtraTreeClassifier: An extremely randomized\n",
      "     |      tree classifier.\n",
      "     |  sklearn.tree.ExtraTreeRegressor : An extremely randomized\n",
      "     |      tree regressor.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "     |         Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  \"Fast discriminative\n",
      "     |         visual codebooks using randomized clustering forests\"\n",
      "     |         NIPS 2007\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import RandomTreesEmbedding\n",
      "     |  >>> X = [[0,0], [1,0], [0,1], [-1,0], [0,-1]]\n",
      "     |  >>> random_trees = RandomTreesEmbedding(\n",
      "     |  ...    n_estimators=5, random_state=0, max_depth=1).fit(X)\n",
      "     |  >>> X_sparse_embedding = random_trees.transform(X)\n",
      "     |  >>> X_sparse_embedding.toarray()\n",
      "     |  array([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n",
      "     |         [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n",
      "     |         [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
      "     |         [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],\n",
      "     |         [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomTreesEmbedding\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.utils._set_output._SetOutputMixin\n",
      "     |      BaseForest\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=100, *, max_depth=5, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_leaf_nodes=None, min_impurity_decrease=0.0, sparse_output=True, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Fit estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Use ``dtype=np.float32`` for maximum\n",
      "     |          efficiency. Sparse matrices are also supported, use sparse\n",
      "     |          ``csc_matrix`` for maximum efficiency.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, sample_weight=None)\n",
      "     |      Fit estimator and transform dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data used to build forests. Use ``dtype=np.float32`` for\n",
      "     |          maximum efficiency.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_transformed : sparse matrix of shape (n_samples, n_out)\n",
      "     |          Transformed dataset.\n",
      "     |  \n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Only used to validate feature names with the names seen in :meth:`fit`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Transformed feature names, in the format of\n",
      "     |          `randomtreesembedding_{tree}_{leaf}`, where `tree` is the tree used\n",
      "     |          to generate the leaf and `leaf` is the index of a leaf node\n",
      "     |          in that tree. Note that the node indexing scheme is used to\n",
      "     |          index both nodes with children (split nodes) and leaf nodes.\n",
      "     |          Only the latter can be present as output features.\n",
      "     |          As a consequence, there are missing indices in the output\n",
      "     |          feature names.\n",
      "     |  \n",
      "     |  set_fit_request(self: sklearn.ensemble._forest.RandomTreesEmbedding, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.RandomTreesEmbedding from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``fit`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data to be transformed. Use ``dtype=np.float32`` for maximum\n",
      "     |          efficiency. Sparse matrices are also supported, use sparse\n",
      "     |          ``csr_matrix`` for maximum efficiency.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_transformed : sparse matrix of shape (n_samples, n_out)\n",
      "     |          Transformed dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  criterion = 'squared_error'\n",
      "     |  \n",
      "     |  max_features = 1\n",
      "     |  \n",
      "     |  param = 'monotonic_cst'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |  \n",
      "     |  set_output(self, *, transform=None)\n",
      "     |      Set output container.\n",
      "     |      \n",
      "     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "     |      for an example on how to use the API.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      "     |          Configure output of `transform` and `fit_transform`.\n",
      "     |      \n",
      "     |          - `\"default\"`: Default output format of a transformer\n",
      "     |          - `\"pandas\"`: DataFrame output\n",
      "     |          - `\"polars\"`: Polars output\n",
      "     |          - `None`: Transform configuration is unchanged\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |              `\"polars\"` option was added.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |  \n",
      "     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)\n",
      "     |      Set the ``set_{method}_request`` methods.\n",
      "     |      \n",
      "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      "     |      looks for the information available in the set default values which are\n",
      "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      "     |      from method signatures.\n",
      "     |      \n",
      "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
      "     |      does not explicitly accept a metadata through its arguments or if the\n",
      "     |      developer would like to specify a request value for those metadata\n",
      "     |      which are different from the default ``None``.\n",
      "     |      \n",
      "     |      References\n",
      "     |      ----------\n",
      "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      "     |          Return a node indicator matrix where non zero elements indicates\n",
      "     |          that the samples goes through the nodes. The matrix is of CSR\n",
      "     |          format.\n",
      "     |      \n",
      "     |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseForest:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of indices identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRequest\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      "     |          routing information.\n",
      "    \n",
      "    class StackingClassifier(sklearn.base.ClassifierMixin, _BaseStacking)\n",
      "     |  StackingClassifier(estimators, final_estimator=None, *, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)\n",
      "     |  \n",
      "     |  Stack of estimators with a final classifier.\n",
      "     |  \n",
      "     |  Stacked generalization consists in stacking the output of individual\n",
      "     |  estimator and use a classifier to compute the final prediction. Stacking\n",
      "     |  allows to use the strength of each individual estimator by using their\n",
      "     |  output as input of a final estimator.\n",
      "     |  \n",
      "     |  Note that `estimators_` are fitted on the full `X` while `final_estimator_`\n",
      "     |  is trained using cross-validated predictions of the base estimators using\n",
      "     |  `cross_val_predict`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <stacking>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimators : list of (str, estimator)\n",
      "     |      Base estimators which will be stacked together. Each element of the\n",
      "     |      list is defined as a tuple of string (i.e. name) and an estimator\n",
      "     |      instance. An estimator can be set to 'drop' using `set_params`.\n",
      "     |  \n",
      "     |      The type of estimator is generally expected to be a classifier.\n",
      "     |      However, one can pass a regressor for some use case (e.g. ordinal\n",
      "     |      regression).\n",
      "     |  \n",
      "     |  final_estimator : estimator, default=None\n",
      "     |      A classifier which will be used to combine the base estimators.\n",
      "     |      The default classifier is a\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression`.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator, iterable, or \"prefit\", default=None\n",
      "     |      Determines the cross-validation splitting strategy used in\n",
      "     |      `cross_val_predict` to train `final_estimator`. Possible inputs for\n",
      "     |      cv are:\n",
      "     |  \n",
      "     |      * None, to use the default 5-fold cross validation,\n",
      "     |      * integer, to specify the number of folds in a (Stratified) KFold,\n",
      "     |      * An object to be used as a cross-validation generator,\n",
      "     |      * An iterable yielding train, test splits,\n",
      "     |      * `\"prefit\"`, to assume the `estimators` are prefit. In this case, the\n",
      "     |        estimators will not be refitted.\n",
      "     |  \n",
      "     |      For integer/None inputs, if the estimator is a classifier and y is\n",
      "     |      either binary or multiclass,\n",
      "     |      :class:`~sklearn.model_selection.StratifiedKFold` is used.\n",
      "     |      In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n",
      "     |      These splitters are instantiated with `shuffle=False` so the splits\n",
      "     |      will be the same across calls.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      If \"prefit\" is passed, it is assumed that all `estimators` have\n",
      "     |      been fitted already. The `final_estimator_` is trained on the `estimators`\n",
      "     |      predictions on the full training set and are **not** cross validated\n",
      "     |      predictions. Please note that if the models have been trained on the same\n",
      "     |      data to train the stacking model, there is a very high risk of overfitting.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.1\n",
      "     |          The 'prefit' option was added in 1.1\n",
      "     |  \n",
      "     |      .. note::\n",
      "     |         A larger number of split will provide no benefits if the number\n",
      "     |         of training samples is large enough. Indeed, the training time\n",
      "     |         will increase. ``cv`` is not used for model evaluation but for\n",
      "     |         prediction.\n",
      "     |  \n",
      "     |  stack_method : {'auto', 'predict_proba', 'decision_function', 'predict'},             default='auto'\n",
      "     |      Methods called for each base estimator. It can be:\n",
      "     |  \n",
      "     |      * if 'auto', it will try to invoke, for each estimator,\n",
      "     |        `'predict_proba'`, `'decision_function'` or `'predict'` in that\n",
      "     |        order.\n",
      "     |      * otherwise, one of `'predict_proba'`, `'decision_function'` or\n",
      "     |        `'predict'`. If the method is not implemented by the estimator, it\n",
      "     |        will raise an error.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel for `fit` of all `estimators`.\n",
      "     |      `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n",
      "     |      using all processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  passthrough : bool, default=False\n",
      "     |      When False, only the predictions of estimators will be used as\n",
      "     |      training data for `final_estimator`. When True, the\n",
      "     |      `final_estimator` is trained on the predictions as well as the\n",
      "     |      original training data.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Verbosity level.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes,) or list of ndarray if `y`         is of type `\"multilabel-indicator\"`.\n",
      "     |      Class labels.\n",
      "     |  \n",
      "     |  estimators_ : list of estimators\n",
      "     |      The elements of the `estimators` parameter, having been fitted on the\n",
      "     |      training data. If an estimator has been set to `'drop'`, it\n",
      "     |      will not appear in `estimators_`. When `cv=\"prefit\"`, `estimators_`\n",
      "     |      is set to `estimators` and is not fitted again.\n",
      "     |  \n",
      "     |  named_estimators_ : :class:`~sklearn.utils.Bunch`\n",
      "     |      Attribute to access any fitted sub-estimators by name.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`. Only defined if the\n",
      "     |      underlying estimator exposes such an attribute when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Only defined if the\n",
      "     |      underlying estimators expose such an attribute when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  final_estimator_ : estimator\n",
      "     |      The classifier fit on the output of `estimators_` and responsible for\n",
      "     |      final predictions.\n",
      "     |  \n",
      "     |  stack_method_ : list of str\n",
      "     |      The method used by each base estimator.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  StackingRegressor : Stack of estimators with a final regressor.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  When `predict_proba` is used by each estimator (i.e. most of the time for\n",
      "     |  `stack_method='auto'` or specifically for `stack_method='predict_proba'`),\n",
      "     |  the first column predicted by each estimator will be dropped in the case\n",
      "     |  of a binary classification problem. Indeed, both feature will be perfectly\n",
      "     |  collinear.\n",
      "     |  \n",
      "     |  In some cases (e.g. ordinal regression), one can pass regressors as the\n",
      "     |  first layer of the :class:`StackingClassifier`. However, note that `y` will\n",
      "     |  be internally encoded in a numerically increasing order or lexicographic\n",
      "     |  order. If this ordering is not adequate, one should manually numerically\n",
      "     |  encode the classes in the desired order.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n",
      "     |     (1992): 241-259.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      "     |  >>> from sklearn.svm import LinearSVC\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> from sklearn.ensemble import StackingClassifier\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> estimators = [\n",
      "     |  ...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
      "     |  ...     ('svr', make_pipeline(StandardScaler(),\n",
      "     |  ...                           LinearSVC(random_state=42)))\n",
      "     |  ... ]\n",
      "     |  >>> clf = StackingClassifier(\n",
      "     |  ...     estimators=estimators, final_estimator=LogisticRegression()\n",
      "     |  ... )\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...     X, y, stratify=y, random_state=42\n",
      "     |  ... )\n",
      "     |  >>> clf.fit(X_train, y_train).score(X_test, y_test)\n",
      "     |  0.9...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StackingClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseStacking\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.utils._set_output._SetOutputMixin\n",
      "     |      sklearn.ensemble._base._BaseHeterogeneousEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.utils.metaestimators._BaseComposition\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimators, final_estimator=None, *, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Decision function for samples in `X` using the final estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      decisions : ndarray of shape (n_samples,), (n_samples, n_classes),             or (n_samples, n_classes * (n_classes-1) / 2)\n",
      "     |          The decision function computed the final estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, **fit_params)\n",
      "     |      Fit the estimators.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values. Note that `y` will be internally encoded in\n",
      "     |          numerically increasing order or lexicographic order. If the order\n",
      "     |          matter (e.g. for ordinal regression), one should numerically encode\n",
      "     |          the target `y` before calling :term:`fit`.\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Parameters to pass to the underlying estimators.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.6\n",
      "     |      \n",
      "     |              Only available if `enable_metadata_routing=True`, which can be\n",
      "     |              set by using ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns a fitted instance of estimator.\n",
      "     |  \n",
      "     |  predict(self, X, **predict_params)\n",
      "     |      Predict target for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      **predict_params : dict of str -> obj\n",
      "     |          Parameters to the `predict` called by the `final_estimator`. Note\n",
      "     |          that this may be used to return uncertainties from some estimators\n",
      "     |          with `return_std` or `return_cov`. Be aware that it will only\n",
      "     |          account for uncertainty in the final estimator.\n",
      "     |      \n",
      "     |          - If `enable_metadata_routing=False` (default):\n",
      "     |            Parameters directly passed to the `predict` method of the\n",
      "     |            `final_estimator`.\n",
      "     |      \n",
      "     |          - If `enable_metadata_routing=True`: Parameters safely routed to\n",
      "     |            the `predict` method of the `final_estimator`. See :ref:`Metadata\n",
      "     |            Routing User Guide <metadata_routing>` for more details.\n",
      "     |      \n",
      "     |          .. versionchanged:: 1.6\n",
      "     |              `**predict_params` can be routed via metadata routing API.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_output)\n",
      "     |          Predicted targets.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for `X` using the final estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      probabilities : ndarray of shape (n_samples, n_classes) or             list of ndarray of shape (n_output,)\n",
      "     |          The class probabilities of the input samples.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._stacking.StackingClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._stacking.StackingClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Return class labels or probabilities for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_preds : ndarray of shape (n_samples, n_estimators) or                 (n_samples, n_classes * n_estimators)\n",
      "     |          Prediction outputs for each estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseStacking:\n",
      "     |  \n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features. The input feature names are only used when `passthrough` is\n",
      "     |          `True`.\n",
      "     |      \n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then names are generated: `[x0, x1, ..., x(n_features_in_ - 1)]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |      \n",
      "     |          If `passthrough` is `False`, then only the names of `estimators` are used\n",
      "     |          to generate the output feature names.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Transformed feature names.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseStacking:\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |  \n",
      "     |  set_output(self, *, transform=None)\n",
      "     |      Set output container.\n",
      "     |      \n",
      "     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "     |      for an example on how to use the API.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      "     |          Configure output of `transform` and `fit_transform`.\n",
      "     |      \n",
      "     |          - `\"default\"`: Default output format of a transformer\n",
      "     |          - `\"pandas\"`: DataFrame output\n",
      "     |          - `\"polars\"`: Polars output\n",
      "     |          - `None`: Transform configuration is unchanged\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |              `\"polars\"` option was added.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |  \n",
      "     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Returns the parameters given in the constructor as well as the\n",
      "     |      estimators contained within the `estimators` parameter.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          Setting it to True gets the various estimators and the parameters\n",
      "     |          of the estimators as well.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter and estimator names mapped to their values or parameter\n",
      "     |          names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Valid parameter keys can be listed with `get_params()`. Note that you\n",
      "     |      can directly set the parameters of the estimators contained in\n",
      "     |      `estimators`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : keyword arguments\n",
      "     |          Specific parameters using e.g.\n",
      "     |          `set_params(parameter_name=new_value)`. In addition, to setting the\n",
      "     |          parameters of the estimator, the individual estimator of the\n",
      "     |          estimators can also be set, or can be removed by setting them to\n",
      "     |          'drop'.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  named_estimators\n",
      "     |      Dictionary to access any fitted sub-estimators by name.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`~sklearn.utils.Bunch`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "    \n",
      "    class StackingRegressor(sklearn.base.RegressorMixin, _BaseStacking)\n",
      "     |  StackingRegressor(estimators, final_estimator=None, *, cv=None, n_jobs=None, passthrough=False, verbose=0)\n",
      "     |  \n",
      "     |  Stack of estimators with a final regressor.\n",
      "     |  \n",
      "     |  Stacked generalization consists in stacking the output of individual\n",
      "     |  estimator and use a regressor to compute the final prediction. Stacking\n",
      "     |  allows to use the strength of each individual estimator by using their\n",
      "     |  output as input of a final estimator.\n",
      "     |  \n",
      "     |  Note that `estimators_` are fitted on the full `X` while `final_estimator_`\n",
      "     |  is trained using cross-validated predictions of the base estimators using\n",
      "     |  `cross_val_predict`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <stacking>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimators : list of (str, estimator)\n",
      "     |      Base estimators which will be stacked together. Each element of the\n",
      "     |      list is defined as a tuple of string (i.e. name) and an estimator\n",
      "     |      instance. An estimator can be set to 'drop' using `set_params`.\n",
      "     |  \n",
      "     |  final_estimator : estimator, default=None\n",
      "     |      A regressor which will be used to combine the base estimators.\n",
      "     |      The default regressor is a :class:`~sklearn.linear_model.RidgeCV`.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator, iterable, or \"prefit\", default=None\n",
      "     |      Determines the cross-validation splitting strategy used in\n",
      "     |      `cross_val_predict` to train `final_estimator`. Possible inputs for\n",
      "     |      cv are:\n",
      "     |  \n",
      "     |      * None, to use the default 5-fold cross validation,\n",
      "     |      * integer, to specify the number of folds in a (Stratified) KFold,\n",
      "     |      * An object to be used as a cross-validation generator,\n",
      "     |      * An iterable yielding train, test splits,\n",
      "     |      * `\"prefit\"`, to assume the `estimators` are prefit. In this case, the\n",
      "     |        estimators will not be refitted.\n",
      "     |  \n",
      "     |      For integer/None inputs, if the estimator is a classifier and y is\n",
      "     |      either binary or multiclass,\n",
      "     |      :class:`~sklearn.model_selection.StratifiedKFold` is used.\n",
      "     |      In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n",
      "     |      These splitters are instantiated with `shuffle=False` so the splits\n",
      "     |      will be the same across calls.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      If \"prefit\" is passed, it is assumed that all `estimators` have\n",
      "     |      been fitted already. The `final_estimator_` is trained on the `estimators`\n",
      "     |      predictions on the full training set and are **not** cross validated\n",
      "     |      predictions. Please note that if the models have been trained on the same\n",
      "     |      data to train the stacking model, there is a very high risk of overfitting.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.1\n",
      "     |          The 'prefit' option was added in 1.1\n",
      "     |  \n",
      "     |      .. note::\n",
      "     |         A larger number of split will provide no benefits if the number\n",
      "     |         of training samples is large enough. Indeed, the training time\n",
      "     |         will increase. ``cv`` is not used for model evaluation but for\n",
      "     |         prediction.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel for `fit` of all `estimators`.\n",
      "     |      `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n",
      "     |      using all processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  passthrough : bool, default=False\n",
      "     |      When False, only the predictions of estimators will be used as\n",
      "     |      training data for `final_estimator`. When True, the\n",
      "     |      `final_estimator` is trained on the predictions as well as the\n",
      "     |      original training data.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Verbosity level.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of estimators\n",
      "     |      The elements of the `estimators` parameter, having been fitted on the\n",
      "     |      training data. If an estimator has been set to `'drop'`, it\n",
      "     |      will not appear in `estimators_`. When `cv=\"prefit\"`, `estimators_`\n",
      "     |      is set to `estimators` and is not fitted again.\n",
      "     |  \n",
      "     |  named_estimators_ : :class:`~sklearn.utils.Bunch`\n",
      "     |      Attribute to access any fitted sub-estimators by name.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`. Only defined if the\n",
      "     |      underlying estimator exposes such an attribute when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Only defined if the\n",
      "     |      underlying estimators expose such an attribute when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  final_estimator_ : estimator\n",
      "     |      The regressor fit on the output of `estimators_` and responsible for\n",
      "     |      final predictions.\n",
      "     |  \n",
      "     |  stack_method_ : list of str\n",
      "     |      The method used by each base estimator.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  StackingClassifier : Stack of estimators with a final classifier.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n",
      "     |     (1992): 241-259.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_diabetes\n",
      "     |  >>> from sklearn.linear_model import RidgeCV\n",
      "     |  >>> from sklearn.svm import LinearSVR\n",
      "     |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      "     |  >>> from sklearn.ensemble import StackingRegressor\n",
      "     |  >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |  >>> estimators = [\n",
      "     |  ...     ('lr', RidgeCV()),\n",
      "     |  ...     ('svr', LinearSVR(random_state=42))\n",
      "     |  ... ]\n",
      "     |  >>> reg = StackingRegressor(\n",
      "     |  ...     estimators=estimators,\n",
      "     |  ...     final_estimator=RandomForestRegressor(n_estimators=10,\n",
      "     |  ...                                           random_state=42)\n",
      "     |  ... )\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...     X, y, random_state=42\n",
      "     |  ... )\n",
      "     |  >>> reg.fit(X_train, y_train).score(X_test, y_test)\n",
      "     |  0.3...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StackingRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      _BaseStacking\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.utils._set_output._SetOutputMixin\n",
      "     |      sklearn.ensemble._base._BaseHeterogeneousEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.utils.metaestimators._BaseComposition\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimators, final_estimator=None, *, cv=None, n_jobs=None, passthrough=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, **fit_params)\n",
      "     |      Fit the estimators.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Parameters to pass to the underlying estimators.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.6\n",
      "     |      \n",
      "     |              Only available if `enable_metadata_routing=True`, which can be\n",
      "     |              set by using ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns a fitted instance.\n",
      "     |  \n",
      "     |  fit_transform(self, X, y, **fit_params)\n",
      "     |      Fit the estimators and return the predictions for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Parameters to pass to the underlying estimators.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.6\n",
      "     |      \n",
      "     |              Only available if `enable_metadata_routing=True`, which can be\n",
      "     |              set by using ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_preds : ndarray of shape (n_samples, n_estimators)\n",
      "     |          Prediction outputs for each estimator.\n",
      "     |  \n",
      "     |  predict(self, X, **predict_params)\n",
      "     |      Predict target for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      **predict_params : dict of str -> obj\n",
      "     |          Parameters to the `predict` called by the `final_estimator`. Note\n",
      "     |          that this may be used to return uncertainties from some estimators\n",
      "     |          with `return_std` or `return_cov`. Be aware that it will only\n",
      "     |          account for uncertainty in the final estimator.\n",
      "     |      \n",
      "     |          - If `enable_metadata_routing=False` (default):\n",
      "     |            Parameters directly passed to the `predict` method of the\n",
      "     |            `final_estimator`.\n",
      "     |      \n",
      "     |          - If `enable_metadata_routing=True`: Parameters safely routed to\n",
      "     |            the `predict` method of the `final_estimator`. See :ref:`Metadata\n",
      "     |            Routing User Guide <metadata_routing>` for more details.\n",
      "     |      \n",
      "     |          .. versionchanged:: 1.6\n",
      "     |              `**predict_params` can be routed via metadata routing API.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_output)\n",
      "     |          Predicted targets.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._stacking.StackingRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._stacking.StackingRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Return the predictions for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_preds : ndarray of shape (n_samples, n_estimators)\n",
      "     |          Prediction outputs for each estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseStacking:\n",
      "     |  \n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Input features. The input feature names are only used when `passthrough` is\n",
      "     |          `True`.\n",
      "     |      \n",
      "     |          - If `input_features` is `None`, then `feature_names_in_` is\n",
      "     |            used as feature names in. If `feature_names_in_` is not defined,\n",
      "     |            then names are generated: `[x0, x1, ..., x(n_features_in_ - 1)]`.\n",
      "     |          - If `input_features` is an array-like, then `input_features` must\n",
      "     |            match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "     |      \n",
      "     |          If `passthrough` is `False`, then only the names of `estimators` are used\n",
      "     |          to generate the output feature names.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Transformed feature names.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.6\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseStacking:\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |  \n",
      "     |  set_output(self, *, transform=None)\n",
      "     |      Set output container.\n",
      "     |      \n",
      "     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "     |      for an example on how to use the API.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      "     |          Configure output of `transform` and `fit_transform`.\n",
      "     |      \n",
      "     |          - `\"default\"`: Default output format of a transformer\n",
      "     |          - `\"pandas\"`: DataFrame output\n",
      "     |          - `\"polars\"`: Polars output\n",
      "     |          - `None`: Transform configuration is unchanged\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |              `\"polars\"` option was added.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |  \n",
      "     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Returns the parameters given in the constructor as well as the\n",
      "     |      estimators contained within the `estimators` parameter.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          Setting it to True gets the various estimators and the parameters\n",
      "     |          of the estimators as well.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter and estimator names mapped to their values or parameter\n",
      "     |          names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Valid parameter keys can be listed with `get_params()`. Note that you\n",
      "     |      can directly set the parameters of the estimators contained in\n",
      "     |      `estimators`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : keyword arguments\n",
      "     |          Specific parameters using e.g.\n",
      "     |          `set_params(parameter_name=new_value)`. In addition, to setting the\n",
      "     |          parameters of the estimator, the individual estimator of the\n",
      "     |          estimators can also be set, or can be removed by setting them to\n",
      "     |          'drop'.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  named_estimators\n",
      "     |      Dictionary to access any fitted sub-estimators by name.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`~sklearn.utils.Bunch`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "    \n",
      "    class VotingClassifier(sklearn.base.ClassifierMixin, _BaseVoting)\n",
      "     |  VotingClassifier(estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False)\n",
      "     |  \n",
      "     |  Soft Voting/Majority Rule classifier for unfitted estimators.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <voting_classifier>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimators : list of (str, estimator) tuples\n",
      "     |      Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n",
      "     |      of those original estimators that will be stored in the class attribute\n",
      "     |      ``self.estimators_``. An estimator can be set to ``'drop'`` using\n",
      "     |      :meth:`set_params`.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.21\n",
      "     |          ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n",
      "     |          support was removed in 0.24.\n",
      "     |  \n",
      "     |  voting : {'hard', 'soft'}, default='hard'\n",
      "     |      If 'hard', uses predicted class labels for majority rule voting.\n",
      "     |      Else if 'soft', predicts the class label based on the argmax of\n",
      "     |      the sums of the predicted probabilities, which is recommended for\n",
      "     |      an ensemble of well-calibrated classifiers.\n",
      "     |  \n",
      "     |  weights : array-like of shape (n_classifiers,), default=None\n",
      "     |      Sequence of weights (`float` or `int`) to weight the occurrences of\n",
      "     |      predicted class labels (`hard` voting) or class probabilities\n",
      "     |      before averaging (`soft` voting). Uses uniform weights if `None`.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel for ``fit``.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  flatten_transform : bool, default=True\n",
      "     |      Affects shape of transform output only when voting='soft'\n",
      "     |      If voting='soft' and flatten_transform=True, transform method returns\n",
      "     |      matrix with shape (n_samples, n_classifiers * n_classes). If\n",
      "     |      flatten_transform=False, it returns\n",
      "     |      (n_classifiers, n_samples, n_classes).\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      If True, the time elapsed while fitting will be printed as it\n",
      "     |      is completed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of classifiers\n",
      "     |      The collection of fitted sub-estimators as defined in ``estimators``\n",
      "     |      that are not 'drop'.\n",
      "     |  \n",
      "     |  named_estimators_ : :class:`~sklearn.utils.Bunch`\n",
      "     |      Attribute to access any fitted sub-estimators by name.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  le_ : :class:`~sklearn.preprocessing.LabelEncoder`\n",
      "     |      Transformer used to encode the labels during fit and decode during\n",
      "     |      prediction.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`. Only defined if the\n",
      "     |      underlying classifier exposes such an attribute when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Only defined if the\n",
      "     |      underlying estimators expose such an attribute when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  VotingRegressor : Prediction voting regressor.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> from sklearn.naive_bayes import GaussianNB\n",
      "     |  >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
      "     |  >>> clf1 = LogisticRegression(random_state=1)\n",
      "     |  >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
      "     |  >>> clf3 = GaussianNB()\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      "     |  >>> y = np.array([1, 1, 1, 2, 2, 2])\n",
      "     |  >>> eclf1 = VotingClassifier(estimators=[\n",
      "     |  ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
      "     |  >>> eclf1 = eclf1.fit(X, y)\n",
      "     |  >>> print(eclf1.predict(X))\n",
      "     |  [1 1 1 2 2 2]\n",
      "     |  >>> np.array_equal(eclf1.named_estimators_.lr.predict(X),\n",
      "     |  ...                eclf1.named_estimators_['lr'].predict(X))\n",
      "     |  True\n",
      "     |  >>> eclf2 = VotingClassifier(estimators=[\n",
      "     |  ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
      "     |  ...         voting='soft')\n",
      "     |  >>> eclf2 = eclf2.fit(X, y)\n",
      "     |  >>> print(eclf2.predict(X))\n",
      "     |  [1 1 1 2 2 2]\n",
      "     |  \n",
      "     |  To drop an estimator, :meth:`set_params` can be used to remove it. Here we\n",
      "     |  dropped one of the estimators, resulting in 2 fitted estimators:\n",
      "     |  \n",
      "     |  >>> eclf2 = eclf2.set_params(lr='drop')\n",
      "     |  >>> eclf2 = eclf2.fit(X, y)\n",
      "     |  >>> len(eclf2.estimators_)\n",
      "     |  2\n",
      "     |  \n",
      "     |  Setting `flatten_transform=True` with `voting='soft'` flattens output shape of\n",
      "     |  `transform`:\n",
      "     |  \n",
      "     |  >>> eclf3 = VotingClassifier(estimators=[\n",
      "     |  ...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
      "     |  ...        voting='soft', weights=[2,1,1],\n",
      "     |  ...        flatten_transform=True)\n",
      "     |  >>> eclf3 = eclf3.fit(X, y)\n",
      "     |  >>> print(eclf3.predict(X))\n",
      "     |  [1 1 1 2 2 2]\n",
      "     |  >>> print(eclf3.transform(X).shape)\n",
      "     |  (6, 6)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VotingClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseVoting\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.utils._set_output._SetOutputMixin\n",
      "     |      sklearn.ensemble._base._BaseHeterogeneousEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.utils.metaestimators._BaseComposition\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  fit(self, X, y, **fit_params)\n",
      "     |      Fit the estimators.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Parameters to pass to the underlying estimators.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.5\n",
      "     |      \n",
      "     |              Only available if `enable_metadata_routing=True`,\n",
      "     |              which can be set by using\n",
      "     |              ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns the instance itself.\n",
      "     |  \n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Transformed feature names.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      maj : array-like of shape (n_samples,)\n",
      "     |          Predicted class labels.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Compute probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      avg : array-like of shape (n_samples, n_classes)\n",
      "     |          Weighted average probability for each class per sample.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._voting.VotingClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._voting.VotingClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Return class labels or probabilities for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      probabilities_or_labels\n",
      "     |          If `voting='soft'` and `flatten_transform=True`:\n",
      "     |              returns ndarray of shape (n_samples, n_classifiers * n_classes),\n",
      "     |              being class probabilities calculated by each classifier.\n",
      "     |          If `voting='soft' and `flatten_transform=False`:\n",
      "     |              ndarray of shape (n_classifiers, n_samples, n_classes)\n",
      "     |          If `voting='hard'`:\n",
      "     |              ndarray of shape (n_samples, n_classifiers), being\n",
      "     |              class labels predicted by each classifier.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`accuracy <accuracy_score>` on provided data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseVoting:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Return class labels or probabilities for each estimator.\n",
      "     |      \n",
      "     |      Return predictions for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,), default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseVoting:\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |  \n",
      "     |  set_output(self, *, transform=None)\n",
      "     |      Set output container.\n",
      "     |      \n",
      "     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "     |      for an example on how to use the API.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      "     |          Configure output of `transform` and `fit_transform`.\n",
      "     |      \n",
      "     |          - `\"default\"`: Default output format of a transformer\n",
      "     |          - `\"pandas\"`: DataFrame output\n",
      "     |          - `\"polars\"`: Polars output\n",
      "     |          - `None`: Transform configuration is unchanged\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |              `\"polars\"` option was added.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |  \n",
      "     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Returns the parameters given in the constructor as well as the\n",
      "     |      estimators contained within the `estimators` parameter.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          Setting it to True gets the various estimators and the parameters\n",
      "     |          of the estimators as well.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter and estimator names mapped to their values or parameter\n",
      "     |          names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Valid parameter keys can be listed with `get_params()`. Note that you\n",
      "     |      can directly set the parameters of the estimators contained in\n",
      "     |      `estimators`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : keyword arguments\n",
      "     |          Specific parameters using e.g.\n",
      "     |          `set_params(parameter_name=new_value)`. In addition, to setting the\n",
      "     |          parameters of the estimator, the individual estimator of the\n",
      "     |          estimators can also be set, or can be removed by setting them to\n",
      "     |          'drop'.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  named_estimators\n",
      "     |      Dictionary to access any fitted sub-estimators by name.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`~sklearn.utils.Bunch`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "    \n",
      "    class VotingRegressor(sklearn.base.RegressorMixin, _BaseVoting)\n",
      "     |  VotingRegressor(estimators, *, weights=None, n_jobs=None, verbose=False)\n",
      "     |  \n",
      "     |  Prediction voting regressor for unfitted estimators.\n",
      "     |  \n",
      "     |  A voting regressor is an ensemble meta-estimator that fits several base\n",
      "     |  regressors, each on the whole dataset. Then it averages the individual\n",
      "     |  predictions to form a final prediction.\n",
      "     |  \n",
      "     |  For a detailed example, refer to\n",
      "     |  :ref:`sphx_glr_auto_examples_ensemble_plot_voting_regressor.py`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <voting_regressor>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.21\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimators : list of (str, estimator) tuples\n",
      "     |      Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones\n",
      "     |      of those original estimators that will be stored in the class attribute\n",
      "     |      ``self.estimators_``. An estimator can be set to ``'drop'`` using\n",
      "     |      :meth:`set_params`.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.21\n",
      "     |          ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n",
      "     |          support was removed in 0.24.\n",
      "     |  \n",
      "     |  weights : array-like of shape (n_regressors,), default=None\n",
      "     |      Sequence of weights (`float` or `int`) to weight the occurrences of\n",
      "     |      predicted values before averaging. Uses uniform weights if `None`.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel for ``fit``.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      If True, the time elapsed while fitting will be printed as it\n",
      "     |      is completed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of regressors\n",
      "     |      The collection of fitted sub-estimators as defined in ``estimators``\n",
      "     |      that are not 'drop'.\n",
      "     |  \n",
      "     |  named_estimators_ : :class:`~sklearn.utils.Bunch`\n",
      "     |      Attribute to access any fitted sub-estimators by name.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_features_in_ : int\n",
      "     |      Number of features seen during :term:`fit`. Only defined if the\n",
      "     |      underlying regressor exposes such an attribute when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "     |      Names of features seen during :term:`fit`. Only defined if the\n",
      "     |      underlying estimators expose such an attribute when fit.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.0\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  VotingClassifier : Soft Voting/Majority Rule classifier.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import LinearRegression\n",
      "     |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      "     |  >>> from sklearn.ensemble import VotingRegressor\n",
      "     |  >>> from sklearn.neighbors import KNeighborsRegressor\n",
      "     |  >>> r1 = LinearRegression()\n",
      "     |  >>> r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n",
      "     |  >>> r3 = KNeighborsRegressor()\n",
      "     |  >>> X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n",
      "     |  >>> y = np.array([2, 6, 12, 20, 30, 42])\n",
      "     |  >>> er = VotingRegressor([('lr', r1), ('rf', r2), ('r3', r3)])\n",
      "     |  >>> print(er.fit(X, y).predict(X))\n",
      "     |  [ 6.8  8.4 12.5 17.8 26  34]\n",
      "     |  \n",
      "     |  In the following example, we drop the `'lr'` estimator with\n",
      "     |  :meth:`~VotingRegressor.set_params` and fit the remaining two estimators:\n",
      "     |  \n",
      "     |  >>> er = er.set_params(lr='drop')\n",
      "     |  >>> er = er.fit(X, y)\n",
      "     |  >>> len(er.estimators_)\n",
      "     |  2\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VotingRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      _BaseVoting\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.utils._set_output._SetOutputMixin\n",
      "     |      sklearn.ensemble._base._BaseHeterogeneousEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.utils.metaestimators._BaseComposition\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.utils._repr_html.base.ReprHTMLMixin\n",
      "     |      sklearn.utils._repr_html.base._HTMLDocumentationLinkMixin\n",
      "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimators, *, weights=None, n_jobs=None, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, **fit_params)\n",
      "     |      Fit the estimators.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Parameters to pass to the underlying estimators.\n",
      "     |      \n",
      "     |          .. versionadded:: 1.5\n",
      "     |      \n",
      "     |              Only available if `enable_metadata_routing=True`,\n",
      "     |              which can be set by using\n",
      "     |              ``sklearn.set_config(enable_metadata_routing=True)``.\n",
      "     |              See :ref:`Metadata Routing User Guide <metadata_routing>` for\n",
      "     |              more details.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  get_feature_names_out(self, input_features=None)\n",
      "     |      Get output feature names for transformation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      input_features : array-like of str or None, default=None\n",
      "     |          Not used, present here for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names_out : ndarray of str objects\n",
      "     |          Transformed feature names.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      The predicted regression target of an input sample is computed as the\n",
      "     |      mean predicted regression targets of the estimators in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  set_score_request(self: sklearn.ensemble._voting.VotingRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._voting.VotingRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      "     |      Configure whether metadata should be requested to be passed to the ``score`` method.\n",
      "     |      \n",
      "     |      Note that this method is only relevant when this estimator is used as a\n",
      "     |      sub-estimator within a :term:`meta-estimator` and metadata routing is enabled\n",
      "     |      with ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      "     |      Please check the :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      The options for each parameter are:\n",
      "     |      \n",
      "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      "     |      \n",
      "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      "     |      \n",
      "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      "     |      \n",
      "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      "     |      \n",
      "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      "     |      existing request. This allows you to change the request for some\n",
      "     |      parameters and not others.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.3\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          The updated object.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Return predictions for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      predictions : ndarray of shape (n_samples, n_classifiers)\n",
      "     |          Values predicted by each regressor.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __sklearn_tags__(self)\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return :ref:`coefficient of determination <r2_score>` on test data.\n",
      "     |      \n",
      "     |      The coefficient of determination, :math:`R^2`, is defined as\n",
      "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always predicts\n",
      "     |      the expected value of `y`, disregarding the input features, would get\n",
      "     |      a :math:`R^2` score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseVoting:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Return class labels or probabilities for each estimator.\n",
      "     |      \n",
      "     |      Return predictions for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,), default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  get_metadata_routing(self)\n",
      "     |      Get metadata routing of this object.\n",
      "     |      \n",
      "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      "     |      mechanism works.\n",
      "     |      \n",
      "     |      .. versionadded:: 1.5\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      routing : MetadataRouter\n",
      "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRouter` encapsulating\n",
      "     |          routing information.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseVoting:\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |  \n",
      "     |  set_output(self, *, transform=None)\n",
      "     |      Set output container.\n",
      "     |      \n",
      "     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\n",
      "     |      for an example on how to use the API.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      transform : {\"default\", \"pandas\", \"polars\"}, default=None\n",
      "     |          Configure output of `transform` and `fit_transform`.\n",
      "     |      \n",
      "     |          - `\"default\"`: Default output format of a transformer\n",
      "     |          - `\"pandas\"`: DataFrame output\n",
      "     |          - `\"polars\"`: Polars output\n",
      "     |          - `None`: Transform configuration is unchanged\n",
      "     |      \n",
      "     |          .. versionadded:: 1.4\n",
      "     |              `\"polars\"` option was added.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:\n",
      "     |  \n",
      "     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Returns the parameters given in the constructor as well as the\n",
      "     |      estimators contained within the `estimators` parameter.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          Setting it to True gets the various estimators and the parameters\n",
      "     |          of the estimators as well.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter and estimator names mapped to their values or parameter\n",
      "     |          names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Valid parameter keys can be listed with `get_params()`. Note that you\n",
      "     |      can directly set the parameters of the estimators contained in\n",
      "     |      `estimators`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : keyword arguments\n",
      "     |          Specific parameters using e.g.\n",
      "     |          `set_params(parameter_name=new_value)`. In addition, to setting the\n",
      "     |          parameters of the estimator, the individual estimator of the\n",
      "     |          estimators can also be set, or can be removed by setting them to\n",
      "     |          'drop'.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  named_estimators\n",
      "     |      Dictionary to access any fitted sub-estimators by name.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      :class:`~sklearn.utils.Bunch`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __sklearn_clone__(self)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['AdaBoostClassifier', 'AdaBoostRegressor', 'BaggingClassifi...\n",
      "\n",
      "FILE\n",
      "    c:\\project\\bigdata_cert\\.venv\\lib\\site-packages\\sklearn\\ensemble\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "help(ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d1c5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c128ee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_metric(models, X_train, X_test, y_train, y_test):\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = root_mean_squared_error(y_test, y_pred)\n",
    "        print(f'{model} 결과: {rmse}')\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e704b3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression() 결과: 6637.045965650744\n",
      "--------------------------------------------------\n",
      "ElasticNet() 결과: 12928.914036912636\n",
      "--------------------------------------------------\n",
      "RandomForestRegressor() 결과: 4269.156531295163\n",
      "--------------------------------------------------\n",
      "GradientBoostingRegressor() 결과: 4956.774380272118\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "elastic = ElasticNet()\n",
    "rf = RandomForestRegressor()\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "models = [lr, elastic, rf, gbr]\n",
    "\n",
    "model_fit_metric(models, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cc32b7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>flight</th>\n",
       "      <th>source_city</th>\n",
       "      <th>departure_time</th>\n",
       "      <th>stops</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>destination_city</th>\n",
       "      <th>class</th>\n",
       "      <th>duration</th>\n",
       "      <th>days_left</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-778</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>one</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Business</td>\n",
       "      <td>18.58</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Air_India</td>\n",
       "      <td>AI-764</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Evening</td>\n",
       "      <td>one</td>\n",
       "      <td>Late_Night</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Economy</td>\n",
       "      <td>8.92</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Air_India</td>\n",
       "      <td>AI-569</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Economy</td>\n",
       "      <td>12.17</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-960</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Economy</td>\n",
       "      <td>7.67</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GO_FIRST</td>\n",
       "      <td>G8-302</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Economy</td>\n",
       "      <td>14.67</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>Air_India</td>\n",
       "      <td>AI-660</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Evening</td>\n",
       "      <td>one</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>14.92</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-960</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Night</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Economy</td>\n",
       "      <td>11.00</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499</th>\n",
       "      <td>Air_India</td>\n",
       "      <td>AI-442</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Business</td>\n",
       "      <td>3.75</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500</th>\n",
       "      <td>Vistara</td>\n",
       "      <td>UK-860</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Early_Morning</td>\n",
       "      <td>one</td>\n",
       "      <td>Evening</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Economy</td>\n",
       "      <td>9.67</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4501</th>\n",
       "      <td>AirAsia</td>\n",
       "      <td>I5-974</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Night</td>\n",
       "      <td>one</td>\n",
       "      <td>Morning</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Economy</td>\n",
       "      <td>10.83</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4502 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        airline  flight source_city departure_time stops arrival_time  \\\n",
       "0       Vistara  UK-778     Kolkata      Afternoon   one      Morning   \n",
       "1     Air_India  AI-764       Delhi        Evening   one   Late_Night   \n",
       "2     Air_India  AI-569     Chennai  Early_Morning   one      Evening   \n",
       "3       Vistara  UK-960      Mumbai        Morning   one      Evening   \n",
       "4      GO_FIRST  G8-302     Chennai  Early_Morning   one      Evening   \n",
       "...         ...     ...         ...            ...   ...          ...   \n",
       "4497  Air_India  AI-660      Mumbai        Evening   one      Morning   \n",
       "4498    Vistara  UK-960      Mumbai        Morning   one        Night   \n",
       "4499  Air_India  AI-442      Mumbai      Afternoon   one      Evening   \n",
       "4500    Vistara  UK-860   Hyderabad  Early_Morning   one      Evening   \n",
       "4501    AirAsia  I5-974   Hyderabad          Night   one      Morning   \n",
       "\n",
       "     destination_city     class  duration  days_left  \n",
       "0             Chennai  Business     18.58         35  \n",
       "1           Bangalore   Economy      8.92         35  \n",
       "2           Bangalore   Economy     12.17         13  \n",
       "3             Kolkata   Economy      7.67         22  \n",
       "4           Bangalore   Economy     14.67         22  \n",
       "...               ...       ...       ...        ...  \n",
       "4497          Chennai   Economy     14.92          6  \n",
       "4498        Hyderabad   Economy     11.00         47  \n",
       "4499            Delhi  Business      3.75         26  \n",
       "4500           Mumbai   Economy      9.67          2  \n",
       "4501          Kolkata   Economy     10.83         20  \n",
       "\n",
       "[4502 rows x 10 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d3487",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'get_dummies'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m df_test[[\u001b[33m'\u001b[39m\u001b[33mduration\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdays_left\u001b[39m\u001b[33m'\u001b[39m]] = std.transform(df_test[[\u001b[33m'\u001b[39m\u001b[33mduration\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdays_left\u001b[39m\u001b[33m'\u001b[39m]])\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdf_test\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_dummies\u001b[49m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\bigdata_cert\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:6321\u001b[39m, in \u001b[36mNDFrame.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6314\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   6315\u001b[39m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._internal_names_set\n\u001b[32m   6316\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._metadata\n\u001b[32m   6317\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessors\n\u001b[32m   6318\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6319\u001b[39m ):\n\u001b[32m   6320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[32m-> \u001b[39m\u001b[32m6321\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataFrame' object has no attribute 'get_dummies'"
     ]
    }
   ],
   "source": [
    "df_test[['duration', 'days_left']] = std.transform(df_test[['duration', 'days_left']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f200ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df_test, prefix=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata-cert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
